2023-04-25 00:42:13,242 - 0:00:05 - 0.0s - INFO - __main__ - args = Namespace(REG_TYPE_KEYS=['mas', 'ewc'], adam_epsilon=0.0001, add_task_tokens=False, data_dir='../../data', debug=False, decay_style='linear', device_ids=[1, 2, 3, 4, 5, 6], dynamic_epochs=False, fp32=True, gen_lm_sample_percentage=0.0, learning_rate=6.25e-05, lm_lambda=0.0, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[32429.600000000002, 42158.48, 42158.48, 42158.48, 42158.48, 42158.48], min_batch_size=4, min_n_steps=1500, model_dir_root='../../model_conf3/gpt2/lll/sst_srl_woz.en_0.0', model_name='gpt2', n_gpus=6, n_train_epochs={'sst': 12, 'srl': 12, 'woz.en': 12}, n_warmup_ratio=0.005, n_workers=75, qp_margin=0.5, real_sample=False, reg_lambda=1.0, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[11350, 14755, 14755, 14755, 14755, 14755], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[11350, 14755, 14755, 14755, 14755, 14755], unbound=0, use_sep=False, weight_decay=0.01)
2023-04-25 00:42:13,242 - 0:00:05 - 0.0s - INFO - __main__ - start to train { task: ['sst'], seq train type: lll }
2023-04-25 00:42:13,243 - 0:00:05 - 0.0s - INFO - __main__ - extra training data size: 0
2023-04-25 00:42:15,526 - 0:00:07 - 2.3s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
2023-04-25 00:42:18,025 - 0:00:10 - 2.5s - INFO - __main__ - len of train dataset: 6920 , max train batch size 4 , num of opt steps: 83040
2023-04-25 00:44:22,940 - 0:02:14 - 124.9s - INFO - __main__ - progress 0.578 , lr 6.0E-05 , loss 3.012 , qa loss 3.012 , lm loss 0.000 , avg batch size 4.0
2023-04-25 00:45:58,121 - 0:03:50 - 95.2s - INFO - __main__ - epoch 1/12 done , tot steps 1730 , lr 5.7E-05 , loss 1.90 , qa loss 1.90 , lm loss 0.00 , avg batch size 4.0
2023-04-25 00:48:01,503 - 0:05:53 - 123.4s - INFO - __main__ - progress 1.578 , lr 5.4E-05 , loss 0.280 , qa loss 0.280 , lm loss 0.000 , avg batch size 4.0
2023-04-25 00:49:31,705 - 0:07:23 - 90.2s - INFO - __main__ - epoch 2/12 done , tot steps 3460 , lr 5.2E-05 , loss 0.27 , qa loss 0.27 , lm loss 0.00 , avg batch size 4.0
2023-04-25 00:51:35,643 - 0:09:27 - 123.9s - INFO - __main__ - progress 2.578 , lr 4.9E-05 , loss 0.257 , qa loss 0.257 , lm loss 0.000 , avg batch size 4.0
2023-04-25 00:53:07,470 - 0:10:59 - 91.8s - INFO - __main__ - epoch 3/12 done , tot steps 5190 , lr 4.7E-05 , loss 0.25 , qa loss 0.25 , lm loss 0.00 , avg batch size 4.0
2023-04-25 00:55:11,198 - 0:13:03 - 123.7s - INFO - __main__ - progress 3.578 , lr 4.4E-05 , loss 0.244 , qa loss 0.244 , lm loss 0.000 , avg batch size 4.0
2023-04-25 00:56:43,071 - 0:14:35 - 91.9s - INFO - __main__ - epoch 4/12 done , tot steps 6920 , lr 4.2E-05 , loss 0.24 , qa loss 0.24 , lm loss 0.00 , avg batch size 4.0
2023-04-25 00:58:46,135 - 0:16:38 - 123.1s - INFO - __main__ - progress 4.578 , lr 3.9E-05 , loss 0.231 , qa loss 0.231 , lm loss 0.000 , avg batch size 4.0
2023-04-25 01:00:16,420 - 0:18:08 - 90.3s - INFO - __main__ - epoch 5/12 done , tot steps 8650 , lr 3.6E-05 , loss 0.23 , qa loss 0.23 , lm loss 0.00 , avg batch size 4.0
2023-04-25 01:02:21,046 - 0:20:13 - 124.6s - INFO - __main__ - progress 5.578 , lr 3.3E-05 , loss 0.220 , qa loss 0.220 , lm loss 0.000 , avg batch size 4.0
2023-04-25 01:03:52,314 - 0:21:44 - 91.3s - INFO - __main__ - epoch 6/12 done , tot steps 10380 , lr 3.1E-05 , loss 0.22 , qa loss 0.22 , lm loss 0.00 , avg batch size 4.0
2023-04-25 01:05:56,926 - 0:23:48 - 124.6s - INFO - __main__ - progress 6.578 , lr 2.8E-05 , loss 0.214 , qa loss 0.214 , lm loss 0.000 , avg batch size 4.0
2023-04-25 01:07:27,778 - 0:25:19 - 90.9s - INFO - __main__ - epoch 7/12 done , tot steps 12110 , lr 2.6E-05 , loss 0.21 , qa loss 0.21 , lm loss 0.00 , avg batch size 4.0
2023-04-25 01:09:31,317 - 0:27:23 - 123.5s - INFO - __main__ - progress 7.578 , lr 2.3E-05 , loss 0.202 , qa loss 0.202 , lm loss 0.000 , avg batch size 4.0
2023-04-25 01:11:02,503 - 0:28:54 - 91.2s - INFO - __main__ - epoch 8/12 done , tot steps 13840 , lr 2.1E-05 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-04-25 01:13:07,658 - 0:30:59 - 125.2s - INFO - __main__ - progress 8.578 , lr 1.8E-05 , loss 0.190 , qa loss 0.190 , lm loss 0.000 , avg batch size 4.0
2023-04-25 01:14:39,150 - 0:32:31 - 91.5s - INFO - __main__ - epoch 9/12 done , tot steps 15570 , lr 1.6E-05 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-04-25 01:16:43,897 - 0:34:35 - 124.7s - INFO - __main__ - progress 9.578 , lr 1.3E-05 , loss 0.179 , qa loss 0.179 , lm loss 0.000 , avg batch size 4.0
2023-04-25 01:18:14,019 - 0:36:06 - 90.1s - INFO - __main__ - epoch 10/12 done , tot steps 17300 , lr 1.0E-05 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-04-25 01:20:19,087 - 0:38:11 - 125.1s - INFO - __main__ - progress 10.578 , lr 7.4E-06 , loss 0.171 , qa loss 0.171 , lm loss 0.000 , avg batch size 4.0
2023-04-25 01:21:51,936 - 0:39:43 - 92.8s - INFO - __main__ - epoch 11/12 done , tot steps 19030 , lr 5.2E-06 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-04-25 01:23:56,866 - 0:41:48 - 124.9s - INFO - __main__ - progress 11.578 , lr 2.2E-06 , loss 0.164 , qa loss 0.164 , lm loss 0.000 , avg batch size 4.0
2023-04-25 01:25:28,352 - 0:43:20 - 91.5s - INFO - __main__ - epoch 12/12 done , tot steps 20760 , lr 2.6E-08 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
2023-04-25 01:25:35,220 - 0:43:27 - 6.9s - INFO - __main__ - start to train { task: ['srl'], seq train type: lll }
2023-04-25 01:25:38,533 - 0:43:30 - 3.3s - INFO - utils - writing extra data in ../../model_conf3/gpt2/lll/sst_srl_woz.en_0.0/sst/lm.csv ...
2023-04-25 01:25:38,534 - 0:43:30 - 0.0s - INFO - __main__ - extra training data size: 0
2023-04-25 01:25:38,665 - 0:43:30 - 0.1s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
2023-04-25 01:25:41,379 - 0:43:33 - 2.7s - INFO - __main__ - len of train dataset: 6414 , max train batch size 4 , num of opt steps: 76968
2023-04-25 01:28:45,926 - 0:46:37 - 184.5s - INFO - __main__ - progress 0.624 , lr 5.9E-05 , loss 6.307 , qa loss 6.307 , lm loss 0.000 , avg batch size 4.0
2023-04-25 01:30:45,823 - 0:48:37 - 119.9s - INFO - __main__ - epoch 1/12 done , tot steps 1604 , lr 5.7E-05 , loss 4.75 , qa loss 4.75 , lm loss 0.00 , avg batch size 4.0
2023-04-25 01:33:50,865 - 0:51:42 - 185.0s - INFO - __main__ - progress 1.624 , lr 5.4E-05 , loss 1.430 , qa loss 1.430 , lm loss 0.000 , avg batch size 4.0
2023-04-25 01:35:43,558 - 0:53:35 - 112.7s - INFO - __main__ - epoch 2/12 done , tot steps 3208 , lr 5.2E-05 , loss 1.30 , qa loss 1.30 , lm loss 0.00 , avg batch size 4.0
2023-04-25 01:38:49,039 - 0:56:41 - 185.5s - INFO - __main__ - progress 2.624 , lr 4.9E-05 , loss 0.947 , qa loss 0.947 , lm loss 0.000 , avg batch size 4.0
2023-04-25 01:40:40,986 - 0:58:33 - 111.9s - INFO - __main__ - epoch 3/12 done , tot steps 4812 , lr 4.7E-05 , loss 0.91 , qa loss 0.91 , lm loss 0.00 , avg batch size 4.0
2023-04-25 01:43:46,909 - 1:01:38 - 185.9s - INFO - __main__ - progress 3.624 , lr 4.4E-05 , loss 0.754 , qa loss 0.754 , lm loss 0.000 , avg batch size 4.0
2023-04-25 01:45:38,002 - 1:03:30 - 111.1s - INFO - __main__ - epoch 4/12 done , tot steps 6416 , lr 4.2E-05 , loss 0.75 , qa loss 0.75 , lm loss 0.00 , avg batch size 4.0
2023-04-25 01:48:43,198 - 1:06:35 - 185.2s - INFO - __main__ - progress 4.624 , lr 3.8E-05 , loss 0.681 , qa loss 0.681 , lm loss 0.000 , avg batch size 4.0
2023-04-25 01:50:35,586 - 1:08:27 - 112.4s - INFO - __main__ - epoch 5/12 done , tot steps 8020 , lr 3.6E-05 , loss 0.68 , qa loss 0.68 , lm loss 0.00 , avg batch size 4.0
2023-04-25 01:53:39,821 - 1:11:31 - 184.2s - INFO - __main__ - progress 5.624 , lr 3.3E-05 , loss 0.574 , qa loss 0.574 , lm loss 0.000 , avg batch size 4.0
2023-04-25 01:55:31,926 - 1:13:23 - 112.1s - INFO - __main__ - epoch 6/12 done , tot steps 9624 , lr 3.1E-05 , loss 0.56 , qa loss 0.56 , lm loss 0.00 , avg batch size 4.0
2023-04-25 01:58:36,509 - 1:16:28 - 184.6s - INFO - __main__ - progress 6.624 , lr 2.8E-05 , loss 0.507 , qa loss 0.507 , lm loss 0.000 , avg batch size 4.0
2023-04-25 02:00:28,333 - 1:18:20 - 111.8s - INFO - __main__ - epoch 7/12 done , tot steps 11228 , lr 2.6E-05 , loss 0.51 , qa loss 0.51 , lm loss 0.00 , avg batch size 4.0
2023-04-25 02:03:36,444 - 1:21:28 - 188.1s - INFO - __main__ - progress 7.624 , lr 2.3E-05 , loss 0.471 , qa loss 0.471 , lm loss 0.000 , avg batch size 4.0
2023-04-25 02:05:26,093 - 1:23:18 - 109.6s - INFO - __main__ - epoch 8/12 done , tot steps 12832 , lr 2.1E-05 , loss 0.48 , qa loss 0.48 , lm loss 0.00 , avg batch size 4.0
2023-04-25 02:08:28,959 - 1:26:20 - 182.9s - INFO - __main__ - progress 8.624 , lr 1.8E-05 , loss 0.443 , qa loss 0.443 , lm loss 0.000 , avg batch size 4.0
2023-04-25 02:10:20,752 - 1:28:12 - 111.8s - INFO - __main__ - epoch 9/12 done , tot steps 14436 , lr 1.6E-05 , loss 0.44 , qa loss 0.44 , lm loss 0.00 , avg batch size 4.0
2023-04-25 02:13:25,342 - 1:31:17 - 184.6s - INFO - __main__ - progress 9.624 , lr 1.2E-05 , loss 0.417 , qa loss 0.417 , lm loss 0.000 , avg batch size 4.0
2023-04-25 02:15:20,269 - 1:33:12 - 114.9s - INFO - __main__ - epoch 10/12 done , tot steps 16040 , lr 1.0E-05 , loss 0.43 , qa loss 0.43 , lm loss 0.00 , avg batch size 4.0
2023-04-25 02:18:22,325 - 1:36:14 - 182.1s - INFO - __main__ - progress 10.624 , lr 7.2E-06 , loss 0.407 , qa loss 0.407 , lm loss 0.000 , avg batch size 4.0
2023-04-25 02:20:15,761 - 1:38:07 - 113.4s - INFO - __main__ - epoch 11/12 done , tot steps 17644 , lr 5.2E-06 , loss 0.42 , qa loss 0.42 , lm loss 0.00 , avg batch size 4.0
2023-04-25 02:23:17,816 - 1:41:09 - 182.1s - INFO - __main__ - progress 11.624 , lr 2.0E-06 , loss 0.396 , qa loss 0.396 , lm loss 0.000 , avg batch size 4.0
2023-04-25 02:25:11,716 - 1:43:03 - 113.9s - INFO - __main__ - epoch 12/12 done , tot steps 19248 , lr 2.6E-08 , loss 0.40 , qa loss 0.40 , lm loss 0.00 , avg batch size 4.0
2023-04-25 02:25:18,480 - 1:43:10 - 6.8s - INFO - __main__ - start to train { task: ['woz.en'], seq train type: lll }
2023-04-25 02:25:35,345 - 1:43:27 - 16.9s - INFO - utils - writing extra data in ../../model_conf3/gpt2/lll/sst_srl_woz.en_0.0/srl/lm.csv ...
2023-04-25 02:25:35,346 - 1:43:27 - 0.0s - INFO - __main__ - extra training data size: 0
2023-04-25 02:25:35,464 - 1:43:27 - 0.1s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
2023-04-25 02:25:39,880 - 1:43:31 - 4.4s - INFO - __main__ - len of train dataset: 2536 , max train batch size 4 , num of opt steps: 30432
2023-04-25 02:27:05,978 - 1:44:57 - 86.1s - INFO - __main__ - epoch 1/12 done , tot steps 634 , lr 5.7E-05 , loss 5.16 , qa loss 5.16 , lm loss 0.00 , avg batch size 4.0
2023-04-25 02:28:27,087 - 1:46:19 - 81.1s - INFO - __main__ - epoch 2/12 done , tot steps 1268 , lr 5.2E-05 , loss 0.69 , qa loss 0.69 , lm loss 0.00 , avg batch size 4.0
2023-04-25 02:29:48,692 - 1:47:40 - 81.6s - INFO - __main__ - epoch 3/12 done , tot steps 1902 , lr 4.7E-05 , loss 0.48 , qa loss 0.48 , lm loss 0.00 , avg batch size 4.0
2023-04-25 02:31:10,963 - 1:49:02 - 82.3s - INFO - __main__ - epoch 4/12 done , tot steps 2536 , lr 4.2E-05 , loss 0.37 , qa loss 0.37 , lm loss 0.00 , avg batch size 4.0
2023-04-25 02:32:32,440 - 1:50:24 - 81.5s - INFO - __main__ - epoch 5/12 done , tot steps 3170 , lr 3.6E-05 , loss 0.32 , qa loss 0.32 , lm loss 0.00 , avg batch size 4.0
2023-04-25 02:33:52,802 - 1:51:44 - 80.4s - INFO - __main__ - epoch 6/12 done , tot steps 3804 , lr 3.1E-05 , loss 0.28 , qa loss 0.28 , lm loss 0.00 , avg batch size 4.0
2023-04-25 02:35:14,544 - 1:53:06 - 81.7s - INFO - __main__ - epoch 7/12 done , tot steps 4438 , lr 2.6E-05 , loss 0.25 , qa loss 0.25 , lm loss 0.00 , avg batch size 4.0
2023-04-25 02:36:35,534 - 1:54:27 - 81.0s - INFO - __main__ - epoch 8/12 done , tot steps 5072 , lr 2.1E-05 , loss 0.24 , qa loss 0.24 , lm loss 0.00 , avg batch size 4.0
2023-04-25 02:37:57,231 - 1:55:49 - 81.7s - INFO - __main__ - epoch 9/12 done , tot steps 5706 , lr 1.6E-05 , loss 0.22 , qa loss 0.22 , lm loss 0.00 , avg batch size 4.0
2023-04-25 02:39:18,743 - 1:57:10 - 81.5s - INFO - __main__ - epoch 10/12 done , tot steps 6340 , lr 1.0E-05 , loss 0.21 , qa loss 0.21 , lm loss 0.00 , avg batch size 4.0
2023-04-25 02:40:40,373 - 1:58:32 - 81.6s - INFO - __main__ - epoch 11/12 done , tot steps 6974 , lr 5.2E-06 , loss 0.21 , qa loss 0.21 , lm loss 0.00 , avg batch size 4.0
2023-04-25 02:42:02,610 - 1:59:54 - 82.2s - INFO - __main__ - epoch 12/12 done , tot steps 7608 , lr 2.5E-08 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
