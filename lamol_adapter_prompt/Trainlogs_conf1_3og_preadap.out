Not all gpus support fp16 training! Will use fp32 instead.
2023-04-24 16:00:46,311 - 0:00:05 - 0.0s - INFO - __main__ - args = Namespace(REG_TYPE_KEYS=['mas', 'ewc'], adam_epsilon=0.0001, add_task_tokens=False, data_dir='../../data', debug=False, decay_style='linear', device_ids=[0, 1, 2, 3, 4, 5], dynamic_epochs=False, fp32=True, gen_lm_sample_percentage=0.0, learning_rate=6.25e-05, lm_lambda=0.0, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[32429.600000000002, 42158.48, 42158.48, 42158.48, 42158.48, 42158.48], min_batch_size=4, min_n_steps=1500, model_dir_root='../../model_conf1/gpt2/lll/sst_srl_woz.en_0.0', model_name='gpt2', n_gpus=6, n_train_epochs={'sst': 12, 'srl': 12, 'woz.en': 12}, n_warmup_ratio=0.005, n_workers=75, qp_margin=0.5, real_sample=False, reg_lambda=1.0, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[11350, 14755, 14755, 14755, 14755, 14755], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[11350, 14755, 14755, 14755, 14755, 14755], unbound=0, use_sep=False, weight_decay=0.01)
2023-04-24 16:00:46,311 - 0:00:05 - 0.0s - INFO - __main__ - start to train { task: ['sst'], seq train type: lll }
2023-04-24 16:00:46,311 - 0:00:05 - 0.0s - INFO - __main__ - extra training data size: 0
2023-04-24 16:00:53,271 - 0:00:12 - 7.0s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
2023-04-24 16:00:55,490 - 0:00:14 - 2.2s - INFO - __main__ - len of train dataset: 6920 , max train batch size 4 , num of opt steps: 83040
/home/student/2021/cs21mtech11006/anaconda3/envs/lamol/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/home/student/2021/cs21mtech11006/anaconda3/envs/lamol/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
2023-04-24 16:03:00,725 - 0:02:19 - 125.2s - INFO - __main__ - progress 0.578 , lr 6.0E-05 , loss 2.710 , qa loss 2.710 , lm loss 0.000 , avg batch size 4.0
2023-04-24 16:04:32,018 - 0:03:51 - 91.3s - INFO - __main__ - epoch 1/12 done , tot steps 1730 , lr 5.7E-05 , loss 1.69 , qa loss 1.69 , lm loss 0.00 , avg batch size 4.0
2023-04-24 16:06:38,667 - 0:05:57 - 126.6s - INFO - __main__ - progress 1.578 , lr 5.4E-05 , loss 0.280 , qa loss 0.280 , lm loss 0.000 , avg batch size 4.0
2023-04-24 16:08:16,178 - 0:07:35 - 97.5s - INFO - __main__ - epoch 2/12 done , tot steps 3460 , lr 5.2E-05 , loss 0.27 , qa loss 0.27 , lm loss 0.00 , avg batch size 4.0
2023-04-24 16:10:20,054 - 0:09:39 - 123.9s - INFO - __main__ - progress 2.578 , lr 4.9E-05 , loss 0.256 , qa loss 0.256 , lm loss 0.000 , avg batch size 4.0
2023-04-24 16:11:50,183 - 0:11:09 - 90.1s - INFO - __main__ - epoch 3/12 done , tot steps 5190 , lr 4.7E-05 , loss 0.25 , qa loss 0.25 , lm loss 0.00 , avg batch size 4.0
2023-04-24 16:13:53,054 - 0:13:12 - 122.9s - INFO - __main__ - progress 3.578 , lr 4.4E-05 , loss 0.246 , qa loss 0.246 , lm loss 0.000 , avg batch size 4.0
2023-04-24 16:15:24,051 - 0:14:43 - 91.0s - INFO - __main__ - epoch 4/12 done , tot steps 6920 , lr 4.2E-05 , loss 0.24 , qa loss 0.24 , lm loss 0.00 , avg batch size 4.0
2023-04-24 16:17:32,873 - 0:16:51 - 128.8s - INFO - __main__ - progress 4.578 , lr 3.9E-05 , loss 0.228 , qa loss 0.228 , lm loss 0.000 , avg batch size 4.0
2023-04-24 16:19:07,422 - 0:18:26 - 94.5s - INFO - __main__ - epoch 5/12 done , tot steps 8650 , lr 3.6E-05 , loss 0.23 , qa loss 0.23 , lm loss 0.00 , avg batch size 4.0
2023-04-24 16:21:17,420 - 0:20:36 - 130.0s - INFO - __main__ - progress 5.578 , lr 3.3E-05 , loss 0.209 , qa loss 0.209 , lm loss 0.000 , avg batch size 4.0
2023-04-24 16:22:52,260 - 0:22:11 - 94.8s - INFO - __main__ - epoch 6/12 done , tot steps 10380 , lr 3.1E-05 , loss 0.22 , qa loss 0.22 , lm loss 0.00 , avg batch size 4.0
2023-04-24 16:25:03,061 - 0:24:22 - 130.8s - INFO - __main__ - progress 6.578 , lr 2.8E-05 , loss 0.207 , qa loss 0.207 , lm loss 0.000 , avg batch size 4.0
2023-04-24 16:26:32,840 - 0:25:51 - 89.8s - INFO - __main__ - epoch 7/12 done , tot steps 12110 , lr 2.6E-05 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-04-24 16:28:34,761 - 0:27:53 - 121.9s - INFO - __main__ - progress 7.578 , lr 2.3E-05 , loss 0.198 , qa loss 0.198 , lm loss 0.000 , avg batch size 4.0
2023-04-24 16:30:04,025 - 0:29:23 - 89.3s - INFO - __main__ - epoch 8/12 done , tot steps 13840 , lr 2.1E-05 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-04-24 16:32:07,713 - 0:31:26 - 123.7s - INFO - __main__ - progress 8.578 , lr 1.8E-05 , loss 0.195 , qa loss 0.195 , lm loss 0.000 , avg batch size 4.0
2023-04-24 16:33:39,060 - 0:32:58 - 91.3s - INFO - __main__ - epoch 9/12 done , tot steps 15570 , lr 1.6E-05 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-04-24 16:35:42,772 - 0:35:01 - 123.7s - INFO - __main__ - progress 9.578 , lr 1.3E-05 , loss 0.188 , qa loss 0.188 , lm loss 0.000 , avg batch size 4.0
2023-04-24 16:37:12,063 - 0:36:31 - 89.3s - INFO - __main__ - epoch 10/12 done , tot steps 17300 , lr 1.0E-05 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-04-24 16:39:13,514 - 0:38:32 - 121.5s - INFO - __main__ - progress 10.578 , lr 7.4E-06 , loss 0.173 , qa loss 0.173 , lm loss 0.000 , avg batch size 4.0
2023-04-24 16:40:43,210 - 0:40:02 - 89.7s - INFO - __main__ - epoch 11/12 done , tot steps 19030 , lr 5.2E-06 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
2023-04-24 16:42:44,911 - 0:42:03 - 121.7s - INFO - __main__ - progress 11.578 , lr 2.2E-06 , loss 0.175 , qa loss 0.175 , lm loss 0.000 , avg batch size 4.0
2023-04-24 16:44:15,657 - 0:43:34 - 90.7s - INFO - __main__ - epoch 12/12 done , tot steps 20760 , lr 2.6E-08 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-04-24 16:44:22,547 - 0:43:41 - 6.9s - INFO - __main__ - start to train { task: ['srl'], seq train type: lll }
2023-04-24 16:44:25,973 - 0:43:45 - 3.4s - INFO - utils - writing extra data in ../../model_conf1/gpt2/lll/sst_srl_woz.en_0.0/sst/lm.csv ...
2023-04-24 16:44:25,986 - 0:43:45 - 0.0s - INFO - __main__ - extra training data size: 0
2023-04-24 16:44:26,111 - 0:43:45 - 0.1s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[sst]
The task with which model is saved sst
2023-04-24 16:44:28,985 - 0:43:48 - 2.9s - INFO - __main__ - len of train dataset: 6414 , max train batch size 4 , num of opt steps: 76968
2023-04-24 16:47:34,174 - 0:46:53 - 185.2s - INFO - __main__ - progress 0.624 , lr 5.9E-05 , loss 6.241 , qa loss 6.241 , lm loss 0.000 , avg batch size 4.0
2023-04-24 16:49:23,848 - 0:48:42 - 109.7s - INFO - __main__ - epoch 1/12 done , tot steps 1604 , lr 5.7E-05 , loss 4.54 , qa loss 4.54 , lm loss 0.00 , avg batch size 4.0
2023-04-24 16:52:25,453 - 0:51:44 - 181.6s - INFO - __main__ - progress 1.624 , lr 5.4E-05 , loss 1.180 , qa loss 1.180 , lm loss 0.000 , avg batch size 4.0
2023-04-24 16:54:22,932 - 0:53:41 - 117.5s - INFO - __main__ - epoch 2/12 done , tot steps 3208 , lr 5.2E-05 , loss 1.10 , qa loss 1.10 , lm loss 0.00 , avg batch size 4.0
2023-04-24 16:57:25,174 - 0:56:44 - 182.2s - INFO - __main__ - progress 2.624 , lr 4.9E-05 , loss 0.856 , qa loss 0.856 , lm loss 0.000 , avg batch size 4.0
2023-04-24 16:59:18,769 - 0:58:37 - 113.6s - INFO - __main__ - epoch 3/12 done , tot steps 4812 , lr 4.7E-05 , loss 0.83 , qa loss 0.83 , lm loss 0.00 , avg batch size 4.0
2023-04-24 17:02:23,461 - 1:01:42 - 184.7s - INFO - __main__ - progress 3.624 , lr 4.4E-05 , loss 0.739 , qa loss 0.739 , lm loss 0.000 , avg batch size 4.0
2023-04-24 17:04:15,676 - 1:03:34 - 112.2s - INFO - __main__ - epoch 4/12 done , tot steps 6416 , lr 4.2E-05 , loss 0.73 , qa loss 0.73 , lm loss 0.00 , avg batch size 4.0
2023-04-24 17:07:13,992 - 1:06:33 - 178.3s - INFO - __main__ - progress 4.624 , lr 3.8E-05 , loss 0.653 , qa loss 0.653 , lm loss 0.000 , avg batch size 4.0
2023-04-24 17:09:10,031 - 1:08:29 - 116.0s - INFO - __main__ - epoch 5/12 done , tot steps 8020 , lr 3.6E-05 , loss 0.64 , qa loss 0.64 , lm loss 0.00 , avg batch size 4.0
2023-04-24 17:12:13,195 - 1:11:32 - 183.2s - INFO - __main__ - progress 5.624 , lr 3.3E-05 , loss 0.594 , qa loss 0.594 , lm loss 0.000 , avg batch size 4.0
2023-04-24 17:14:06,245 - 1:13:25 - 113.1s - INFO - __main__ - epoch 6/12 done , tot steps 9624 , lr 3.1E-05 , loss 0.59 , qa loss 0.59 , lm loss 0.00 , avg batch size 4.0
2023-04-24 17:17:11,189 - 1:16:30 - 184.9s - INFO - __main__ - progress 6.624 , lr 2.8E-05 , loss 0.542 , qa loss 0.542 , lm loss 0.000 , avg batch size 4.0
2023-04-24 17:19:02,941 - 1:18:21 - 111.8s - INFO - __main__ - epoch 7/12 done , tot steps 11228 , lr 2.6E-05 , loss 0.54 , qa loss 0.54 , lm loss 0.00 , avg batch size 4.0
2023-04-24 17:22:05,193 - 1:21:24 - 182.3s - INFO - __main__ - progress 7.624 , lr 2.3E-05 , loss 0.501 , qa loss 0.501 , lm loss 0.000 , avg batch size 4.0
2023-04-24 17:23:54,306 - 1:23:13 - 109.1s - INFO - __main__ - epoch 8/12 done , tot steps 12832 , lr 2.1E-05 , loss 0.51 , qa loss 0.51 , lm loss 0.00 , avg batch size 4.0
2023-04-24 17:26:59,215 - 1:26:18 - 184.9s - INFO - __main__ - progress 8.624 , lr 1.8E-05 , loss 0.487 , qa loss 0.487 , lm loss 0.000 , avg batch size 4.0
2023-04-24 17:28:49,522 - 1:28:08 - 110.3s - INFO - __main__ - epoch 9/12 done , tot steps 14436 , lr 1.6E-05 , loss 0.47 , qa loss 0.47 , lm loss 0.00 , avg batch size 4.0
2023-04-24 17:31:49,585 - 1:31:08 - 180.1s - INFO - __main__ - progress 9.624 , lr 1.2E-05 , loss 0.446 , qa loss 0.446 , lm loss 0.000 , avg batch size 4.0
2023-04-24 17:33:42,712 - 1:33:01 - 113.1s - INFO - __main__ - epoch 10/12 done , tot steps 16040 , lr 1.0E-05 , loss 0.45 , qa loss 0.45 , lm loss 0.00 , avg batch size 4.0
2023-04-24 17:36:45,423 - 1:36:04 - 182.7s - INFO - __main__ - progress 10.624 , lr 7.2E-06 , loss 0.430 , qa loss 0.430 , lm loss 0.000 , avg batch size 4.0
2023-04-24 17:38:36,096 - 1:37:55 - 110.7s - INFO - __main__ - epoch 11/12 done , tot steps 17644 , lr 5.2E-06 , loss 0.43 , qa loss 0.43 , lm loss 0.00 , avg batch size 4.0
2023-04-24 17:41:39,446 - 1:40:58 - 183.3s - INFO - __main__ - progress 11.624 , lr 2.0E-06 , loss 0.417 , qa loss 0.417 , lm loss 0.000 , avg batch size 4.0
2023-04-24 17:43:33,087 - 1:42:52 - 113.6s - INFO - __main__ - epoch 12/12 done , tot steps 19248 , lr 2.6E-08 , loss 0.42 , qa loss 0.42 , lm loss 0.00 , avg batch size 4.0
2023-04-24 17:43:39,945 - 1:42:58 - 6.9s - INFO - __main__ - start to train { task: ['woz.en'], seq train type: lll }
2023-04-24 17:43:41,997 - 1:43:01 - 2.1s - INFO - utils - writing extra data in ../../model_conf1/gpt2/lll/sst_srl_woz.en_0.0/srl/lm.csv ...
2023-04-24 17:43:41,998 - 1:43:01 - 0.0s - INFO - __main__ - extra training data size: 0
2023-04-24 17:43:42,118 - 1:43:01 - 0.1s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[sst]
The task with which model is saved sst
2023-04-24 17:43:44,722 - 1:43:03 - 2.6s - INFO - __main__ - len of train dataset: 2536 , max train batch size 4 , num of opt steps: 30432
2023-04-24 17:45:04,490 - 1:44:23 - 79.8s - INFO - __main__ - epoch 1/12 done , tot steps 634 , lr 5.7E-05 , loss 5.02 , qa loss 5.02 , lm loss 0.00 , avg batch size 4.0
2023-04-24 17:46:29,351 - 1:45:48 - 84.9s - INFO - __main__ - epoch 2/12 done , tot steps 1268 , lr 5.2E-05 , loss 0.85 , qa loss 0.85 , lm loss 0.00 , avg batch size 4.0
2023-04-24 17:47:48,216 - 1:47:07 - 78.9s - INFO - __main__ - epoch 3/12 done , tot steps 1902 , lr 4.7E-05 , loss 0.53 , qa loss 0.53 , lm loss 0.00 , avg batch size 4.0
2023-04-24 17:49:08,411 - 1:48:27 - 80.2s - INFO - __main__ - epoch 4/12 done , tot steps 2536 , lr 4.2E-05 , loss 0.41 , qa loss 0.41 , lm loss 0.00 , avg batch size 4.0
2023-04-24 17:50:28,731 - 1:49:47 - 80.3s - INFO - __main__ - epoch 5/12 done , tot steps 3170 , lr 3.6E-05 , loss 0.34 , qa loss 0.34 , lm loss 0.00 , avg batch size 4.0
2023-04-24 17:51:48,029 - 1:51:07 - 79.3s - INFO - __main__ - epoch 6/12 done , tot steps 3804 , lr 3.1E-05 , loss 0.29 , qa loss 0.29 , lm loss 0.00 , avg batch size 4.0
2023-04-24 17:53:07,826 - 1:52:26 - 79.8s - INFO - __main__ - epoch 7/12 done , tot steps 4438 , lr 2.6E-05 , loss 0.27 , qa loss 0.27 , lm loss 0.00 , avg batch size 4.0
2023-04-24 17:54:26,667 - 1:53:45 - 78.8s - INFO - __main__ - epoch 8/12 done , tot steps 5072 , lr 2.1E-05 , loss 0.24 , qa loss 0.24 , lm loss 0.00 , avg batch size 4.0
2023-04-24 17:55:46,742 - 1:55:05 - 80.1s - INFO - __main__ - epoch 9/12 done , tot steps 5706 , lr 1.6E-05 , loss 0.23 , qa loss 0.23 , lm loss 0.00 , avg batch size 4.0
2023-04-24 17:57:06,630 - 1:56:25 - 79.9s - INFO - __main__ - epoch 10/12 done , tot steps 6340 , lr 1.0E-05 , loss 0.21 , qa loss 0.21 , lm loss 0.00 , avg batch size 4.0
2023-04-24 17:58:25,628 - 1:57:44 - 79.0s - INFO - __main__ - epoch 11/12 done , tot steps 6974 , lr 5.2E-06 , loss 0.21 , qa loss 0.21 , lm loss 0.00 , avg batch size 4.0
2023-04-24 17:59:45,964 - 1:59:04 - 80.3s - INFO - __main__ - epoch 12/12 done , tot steps 7608 , lr 2.5E-08 , loss 0.21 , qa loss 0.21 , lm loss 0.00 , avg batch size 4.0
The current active adapter is Stack[sst]
The task with which model is saved sst
Wall Execution time: 01:59:06
CPU Execution time: 02:00:15
