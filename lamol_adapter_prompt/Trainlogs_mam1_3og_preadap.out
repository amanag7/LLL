Not all gpus support fp16 training! Will use fp32 instead.
2023-05-31 09:55:09,977 - 0:00:06 - 0.0s - INFO - __main__ - args = Namespace(REG_TYPE_KEYS=['mas', 'ewc'], adam_epsilon=0.0001, add_task_tokens=False, data_dir='../../data', debug=False, decay_style='linear', device_ids=[0, 1, 2, 3], dynamic_epochs=False, fp32=True, gen_lm_sample_percentage=0.0, learning_rate=6.25e-05, lm_lambda=0.0, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[35672.56, 42158.48, 42158.48, 42158.48], min_batch_size=4, min_n_steps=1500, model_dir_root='../../model_mam1/gpt2/lll/sst_srl_woz.en_0.0', model_name='gpt2', n_gpus=4, n_train_epochs={'sst': 20, 'srl': 20, 'woz.en': 20}, n_warmup_ratio=0.005, n_workers=75, qp_margin=0.5, real_sample=False, reg_lambda=1.0, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[12485, 14755, 14755, 14755], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[12485, 14755, 14755, 14755], unbound=0, use_sep=False, weight_decay=0.01)
2023-05-31 09:55:09,977 - 0:00:06 - 0.0s - INFO - __main__ - start to train { task: ['sst'], seq train type: lll }
2023-05-31 09:55:09,978 - 0:00:06 - 0.0s - INFO - __main__ - extra training data size: 0
2023-05-31 09:55:13,056 - 0:00:09 - 3.1s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
2023-05-31 09:55:15,417 - 0:00:12 - 2.4s - INFO - __main__ - len of train dataset: 6920 , max train batch size 4 , num of opt steps: 138400
/home/student/2021/cs21mtech11006/anaconda3/envs/lamol/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/home/student/2021/cs21mtech11006/anaconda3/envs/lamol/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
2023-05-31 09:57:54,105 - 0:02:50 - 158.7s - INFO - __main__ - progress 0.578 , lr 6.1E-05 , loss 1.330 , qa loss 1.330 , lm loss 0.000 , avg batch size 4.0
2023-05-31 09:59:51,155 - 0:04:47 - 117.1s - INFO - __main__ - epoch 1/20 done , tot steps 1730 , lr 5.9E-05 , loss 0.89 , qa loss 0.89 , lm loss 0.00 , avg batch size 4.0
2023-05-31 10:02:27,586 - 0:07:24 - 156.4s - INFO - __main__ - progress 1.578 , lr 5.8E-05 , loss 0.259 , qa loss 0.259 , lm loss 0.000 , avg batch size 4.0
2023-05-31 10:04:20,724 - 0:09:17 - 113.1s - INFO - __main__ - epoch 2/20 done , tot steps 3460 , lr 5.6E-05 , loss 0.26 , qa loss 0.26 , lm loss 0.00 , avg batch size 4.0
2023-05-31 10:06:57,945 - 0:11:54 - 157.2s - INFO - __main__ - progress 2.578 , lr 5.4E-05 , loss 0.220 , qa loss 0.220 , lm loss 0.000 , avg batch size 4.0
2023-05-31 10:08:50,629 - 0:13:47 - 112.7s - INFO - __main__ - epoch 3/20 done , tot steps 5190 , lr 5.3E-05 , loss 0.21 , qa loss 0.21 , lm loss 0.00 , avg batch size 4.0
2023-05-31 10:11:23,361 - 0:16:20 - 152.7s - INFO - __main__ - progress 3.578 , lr 5.1E-05 , loss 0.183 , qa loss 0.183 , lm loss 0.000 , avg batch size 4.0
2023-05-31 10:13:19,451 - 0:18:16 - 116.1s - INFO - __main__ - epoch 4/20 done , tot steps 6920 , lr 5.0E-05 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-05-31 10:15:53,375 - 0:20:50 - 153.9s - INFO - __main__ - progress 4.578 , lr 4.8E-05 , loss 0.160 , qa loss 0.160 , lm loss 0.000 , avg batch size 4.0
2023-05-31 10:17:48,031 - 0:22:44 - 114.7s - INFO - __main__ - epoch 5/20 done , tot steps 8650 , lr 4.7E-05 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-05-31 10:20:22,277 - 0:25:18 - 154.2s - INFO - __main__ - progress 5.578 , lr 4.5E-05 , loss 0.128 , qa loss 0.128 , lm loss 0.000 , avg batch size 4.0
2023-05-31 10:22:16,052 - 0:27:12 - 113.8s - INFO - __main__ - epoch 6/20 done , tot steps 10380 , lr 4.4E-05 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-05-31 10:24:50,946 - 0:29:47 - 154.9s - INFO - __main__ - progress 6.578 , lr 4.2E-05 , loss 0.121 , qa loss 0.121 , lm loss 0.000 , avg batch size 4.0
2023-05-31 10:26:45,290 - 0:31:42 - 114.3s - INFO - __main__ - epoch 7/20 done , tot steps 12110 , lr 4.1E-05 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2023-05-31 10:29:21,267 - 0:34:17 - 156.0s - INFO - __main__ - progress 7.578 , lr 3.9E-05 , loss 0.093 , qa loss 0.093 , lm loss 0.000 , avg batch size 4.0
2023-05-31 10:31:10,901 - 0:36:07 - 109.6s - INFO - __main__ - epoch 8/20 done , tot steps 13840 , lr 3.8E-05 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 4.0
2023-05-31 10:33:48,222 - 0:38:44 - 157.3s - INFO - __main__ - progress 8.578 , lr 3.6E-05 , loss 0.079 , qa loss 0.079 , lm loss 0.000 , avg batch size 4.0
2023-05-31 10:35:39,170 - 0:40:35 - 110.9s - INFO - __main__ - epoch 9/20 done , tot steps 15570 , lr 3.4E-05 , loss 0.08 , qa loss 0.08 , lm loss 0.00 , avg batch size 4.0
2023-05-31 10:38:15,655 - 0:43:12 - 156.5s - INFO - __main__ - progress 9.578 , lr 3.3E-05 , loss 0.061 , qa loss 0.061 , lm loss 0.000 , avg batch size 4.0
2023-05-31 10:40:07,258 - 0:45:03 - 111.6s - INFO - __main__ - epoch 10/20 done , tot steps 17300 , lr 3.1E-05 , loss 0.07 , qa loss 0.07 , lm loss 0.00 , avg batch size 4.0
2023-05-31 10:42:43,962 - 0:47:40 - 156.7s - INFO - __main__ - progress 10.578 , lr 2.9E-05 , loss 0.050 , qa loss 0.050 , lm loss 0.000 , avg batch size 4.0
2023-05-31 10:44:34,993 - 0:49:31 - 111.0s - INFO - __main__ - epoch 11/20 done , tot steps 19030 , lr 2.8E-05 , loss 0.05 , qa loss 0.05 , lm loss 0.00 , avg batch size 4.0
2023-05-31 10:47:07,147 - 0:52:03 - 152.2s - INFO - __main__ - progress 11.578 , lr 2.6E-05 , loss 0.044 , qa loss 0.044 , lm loss 0.000 , avg batch size 4.0
2023-05-31 10:49:09,498 - 0:54:06 - 122.4s - INFO - __main__ - epoch 12/20 done , tot steps 20760 , lr 2.5E-05 , loss 0.05 , qa loss 0.05 , lm loss 0.00 , avg batch size 4.0
2023-05-31 10:52:04,957 - 0:57:01 - 175.5s - INFO - __main__ - progress 12.578 , lr 2.3E-05 , loss 0.035 , qa loss 0.035 , lm loss 0.000 , avg batch size 4.0
2023-05-31 10:54:07,869 - 0:59:04 - 122.9s - INFO - __main__ - epoch 13/20 done , tot steps 22490 , lr 2.2E-05 , loss 0.04 , qa loss 0.04 , lm loss 0.00 , avg batch size 4.0
2023-05-31 10:57:04,810 - 1:02:01 - 176.9s - INFO - __main__ - progress 13.578 , lr 2.0E-05 , loss 0.027 , qa loss 0.027 , lm loss 0.000 , avg batch size 4.0
2023-05-31 10:59:16,389 - 1:04:13 - 131.6s - INFO - __main__ - epoch 14/20 done , tot steps 24220 , lr 1.9E-05 , loss 0.03 , qa loss 0.03 , lm loss 0.00 , avg batch size 4.0
2023-05-31 11:02:08,157 - 1:07:04 - 171.8s - INFO - __main__ - progress 14.578 , lr 1.7E-05 , loss 0.022 , qa loss 0.022 , lm loss 0.000 , avg batch size 4.0
2023-05-31 11:04:17,998 - 1:09:14 - 129.8s - INFO - __main__ - epoch 15/20 done , tot steps 25950 , lr 1.6E-05 , loss 0.02 , qa loss 0.02 , lm loss 0.00 , avg batch size 4.0
2023-05-31 11:07:08,978 - 1:12:05 - 171.0s - INFO - __main__ - progress 15.578 , lr 1.4E-05 , loss 0.022 , qa loss 0.022 , lm loss 0.000 , avg batch size 4.0
2023-05-31 11:09:20,815 - 1:14:17 - 131.8s - INFO - __main__ - epoch 16/20 done , tot steps 27680 , lr 1.3E-05 , loss 0.02 , qa loss 0.02 , lm loss 0.00 , avg batch size 4.0
2023-05-31 11:12:18,051 - 1:17:14 - 177.2s - INFO - __main__ - progress 16.578 , lr 1.1E-05 , loss 0.017 , qa loss 0.017 , lm loss 0.000 , avg batch size 4.0
2023-05-31 11:14:24,244 - 1:19:20 - 126.2s - INFO - __main__ - epoch 17/20 done , tot steps 29410 , lr 9.4E-06 , loss 0.01 , qa loss 0.01 , lm loss 0.00 , avg batch size 4.0
2023-05-31 11:17:20,668 - 1:22:17 - 176.4s - INFO - __main__ - progress 17.578 , lr 7.6E-06 , loss 0.012 , qa loss 0.012 , lm loss 0.000 , avg batch size 4.0
2023-05-31 11:19:31,250 - 1:24:27 - 130.6s - INFO - __main__ - epoch 18/20 done , tot steps 31140 , lr 6.3E-06 , loss 0.01 , qa loss 0.01 , lm loss 0.00 , avg batch size 4.0
2023-05-31 11:22:23,841 - 1:27:20 - 172.6s - INFO - __main__ - progress 18.578 , lr 4.5E-06 , loss 0.014 , qa loss 0.014 , lm loss 0.000 , avg batch size 4.0
2023-05-31 11:24:34,236 - 1:29:30 - 130.4s - INFO - __main__ - epoch 19/20 done , tot steps 32870 , lr 3.1E-06 , loss 0.01 , qa loss 0.01 , lm loss 0.00 , avg batch size 4.0
2023-05-31 11:27:23,652 - 1:32:20 - 169.4s - INFO - __main__ - progress 19.578 , lr 1.3E-06 , loss 0.013 , qa loss 0.013 , lm loss 0.000 , avg batch size 4.0
2023-05-31 11:29:35,779 - 1:34:32 - 132.1s - INFO - __main__ - epoch 20/20 done , tot steps 34600 , lr 1.5E-08 , loss 0.01 , qa loss 0.01 , lm loss 0.00 , avg batch size 4.0
2023-05-31 11:29:43,829 - 1:34:40 - 8.0s - INFO - __main__ - start to train { task: ['srl'], seq train type: lll }
2023-05-31 11:29:47,233 - 1:34:43 - 3.4s - INFO - utils - writing extra data in ../../model_mam1/gpt2/lll/sst_srl_woz.en_0.0/sst/lm.csv ...
2023-05-31 11:29:47,234 - 1:34:43 - 0.0s - INFO - __main__ - extra training data size: 0
2023-05-31 11:29:47,344 - 1:34:44 - 0.1s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[sst]
The task with which model is saved sst
2023-05-31 11:29:49,896 - 1:34:46 - 2.6s - INFO - __main__ - len of train dataset: 6414 , max train batch size 4 , num of opt steps: 128280
2023-05-31 11:34:39,742 - 1:39:36 - 289.8s - INFO - __main__ - progress 0.624 , lr 6.1E-05 , loss 2.419 , qa loss 2.419 , lm loss 0.000 , avg batch size 4.0
2023-05-31 11:37:44,719 - 1:42:41 - 185.0s - INFO - __main__ - epoch 1/20 done , tot steps 1604 , lr 5.9E-05 , loss 1.81 , qa loss 1.81 , lm loss 0.00 , avg batch size 4.0
2023-05-31 11:42:33,506 - 1:47:30 - 288.8s - INFO - __main__ - progress 1.624 , lr 5.7E-05 , loss 0.657 , qa loss 0.657 , lm loss 0.000 , avg batch size 4.0
2023-05-31 11:45:33,863 - 1:50:30 - 180.4s - INFO - __main__ - epoch 2/20 done , tot steps 3208 , lr 5.6E-05 , loss 0.63 , qa loss 0.63 , lm loss 0.00 , avg batch size 4.0
2023-05-31 11:50:23,409 - 1:55:20 - 289.5s - INFO - __main__ - progress 2.624 , lr 5.4E-05 , loss 0.488 , qa loss 0.488 , lm loss 0.000 , avg batch size 4.0
2023-05-31 11:53:25,178 - 1:58:21 - 181.8s - INFO - __main__ - epoch 3/20 done , tot steps 4812 , lr 5.3E-05 , loss 0.49 , qa loss 0.49 , lm loss 0.00 , avg batch size 4.0
2023-05-31 11:58:16,572 - 2:03:13 - 291.4s - INFO - __main__ - progress 3.624 , lr 5.1E-05 , loss 0.405 , qa loss 0.405 , lm loss 0.000 , avg batch size 4.0
2023-05-31 12:01:14,414 - 2:06:11 - 177.8s - INFO - __main__ - epoch 4/20 done , tot steps 6416 , lr 5.0E-05 , loss 0.41 , qa loss 0.41 , lm loss 0.00 , avg batch size 4.0
2023-05-31 12:06:10,273 - 2:11:06 - 295.9s - INFO - __main__ - progress 4.624 , lr 4.8E-05 , loss 0.341 , qa loss 0.341 , lm loss 0.000 , avg batch size 4.0
2023-05-31 12:09:06,810 - 2:14:03 - 176.5s - INFO - __main__ - epoch 5/20 done , tot steps 8020 , lr 4.7E-05 , loss 0.35 , qa loss 0.35 , lm loss 0.00 , avg batch size 4.0
2023-05-31 12:13:56,825 - 2:18:53 - 290.0s - INFO - __main__ - progress 5.624 , lr 4.5E-05 , loss 0.296 , qa loss 0.296 , lm loss 0.000 , avg batch size 4.0
2023-05-31 12:16:55,437 - 2:21:52 - 178.6s - INFO - __main__ - epoch 6/20 done , tot steps 9624 , lr 4.4E-05 , loss 0.30 , qa loss 0.30 , lm loss 0.00 , avg batch size 4.0
2023-05-31 12:21:46,895 - 2:26:43 - 291.5s - INFO - __main__ - progress 6.624 , lr 4.2E-05 , loss 0.253 , qa loss 0.253 , lm loss 0.000 , avg batch size 4.0
2023-05-31 12:24:43,177 - 2:29:39 - 176.3s - INFO - __main__ - epoch 7/20 done , tot steps 11228 , lr 4.1E-05 , loss 0.25 , qa loss 0.25 , lm loss 0.00 , avg batch size 4.0
2023-05-31 12:29:42,466 - 2:34:39 - 299.3s - INFO - __main__ - progress 7.624 , lr 3.9E-05 , loss 0.220 , qa loss 0.220 , lm loss 0.000 , avg batch size 4.0
2023-05-31 12:32:35,870 - 2:37:32 - 173.4s - INFO - __main__ - epoch 8/20 done , tot steps 12832 , lr 3.8E-05 , loss 0.22 , qa loss 0.22 , lm loss 0.00 , avg batch size 4.0
2023-05-31 12:37:30,894 - 2:42:27 - 295.0s - INFO - __main__ - progress 8.624 , lr 3.6E-05 , loss 0.197 , qa loss 0.197 , lm loss 0.000 , avg batch size 4.0
2023-05-31 12:40:29,471 - 2:45:26 - 178.6s - INFO - __main__ - epoch 9/20 done , tot steps 14436 , lr 3.4E-05 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-05-31 12:45:19,017 - 2:50:15 - 289.5s - INFO - __main__ - progress 9.624 , lr 3.2E-05 , loss 0.169 , qa loss 0.169 , lm loss 0.000 , avg batch size 4.0
2023-05-31 12:48:16,387 - 2:53:13 - 177.4s - INFO - __main__ - epoch 10/20 done , tot steps 16040 , lr 3.1E-05 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
2023-05-31 12:53:35,822 - 2:58:32 - 319.4s - INFO - __main__ - progress 10.624 , lr 2.9E-05 , loss 0.148 , qa loss 0.148 , lm loss 0.000 , avg batch size 4.0
2023-05-31 12:56:52,603 - 3:01:49 - 196.8s - INFO - __main__ - epoch 11/20 done , tot steps 17644 , lr 2.8E-05 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-05-31 13:02:20,643 - 3:07:17 - 328.0s - INFO - __main__ - progress 11.624 , lr 2.6E-05 , loss 0.123 , qa loss 0.123 , lm loss 0.000 , avg batch size 4.0
2023-05-31 13:05:44,184 - 3:10:40 - 203.5s - INFO - __main__ - epoch 12/20 done , tot steps 19248 , lr 2.5E-05 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-05-31 13:10:52,327 - 3:15:49 - 308.1s - INFO - __main__ - progress 12.624 , lr 2.3E-05 , loss 0.106 , qa loss 0.106 , lm loss 0.000 , avg batch size 4.0
2023-05-31 13:13:36,323 - 3:18:33 - 164.0s - INFO - __main__ - epoch 13/20 done , tot steps 20852 , lr 2.2E-05 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-05-31 13:18:05,858 - 3:23:02 - 269.5s - INFO - __main__ - progress 13.624 , lr 2.0E-05 , loss 0.099 , qa loss 0.099 , lm loss 0.000 , avg batch size 4.0
2023-05-31 13:20:57,673 - 3:25:54 - 171.8s - INFO - __main__ - epoch 14/20 done , tot steps 22456 , lr 1.9E-05 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 4.0
2023-05-31 13:25:29,865 - 3:30:26 - 272.2s - INFO - __main__ - progress 14.624 , lr 1.7E-05 , loss 0.087 , qa loss 0.087 , lm loss 0.000 , avg batch size 4.0
2023-05-31 13:28:15,783 - 3:33:12 - 165.9s - INFO - __main__ - epoch 15/20 done , tot steps 24060 , lr 1.6E-05 , loss 0.09 , qa loss 0.09 , lm loss 0.00 , avg batch size 4.0
2023-05-31 13:32:50,099 - 3:37:46 - 274.3s - INFO - __main__ - progress 15.624 , lr 1.4E-05 , loss 0.079 , qa loss 0.079 , lm loss 0.000 , avg batch size 4.0
2023-05-31 13:35:34,138 - 3:40:30 - 164.0s - INFO - __main__ - epoch 16/20 done , tot steps 25664 , lr 1.3E-05 , loss 0.08 , qa loss 0.08 , lm loss 0.00 , avg batch size 4.0
2023-05-31 13:40:05,841 - 3:45:02 - 271.7s - INFO - __main__ - progress 16.624 , lr 1.1E-05 , loss 0.068 , qa loss 0.068 , lm loss 0.000 , avg batch size 4.0
2023-05-31 13:42:56,702 - 3:47:53 - 170.9s - INFO - __main__ - epoch 17/20 done , tot steps 27268 , lr 9.4E-06 , loss 0.07 , qa loss 0.07 , lm loss 0.00 , avg batch size 4.0
2023-05-31 13:47:12,235 - 3:52:08 - 255.5s - INFO - __main__ - progress 17.624 , lr 7.4E-06 , loss 0.061 , qa loss 0.061 , lm loss 0.000 , avg batch size 4.0
2023-05-31 13:49:16,139 - 3:54:12 - 123.9s - INFO - __main__ - epoch 18/20 done , tot steps 28872 , lr 6.3E-06 , loss 0.06 , qa loss 0.06 , lm loss 0.00 , avg batch size 4.0
2023-05-31 13:52:42,953 - 3:57:39 - 206.8s - INFO - __main__ - progress 18.624 , lr 4.3E-06 , loss 0.050 , qa loss 0.050 , lm loss 0.000 , avg batch size 4.0
2023-05-31 13:54:44,925 - 3:59:41 - 122.0s - INFO - __main__ - epoch 19/20 done , tot steps 30476 , lr 3.1E-06 , loss 0.05 , qa loss 0.05 , lm loss 0.00 , avg batch size 4.0
2023-05-31 13:58:04,943 - 4:03:01 - 200.0s - INFO - __main__ - progress 19.624 , lr 1.2E-06 , loss 0.045 , qa loss 0.045 , lm loss 0.000 , avg batch size 4.0
2023-05-31 14:00:12,329 - 4:05:09 - 127.4s - INFO - __main__ - epoch 20/20 done , tot steps 32080 , lr 1.6E-08 , loss 0.05 , qa loss 0.05 , lm loss 0.00 , avg batch size 4.0
2023-05-31 14:00:21,050 - 4:05:17 - 8.7s - INFO - __main__ - start to train { task: ['woz.en'], seq train type: lll }
2023-05-31 14:00:24,038 - 4:05:20 - 3.0s - INFO - utils - writing extra data in ../../model_mam1/gpt2/lll/sst_srl_woz.en_0.0/srl/lm.csv ...
2023-05-31 14:00:24,038 - 4:05:20 - 0.0s - INFO - __main__ - extra training data size: 0
2023-05-31 14:00:24,165 - 4:05:20 - 0.1s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[srl]
The task with which model is saved srl
2023-05-31 14:00:26,589 - 4:05:23 - 2.4s - INFO - __main__ - len of train dataset: 2536 , max train batch size 4 , num of opt steps: 50720
2023-05-31 14:01:49,175 - 4:06:45 - 82.6s - INFO - __main__ - epoch 1/20 done , tot steps 634 , lr 5.9E-05 , loss 2.18 , qa loss 2.18 , lm loss 0.00 , avg batch size 4.0
2023-05-31 14:03:05,816 - 4:08:02 - 76.6s - INFO - __main__ - epoch 2/20 done , tot steps 1268 , lr 5.6E-05 , loss 0.35 , qa loss 0.35 , lm loss 0.00 , avg batch size 4.0
2023-05-31 14:04:22,721 - 4:09:19 - 76.9s - INFO - __main__ - epoch 3/20 done , tot steps 1902 , lr 5.3E-05 , loss 0.25 , qa loss 0.25 , lm loss 0.00 , avg batch size 4.0
2023-05-31 14:05:39,880 - 4:10:36 - 77.2s - INFO - __main__ - epoch 4/20 done , tot steps 2536 , lr 5.0E-05 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-05-31 14:06:57,810 - 4:11:54 - 77.9s - INFO - __main__ - epoch 5/20 done , tot steps 3170 , lr 4.7E-05 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-05-31 14:08:15,024 - 4:13:11 - 77.2s - INFO - __main__ - epoch 6/20 done , tot steps 3804 , lr 4.4E-05 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-05-31 14:09:31,841 - 4:14:28 - 76.8s - INFO - __main__ - epoch 7/20 done , tot steps 4438 , lr 4.1E-05 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
2023-05-31 14:10:49,357 - 4:15:46 - 77.5s - INFO - __main__ - epoch 8/20 done , tot steps 5072 , lr 3.8E-05 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-05-31 14:12:06,270 - 4:17:02 - 76.9s - INFO - __main__ - epoch 9/20 done , tot steps 5706 , lr 3.4E-05 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-05-31 14:13:23,243 - 4:18:19 - 77.0s - INFO - __main__ - epoch 10/20 done , tot steps 6340 , lr 3.1E-05 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 4.0
2023-05-31 14:14:40,065 - 4:19:36 - 76.8s - INFO - __main__ - epoch 11/20 done , tot steps 6974 , lr 2.8E-05 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 4.0
2023-05-31 14:15:56,489 - 4:20:53 - 76.4s - INFO - __main__ - epoch 12/20 done , tot steps 7608 , lr 2.5E-05 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 4.0
2023-05-31 14:17:13,296 - 4:22:10 - 76.8s - INFO - __main__ - epoch 13/20 done , tot steps 8242 , lr 2.2E-05 , loss 0.09 , qa loss 0.09 , lm loss 0.00 , avg batch size 4.0
2023-05-31 14:18:30,168 - 4:23:26 - 76.9s - INFO - __main__ - epoch 14/20 done , tot steps 8876 , lr 1.9E-05 , loss 0.07 , qa loss 0.07 , lm loss 0.00 , avg batch size 4.0
2023-05-31 14:19:47,027 - 4:24:43 - 76.9s - INFO - __main__ - epoch 15/20 done , tot steps 9510 , lr 1.6E-05 , loss 0.07 , qa loss 0.07 , lm loss 0.00 , avg batch size 4.0
2023-05-31 14:21:04,072 - 4:26:00 - 77.0s - INFO - __main__ - epoch 16/20 done , tot steps 10144 , lr 1.3E-05 , loss 0.07 , qa loss 0.07 , lm loss 0.00 , avg batch size 4.0
2023-05-31 14:22:20,372 - 4:27:17 - 76.3s - INFO - __main__ - epoch 17/20 done , tot steps 10778 , lr 9.4E-06 , loss 0.06 , qa loss 0.06 , lm loss 0.00 , avg batch size 4.0
2023-05-31 14:23:37,100 - 4:28:33 - 76.7s - INFO - __main__ - epoch 18/20 done , tot steps 11412 , lr 6.3E-06 , loss 0.06 , qa loss 0.06 , lm loss 0.00 , avg batch size 4.0
2023-05-31 14:24:53,854 - 4:29:50 - 76.8s - INFO - __main__ - epoch 19/20 done , tot steps 12046 , lr 3.1E-06 , loss 0.06 , qa loss 0.06 , lm loss 0.00 , avg batch size 4.0
2023-05-31 14:26:10,361 - 4:31:07 - 76.5s - INFO - __main__ - epoch 20/20 done , tot steps 12680 , lr 1.5E-08 , loss 0.05 , qa loss 0.05 , lm loss 0.00 , avg batch size 4.0
The current active adapter is Stack[woz_en]
The task with which model is saved woz_en
Wall Execution time: 04:31:08
CPU Execution time: 04:30:55
