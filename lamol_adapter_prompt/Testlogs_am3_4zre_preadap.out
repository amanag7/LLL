Not all gpus support fp16 training! Will use fp32 instead.
2023-06-27 00:51:32,200 - 0:00:06 - 0.0s - INFO - __main__ - args = Namespace(adam_epsilon=0.0001, add_task_tokens=False, data_dir='../../data', debug=False, decay_style='linear', device_ids=[0], dynamic_epochs=False, fp32=False, gen_lm_sample_percentage=0.0, learning_rate=6.25e-05, lm_lambda=0.0, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[40537.0], min_batch_size=4, min_n_steps=1500, model_dir_root='../../model_am3/gpt2/lll/sst_srl_zre_woz.en_0.0', model_name='gpt2', n_gpus=1, n_train_epochs={'sst': 20, 'srl': 20, 'zre': 20, 'woz.en': 20}, n_warmup_ratio=0.005, n_workers=25, qp_margin=0.5, real_sample=False, reg_lambda=1.0, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['sst', 'srl', 'zre', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[14187], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[14187], unbound=0, use_sep=False, weight_decay=0.01)
2023-06-27 00:51:43,091 - 0:00:17 - 10.9s - INFO - __main__ - task: sst, epoch: 20
2023-06-27 00:51:43,091 - 0:00:17 - 0.0s - INFO - __main__ - start to test { task: sst (load) sst (eval), seq train type: lll }
2023-06-27 00:51:44,161 - 0:00:18 - 1.1s - INFO - __main__ - len of test dataset: 1821
2023-06-27 00:51:47,035 - 0:00:21 - 2.9s - INFO - __main__ - score: {'sst': OrderedDict([('em', 87.53432180120812), ('nf1', 87.53432180120812), ('nem', 87.53432180120812)]), 'srl': None, 'zre': None, 'woz.en': None}
2023-06-27 00:51:58,090 - 0:00:32 - 11.1s - INFO - __main__ - task: srl, epoch: 20
2023-06-27 00:51:58,090 - 0:00:32 - 0.0s - INFO - __main__ - start to test { task: srl (load) srl (eval), seq train type: lll }
2023-06-27 00:51:59,924 - 0:00:34 - 1.8s - INFO - __main__ - len of test dataset: 2201
2023-06-27 01:15:56,558 - 0:24:30 - 1436.6s - INFO - __main__ - score: {'sst': None, 'srl': OrderedDict([('em', 31.25851885506588), ('nf1', 52.26836064665821), ('nem', 35.98364379827351)]), 'zre': None, 'woz.en': None}
2023-06-27 01:16:07,794 - 0:24:41 - 11.2s - INFO - __main__ - task: zre, epoch: 20
2023-06-27 01:16:07,795 - 0:24:41 - 0.0s - INFO - __main__ - start to test { task: zre (load) zre (eval), seq train type: lll }
2023-06-27 01:16:09,773 - 0:24:43 - 2.0s - INFO - __main__ - len of test dataset: 12000
2023-06-27 02:54:01,685 - 2:02:35 - 5871.9s - INFO - __main__ - score: {'sst': None, 'srl': None, 'zre': OrderedDict([('em', 66.98333333333333), ('nf1', 69.70976315351336), ('nem', 67.15833333333333), ('corpus_f1', 50.34664583536763), ('precision', 60.78755010610705), ('recall', 42.96666666666666)]), 'woz.en': None}
2023-06-27 02:54:12,417 - 2:02:46 - 10.7s - INFO - __main__ - task: woz.en, epoch: 20
2023-06-27 02:54:12,418 - 2:02:46 - 0.0s - INFO - __main__ - start to test { task: woz.en (load) woz.en (eval), seq train type: lll }
2023-06-27 02:54:14,097 - 2:02:48 - 1.7s - INFO - __main__ - len of test dataset: 1646
2023-06-27 03:05:37,467 - 2:14:11 - 683.4s - INFO - __main__ - score: {'sst': None, 'srl': None, 'zre': None, 'woz.en': OrderedDict([('em', 6.865127582017011), ('nf1', 77.20700084308828), ('nem', 56.50060753341434), ('joint_goal_em', 35.78371810449575), ('turn_request_em', 68.89428918590522), ('turn_goal_em', 69.19805589307411), ('avg_dialogue', 52.339003645200485)])}
