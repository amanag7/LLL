Not all gpus support fp16 training! Will use fp32 instead.
2023-04-25 11:39:02,012 - 0:00:05 - 0.0s - INFO - __main__ - args = Namespace(REG_TYPE_KEYS=['mas', 'ewc'], adam_epsilon=0.0001, add_task_tokens=False, data_dir='../../data', debug=False, decay_style='linear', device_ids=[2, 3, 4, 5, 6, 7], dynamic_epochs=False, fp32=True, gen_lm_sample_percentage=0.0, learning_rate=6.25e-05, lm_lambda=0.0, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[32429.600000000002, 42158.48, 42158.48, 42158.48, 42158.48, 42158.48], min_batch_size=4, min_n_steps=1500, model_dir_root='../../model_adap/gpt2/lll/sst_srl_woz.en_0.0', model_name='gpt2', n_gpus=6, n_train_epochs={'sst': 12, 'srl': 12, 'woz.en': 12}, n_warmup_ratio=0.005, n_workers=75, qp_margin=0.5, real_sample=False, reg_lambda=1.0, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[11350, 14755, 14755, 14755, 14755, 14755], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[11350, 14755, 14755, 14755, 14755, 14755], unbound=0, use_sep=False, weight_decay=0.01)
2023-04-25 11:39:02,012 - 0:00:05 - 0.0s - INFO - __main__ - start to train { task: ['sst'], seq train type: lll }
2023-04-25 11:39:02,013 - 0:00:05 - 0.0s - INFO - __main__ - extra training data size: 0
2023-04-25 11:39:04,284 - 0:00:08 - 2.3s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
2023-04-25 11:39:06,625 - 0:00:10 - 2.3s - INFO - __main__ - len of train dataset: 6920 , max train batch size 4 , num of opt steps: 83040
/home/student/2021/cs21mtech11006/anaconda3/envs/lamol/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/home/student/2021/cs21mtech11006/anaconda3/envs/lamol/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
2023-04-25 11:41:08,201 - 0:02:12 - 121.6s - INFO - __main__ - progress 0.578 , lr 6.0E-05 , loss 3.083 , qa loss 3.083 , lm loss 0.000 , avg batch size 4.0
2023-04-25 11:42:36,692 - 0:03:40 - 88.5s - INFO - __main__ - epoch 1/12 done , tot steps 1730 , lr 5.7E-05 , loss 1.89 , qa loss 1.89 , lm loss 0.00 , avg batch size 4.0
2023-04-25 11:44:31,646 - 0:05:35 - 115.0s - INFO - __main__ - progress 1.578 , lr 5.4E-05 , loss 0.220 , qa loss 0.220 , lm loss 0.000 , avg batch size 4.0
2023-04-25 11:45:56,656 - 0:07:00 - 85.0s - INFO - __main__ - epoch 2/12 done , tot steps 3460 , lr 5.2E-05 , loss 0.21 , qa loss 0.21 , lm loss 0.00 , avg batch size 4.0
2023-04-25 11:47:51,781 - 0:08:55 - 115.1s - INFO - __main__ - progress 2.578 , lr 4.9E-05 , loss 0.199 , qa loss 0.199 , lm loss 0.000 , avg batch size 4.0
2023-04-25 11:49:16,553 - 0:10:20 - 84.8s - INFO - __main__ - epoch 3/12 done , tot steps 5190 , lr 4.7E-05 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-04-25 11:51:11,256 - 0:12:15 - 114.7s - INFO - __main__ - progress 3.578 , lr 4.4E-05 , loss 0.170 , qa loss 0.170 , lm loss 0.000 , avg batch size 4.0
2023-04-25 11:52:35,282 - 0:13:39 - 84.0s - INFO - __main__ - epoch 4/12 done , tot steps 6920 , lr 4.2E-05 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-04-25 11:54:31,213 - 0:15:35 - 115.9s - INFO - __main__ - progress 4.578 , lr 3.9E-05 , loss 0.157 , qa loss 0.157 , lm loss 0.000 , avg batch size 4.0
2023-04-25 11:55:52,908 - 0:16:56 - 81.7s - INFO - __main__ - epoch 5/12 done , tot steps 8650 , lr 3.6E-05 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-04-25 11:57:48,326 - 0:18:52 - 115.4s - INFO - __main__ - progress 5.578 , lr 3.3E-05 , loss 0.141 , qa loss 0.141 , lm loss 0.000 , avg batch size 4.0
2023-04-25 11:59:11,082 - 0:20:15 - 82.8s - INFO - __main__ - epoch 6/12 done , tot steps 10380 , lr 3.1E-05 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
2023-04-25 12:01:06,130 - 0:22:10 - 115.0s - INFO - __main__ - progress 6.578 , lr 2.8E-05 , loss 0.131 , qa loss 0.131 , lm loss 0.000 , avg batch size 4.0
2023-04-25 12:02:30,615 - 0:23:34 - 84.5s - INFO - __main__ - epoch 7/12 done , tot steps 12110 , lr 2.6E-05 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-04-25 12:04:26,195 - 0:25:30 - 115.6s - INFO - __main__ - progress 7.578 , lr 2.3E-05 , loss 0.118 , qa loss 0.118 , lm loss 0.000 , avg batch size 4.0
2023-04-25 12:05:50,663 - 0:26:54 - 84.5s - INFO - __main__ - epoch 8/12 done , tot steps 13840 , lr 2.1E-05 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2023-04-25 12:07:45,716 - 0:28:49 - 115.1s - INFO - __main__ - progress 8.578 , lr 1.8E-05 , loss 0.106 , qa loss 0.106 , lm loss 0.000 , avg batch size 4.0
2023-04-25 12:09:10,642 - 0:30:14 - 84.9s - INFO - __main__ - epoch 9/12 done , tot steps 15570 , lr 1.6E-05 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-04-25 12:11:05,096 - 0:32:09 - 114.5s - INFO - __main__ - progress 9.578 , lr 1.3E-05 , loss 0.110 , qa loss 0.110 , lm loss 0.000 , avg batch size 4.0
2023-04-25 12:12:29,770 - 0:33:33 - 84.7s - INFO - __main__ - epoch 10/12 done , tot steps 17300 , lr 1.0E-05 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-04-25 12:14:23,127 - 0:35:27 - 113.4s - INFO - __main__ - progress 10.578 , lr 7.4E-06 , loss 0.100 , qa loss 0.100 , lm loss 0.000 , avg batch size 4.0
2023-04-25 12:15:47,405 - 0:36:51 - 84.3s - INFO - __main__ - epoch 11/12 done , tot steps 19030 , lr 5.2E-06 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 4.0
2023-04-25 12:17:40,649 - 0:38:44 - 113.2s - INFO - __main__ - progress 11.578 , lr 2.2E-06 , loss 0.089 , qa loss 0.089 , lm loss 0.000 , avg batch size 4.0
2023-04-25 12:19:03,181 - 0:40:07 - 82.5s - INFO - __main__ - epoch 12/12 done , tot steps 20760 , lr 2.6E-08 , loss 0.09 , qa loss 0.09 , lm loss 0.00 , avg batch size 4.0
2023-04-25 12:19:10,030 - 0:40:13 - 6.8s - INFO - __main__ - start to train { task: ['srl'], seq train type: lll }
2023-04-25 12:19:12,493 - 0:40:16 - 2.5s - INFO - utils - writing extra data in ../../model_adap/gpt2/lll/sst_srl_woz.en_0.0/sst/lm.csv ...
2023-04-25 12:19:12,494 - 0:40:16 - 0.0s - INFO - __main__ - extra training data size: 0
2023-04-25 12:19:12,606 - 0:40:16 - 0.1s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[sst]
The task with which model is saved sst
2023-04-25 12:19:14,751 - 0:40:18 - 2.1s - INFO - __main__ - len of train dataset: 6414 , max train batch size 4 , num of opt steps: 76968
2023-04-25 12:22:09,937 - 0:43:13 - 175.2s - INFO - __main__ - progress 0.624 , lr 5.9E-05 , loss 4.158 , qa loss 4.158 , lm loss 0.000 , avg batch size 4.0
2023-04-25 12:24:01,611 - 0:45:05 - 111.7s - INFO - __main__ - epoch 1/12 done , tot steps 1604 , lr 5.7E-05 , loss 2.95 , qa loss 2.95 , lm loss 0.00 , avg batch size 4.0
2023-04-25 12:26:56,077 - 0:48:00 - 174.5s - INFO - __main__ - progress 1.624 , lr 5.4E-05 , loss 0.781 , qa loss 0.781 , lm loss 0.000 , avg batch size 4.0
2023-04-25 12:28:40,775 - 0:49:44 - 104.7s - INFO - __main__ - epoch 2/12 done , tot steps 3208 , lr 5.2E-05 , loss 0.75 , qa loss 0.75 , lm loss 0.00 , avg batch size 4.0
2023-04-25 12:31:30,501 - 0:52:34 - 169.7s - INFO - __main__ - progress 2.624 , lr 4.9E-05 , loss 0.661 , qa loss 0.661 , lm loss 0.000 , avg batch size 4.0
2023-04-25 12:33:19,258 - 0:54:23 - 108.8s - INFO - __main__ - epoch 3/12 done , tot steps 4812 , lr 4.7E-05 , loss 0.64 , qa loss 0.64 , lm loss 0.00 , avg batch size 4.0
2023-04-25 12:36:13,233 - 0:57:17 - 174.0s - INFO - __main__ - progress 3.624 , lr 4.4E-05 , loss 0.565 , qa loss 0.565 , lm loss 0.000 , avg batch size 4.0
2023-04-25 12:37:57,840 - 0:59:01 - 104.6s - INFO - __main__ - epoch 4/12 done , tot steps 6416 , lr 4.2E-05 , loss 0.57 , qa loss 0.57 , lm loss 0.00 , avg batch size 4.0
2023-04-25 12:40:50,996 - 1:01:54 - 173.2s - INFO - __main__ - progress 4.624 , lr 3.8E-05 , loss 0.513 , qa loss 0.513 , lm loss 0.000 , avg batch size 4.0
2023-04-25 12:42:38,088 - 1:03:42 - 107.1s - INFO - __main__ - epoch 5/12 done , tot steps 8020 , lr 3.6E-05 , loss 0.51 , qa loss 0.51 , lm loss 0.00 , avg batch size 4.0
2023-04-25 12:45:32,860 - 1:06:36 - 174.8s - INFO - __main__ - progress 5.624 , lr 3.3E-05 , loss 0.478 , qa loss 0.478 , lm loss 0.000 , avg batch size 4.0
2023-04-25 12:47:20,004 - 1:08:23 - 107.1s - INFO - __main__ - epoch 6/12 done , tot steps 9624 , lr 3.1E-05 , loss 0.48 , qa loss 0.48 , lm loss 0.00 , avg batch size 4.0
2023-04-25 12:50:15,461 - 1:11:19 - 175.5s - INFO - __main__ - progress 6.624 , lr 2.8E-05 , loss 0.432 , qa loss 0.432 , lm loss 0.000 , avg batch size 4.0
2023-04-25 12:52:02,798 - 1:13:06 - 107.3s - INFO - __main__ - epoch 7/12 done , tot steps 11228 , lr 2.6E-05 , loss 0.44 , qa loss 0.44 , lm loss 0.00 , avg batch size 4.0
2023-04-25 12:55:00,923 - 1:16:04 - 178.1s - INFO - __main__ - progress 7.624 , lr 2.3E-05 , loss 0.413 , qa loss 0.413 , lm loss 0.000 , avg batch size 4.0
2023-04-25 12:56:44,319 - 1:17:48 - 103.4s - INFO - __main__ - epoch 8/12 done , tot steps 12832 , lr 2.1E-05 , loss 0.40 , qa loss 0.40 , lm loss 0.00 , avg batch size 4.0
2023-04-25 12:59:37,283 - 1:20:41 - 173.0s - INFO - __main__ - progress 8.624 , lr 1.8E-05 , loss 0.394 , qa loss 0.394 , lm loss 0.000 , avg batch size 4.0
2023-04-25 13:01:24,675 - 1:22:28 - 107.4s - INFO - __main__ - epoch 9/12 done , tot steps 14436 , lr 1.6E-05 , loss 0.39 , qa loss 0.39 , lm loss 0.00 , avg batch size 4.0
2023-04-25 13:04:18,872 - 1:25:22 - 174.2s - INFO - __main__ - progress 9.624 , lr 1.2E-05 , loss 0.390 , qa loss 0.390 , lm loss 0.000 , avg batch size 4.0
2023-04-25 13:06:03,760 - 1:27:07 - 104.9s - INFO - __main__ - epoch 10/12 done , tot steps 16040 , lr 1.0E-05 , loss 0.38 , qa loss 0.38 , lm loss 0.00 , avg batch size 4.0
2023-04-25 13:08:59,258 - 1:30:03 - 175.5s - INFO - __main__ - progress 10.624 , lr 7.2E-06 , loss 0.348 , qa loss 0.348 , lm loss 0.000 , avg batch size 4.0
2023-04-25 13:10:45,378 - 1:31:49 - 106.1s - INFO - __main__ - epoch 11/12 done , tot steps 17644 , lr 5.2E-06 , loss 0.35 , qa loss 0.35 , lm loss 0.00 , avg batch size 4.0
2023-04-25 13:13:42,941 - 1:34:46 - 177.6s - INFO - __main__ - progress 11.624 , lr 2.0E-06 , loss 0.353 , qa loss 0.353 , lm loss 0.000 , avg batch size 4.0
2023-04-25 13:15:26,784 - 1:36:30 - 103.8s - INFO - __main__ - epoch 12/12 done , tot steps 19248 , lr 2.6E-08 , loss 0.35 , qa loss 0.35 , lm loss 0.00 , avg batch size 4.0
2023-04-25 13:15:33,476 - 1:36:37 - 6.7s - INFO - __main__ - start to train { task: ['woz.en'], seq train type: lll }
2023-04-25 13:15:50,942 - 1:36:54 - 17.5s - INFO - utils - writing extra data in ../../model_adap/gpt2/lll/sst_srl_woz.en_0.0/srl/lm.csv ...
2023-04-25 13:15:50,943 - 1:36:54 - 0.0s - INFO - __main__ - extra training data size: 0
2023-04-25 13:15:51,071 - 1:36:55 - 0.1s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[sst]
The task with which model is saved sst
2023-04-25 13:15:53,479 - 1:36:57 - 2.4s - INFO - __main__ - len of train dataset: 2536 , max train batch size 4 , num of opt steps: 30432
2023-04-25 13:17:12,726 - 1:38:16 - 79.2s - INFO - __main__ - epoch 1/12 done , tot steps 634 , lr 5.7E-05 , loss 5.17 , qa loss 5.17 , lm loss 0.00 , avg batch size 4.0
2023-04-25 13:18:27,076 - 1:39:31 - 74.3s - INFO - __main__ - epoch 2/12 done , tot steps 1268 , lr 5.2E-05 , loss 0.42 , qa loss 0.42 , lm loss 0.00 , avg batch size 4.0
2023-04-25 13:19:42,622 - 1:40:46 - 75.5s - INFO - __main__ - epoch 3/12 done , tot steps 1902 , lr 4.7E-05 , loss 0.30 , qa loss 0.30 , lm loss 0.00 , avg batch size 4.0
2023-04-25 13:20:57,099 - 1:42:01 - 74.5s - INFO - __main__ - epoch 4/12 done , tot steps 2536 , lr 4.2E-05 , loss 0.25 , qa loss 0.25 , lm loss 0.00 , avg batch size 4.0
2023-04-25 13:22:10,390 - 1:43:14 - 73.3s - INFO - __main__ - epoch 5/12 done , tot steps 3170 , lr 3.6E-05 , loss 0.22 , qa loss 0.22 , lm loss 0.00 , avg batch size 4.0
2023-04-25 13:23:24,250 - 1:44:28 - 73.9s - INFO - __main__ - epoch 6/12 done , tot steps 3804 , lr 3.1E-05 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-04-25 13:24:38,429 - 1:45:42 - 74.2s - INFO - __main__ - epoch 7/12 done , tot steps 4438 , lr 2.6E-05 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
2023-04-25 13:25:52,194 - 1:46:56 - 73.8s - INFO - __main__ - epoch 8/12 done , tot steps 5072 , lr 2.1E-05 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-04-25 13:27:06,771 - 1:48:10 - 74.6s - INFO - __main__ - epoch 9/12 done , tot steps 5706 , lr 1.6E-05 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-04-25 13:28:21,121 - 1:49:25 - 74.3s - INFO - __main__ - epoch 10/12 done , tot steps 6340 , lr 1.0E-05 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
2023-04-25 13:29:35,611 - 1:50:39 - 74.5s - INFO - __main__ - epoch 11/12 done , tot steps 6974 , lr 5.2E-06 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
2023-04-25 13:30:50,145 - 1:51:54 - 74.5s - INFO - __main__ - epoch 12/12 done , tot steps 7608 , lr 2.5E-08 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
The current active adapter is Stack[sst]
The task with which model is saved sst
Wall Execution time: 01:51:54
CPU Execution time: 01:52:17
