2023-06-29 16:28:30,064 - 0:00:10 - 0.0s - INFO - __main__ - args = Namespace(REG_TYPE_KEYS=['mas', 'ewc'], adam_epsilon=0.0001, add_task_tokens=False, data_dir='../../data', debug=False, decay_style='linear', device_ids=[1], dynamic_epochs=False, fp32=True, gen_lm_sample_percentage=0.0, learning_rate=6.25e-05, lm_lambda=0.0, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[40537.0], min_batch_size=4, min_n_steps=1500, model_dir_root='../../model_cm40/gpt2/lll/sst_srl_woz.en_0.0', model_name='gpt2', n_gpus=1, n_train_epochs={'sst': 20, 'srl': 20, 'woz.en': 20}, n_warmup_ratio=0.005, n_workers=75, qp_margin=0.5, real_sample=False, reg_lambda=1.0, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[14187], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[14187], unbound=0, use_sep=False, weight_decay=0.01)
2023-06-29 16:28:30,065 - 0:00:10 - 0.0s - INFO - __main__ - start to train { task: ['sst'], seq train type: lll }
2023-06-29 16:28:30,072 - 0:00:10 - 0.0s - INFO - __main__ - extra training data size: 0
2023-06-29 16:28:35,174 - 0:00:15 - 5.1s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
2023-06-29 16:28:37,995 - 0:00:18 - 2.8s - INFO - __main__ - len of train dataset: 6920 , max train batch size 4 , num of opt steps: 138400
/home/student/2021/cs21mtech11006/anaconda3/envs/lamol/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
2023-06-29 16:29:52,379 - 0:01:33 - 74.4s - INFO - __main__ - progress 0.578 , lr 6.1E-05 , loss 3.335 , qa loss 3.335 , lm loss 0.000 , avg batch size 4.0
2023-06-29 16:31:22,509 - 0:03:03 - 90.1s - INFO - __main__ - epoch 1/20 done , tot steps 1730 , lr 5.9E-05 , loss 2.06 , qa loss 2.06 , lm loss 0.00 , avg batch size 4.0
2023-06-29 16:33:23,250 - 0:05:03 - 120.7s - INFO - __main__ - progress 1.578 , lr 5.8E-05 , loss 0.304 , qa loss 0.304 , lm loss 0.000 , avg batch size 4.0
2023-06-29 16:34:51,372 - 0:06:32 - 88.1s - INFO - __main__ - epoch 2/20 done , tot steps 3460 , lr 5.6E-05 , loss 0.29 , qa loss 0.29 , lm loss 0.00 , avg batch size 4.0
2023-06-29 16:36:53,539 - 0:08:34 - 122.2s - INFO - __main__ - progress 2.578 , lr 5.4E-05 , loss 0.241 , qa loss 0.241 , lm loss 0.000 , avg batch size 4.0
2023-06-29 16:38:20,481 - 0:10:01 - 86.9s - INFO - __main__ - epoch 3/20 done , tot steps 5190 , lr 5.3E-05 , loss 0.23 , qa loss 0.23 , lm loss 0.00 , avg batch size 4.0
2023-06-29 16:40:22,131 - 0:12:02 - 121.7s - INFO - __main__ - progress 3.578 , lr 5.1E-05 , loss 0.200 , qa loss 0.200 , lm loss 0.000 , avg batch size 4.0
2023-06-29 16:41:49,794 - 0:13:30 - 87.7s - INFO - __main__ - epoch 4/20 done , tot steps 6920 , lr 5.0E-05 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-06-29 16:43:53,276 - 0:15:33 - 123.5s - INFO - __main__ - progress 4.578 , lr 4.8E-05 , loss 0.183 , qa loss 0.183 , lm loss 0.000 , avg batch size 4.0
2023-06-29 16:45:19,021 - 0:16:59 - 85.7s - INFO - __main__ - epoch 5/20 done , tot steps 8650 , lr 4.7E-05 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-06-29 16:47:23,554 - 0:19:04 - 124.5s - INFO - __main__ - progress 5.578 , lr 4.5E-05 , loss 0.171 , qa loss 0.171 , lm loss 0.000 , avg batch size 4.0
2023-06-29 16:48:47,941 - 0:20:28 - 84.4s - INFO - __main__ - epoch 6/20 done , tot steps 10380 , lr 4.4E-05 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
2023-06-29 16:50:51,105 - 0:22:31 - 123.2s - INFO - __main__ - progress 6.578 , lr 4.2E-05 , loss 0.153 , qa loss 0.153 , lm loss 0.000 , avg batch size 4.0
2023-06-29 16:52:16,706 - 0:23:57 - 85.6s - INFO - __main__ - epoch 7/20 done , tot steps 12110 , lr 4.1E-05 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-06-29 16:54:19,845 - 0:26:00 - 123.1s - INFO - __main__ - progress 7.578 , lr 3.9E-05 , loss 0.139 , qa loss 0.139 , lm loss 0.000 , avg batch size 4.0
2023-06-29 16:55:46,095 - 0:27:26 - 86.2s - INFO - __main__ - epoch 8/20 done , tot steps 13840 , lr 3.8E-05 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
2023-06-29 16:57:48,628 - 0:29:29 - 122.5s - INFO - __main__ - progress 8.578 , lr 3.6E-05 , loss 0.138 , qa loss 0.138 , lm loss 0.000 , avg batch size 4.0
2023-06-29 16:59:14,635 - 0:30:55 - 86.0s - INFO - __main__ - epoch 9/20 done , tot steps 15570 , lr 3.4E-05 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
2023-06-29 17:01:17,953 - 0:32:58 - 123.3s - INFO - __main__ - progress 9.578 , lr 3.3E-05 , loss 0.118 , qa loss 0.118 , lm loss 0.000 , avg batch size 4.0
2023-06-29 17:02:43,389 - 0:34:24 - 85.4s - INFO - __main__ - epoch 10/20 done , tot steps 17300 , lr 3.1E-05 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-06-29 17:04:46,664 - 0:36:27 - 123.3s - INFO - __main__ - progress 10.578 , lr 2.9E-05 , loss 0.117 , qa loss 0.117 , lm loss 0.000 , avg batch size 4.0
2023-06-29 17:06:11,955 - 0:37:52 - 85.3s - INFO - __main__ - epoch 11/20 done , tot steps 19030 , lr 2.8E-05 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2023-06-29 17:08:14,314 - 0:39:55 - 122.4s - INFO - __main__ - progress 11.578 , lr 2.6E-05 , loss 0.110 , qa loss 0.110 , lm loss 0.000 , avg batch size 4.0
2023-06-29 17:09:40,430 - 0:41:21 - 86.1s - INFO - __main__ - epoch 12/20 done , tot steps 20760 , lr 2.5E-05 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-06-29 17:11:42,649 - 0:43:23 - 122.2s - INFO - __main__ - progress 12.578 , lr 2.3E-05 , loss 0.109 , qa loss 0.109 , lm loss 0.000 , avg batch size 4.0
2023-06-29 17:13:09,032 - 0:44:49 - 86.4s - INFO - __main__ - epoch 13/20 done , tot steps 22490 , lr 2.2E-05 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-06-29 17:15:11,419 - 0:46:52 - 122.4s - INFO - __main__ - progress 13.578 , lr 2.0E-05 , loss 0.103 , qa loss 0.103 , lm loss 0.000 , avg batch size 4.0
2023-06-29 17:16:38,125 - 0:48:18 - 86.7s - INFO - __main__ - epoch 14/20 done , tot steps 24220 , lr 1.9E-05 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-06-29 17:18:42,356 - 0:50:23 - 124.2s - INFO - __main__ - progress 14.578 , lr 1.7E-05 , loss 0.096 , qa loss 0.096 , lm loss 0.000 , avg batch size 4.0
2023-06-29 17:20:07,456 - 0:51:48 - 85.1s - INFO - __main__ - epoch 15/20 done , tot steps 25950 , lr 1.6E-05 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 4.0
2023-06-29 17:22:11,678 - 0:53:52 - 124.2s - INFO - __main__ - progress 15.578 , lr 1.4E-05 , loss 0.094 , qa loss 0.094 , lm loss 0.000 , avg batch size 4.0
2023-06-29 17:23:36,859 - 0:55:17 - 85.2s - INFO - __main__ - epoch 16/20 done , tot steps 27680 , lr 1.3E-05 , loss 0.09 , qa loss 0.09 , lm loss 0.00 , avg batch size 4.0
2023-06-29 17:25:41,148 - 0:57:21 - 124.3s - INFO - __main__ - progress 16.578 , lr 1.1E-05 , loss 0.101 , qa loss 0.101 , lm loss 0.000 , avg batch size 4.0
2023-06-29 17:27:06,417 - 0:58:47 - 85.3s - INFO - __main__ - epoch 17/20 done , tot steps 29410 , lr 9.4E-06 , loss 0.09 , qa loss 0.09 , lm loss 0.00 , avg batch size 4.0
2023-06-29 17:29:09,626 - 1:00:50 - 123.2s - INFO - __main__ - progress 17.578 , lr 7.6E-06 , loss 0.083 , qa loss 0.083 , lm loss 0.000 , avg batch size 4.0
2023-06-29 17:30:35,770 - 1:02:16 - 86.1s - INFO - __main__ - epoch 18/20 done , tot steps 31140 , lr 6.3E-06 , loss 0.09 , qa loss 0.09 , lm loss 0.00 , avg batch size 4.0
2023-06-29 17:32:38,254 - 1:04:18 - 122.5s - INFO - __main__ - progress 18.578 , lr 4.5E-06 , loss 0.076 , qa loss 0.076 , lm loss 0.000 , avg batch size 4.0
2023-06-29 17:34:04,645 - 1:05:45 - 86.4s - INFO - __main__ - epoch 19/20 done , tot steps 32870 , lr 3.1E-06 , loss 0.08 , qa loss 0.08 , lm loss 0.00 , avg batch size 4.0
2023-06-29 17:36:05,890 - 1:07:46 - 121.2s - INFO - __main__ - progress 19.578 , lr 1.3E-06 , loss 0.072 , qa loss 0.072 , lm loss 0.000 , avg batch size 4.0
2023-06-29 17:37:39,495 - 1:09:20 - 93.6s - INFO - __main__ - epoch 20/20 done , tot steps 34600 , lr 1.5E-08 , loss 0.08 , qa loss 0.08 , lm loss 0.00 , avg batch size 4.0
The current active adapter is Stack[sst]
The task with which model is saved sst
2023-06-29 17:37:50,276 - 1:09:30 - 10.8s - INFO - __main__ - start to train { task: ['srl'], seq train type: lll }
2023-06-29 17:37:53,926 - 1:09:34 - 3.6s - INFO - utils - writing extra data in ../../model_cm40/gpt2/lll/sst_srl_woz.en_0.0/sst/lm.csv ...
2023-06-29 17:37:53,970 - 1:09:34 - 0.0s - INFO - __main__ - extra training data size: 0
2023-06-29 17:37:54,235 - 1:09:34 - 0.3s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
2023-06-29 17:37:56,833 - 1:09:37 - 2.6s - INFO - __main__ - len of train dataset: 6414 , max train batch size 4 , num of opt steps: 128280
2023-06-29 17:41:27,110 - 1:13:07 - 210.3s - INFO - __main__ - progress 0.624 , lr 6.1E-05 , loss 3.166 , qa loss 3.166 , lm loss 0.000 , avg batch size 4.0
2023-06-29 17:44:06,280 - 1:15:46 - 159.2s - INFO - __main__ - epoch 1/20 done , tot steps 1604 , lr 5.9E-05 , loss 2.38 , qa loss 2.38 , lm loss 0.00 , avg batch size 4.0
2023-06-29 17:48:18,611 - 1:19:59 - 252.3s - INFO - __main__ - progress 1.624 , lr 5.7E-05 , loss 0.910 , qa loss 0.910 , lm loss 0.000 , avg batch size 4.0
2023-06-29 17:50:56,692 - 1:22:37 - 158.1s - INFO - __main__ - epoch 2/20 done , tot steps 3208 , lr 5.6E-05 , loss 0.89 , qa loss 0.89 , lm loss 0.00 , avg batch size 4.0
2023-06-29 17:55:17,118 - 1:26:57 - 260.4s - INFO - __main__ - progress 2.624 , lr 5.4E-05 , loss 0.763 , qa loss 0.763 , lm loss 0.000 , avg batch size 4.0
2023-06-29 17:57:55,785 - 1:29:36 - 158.7s - INFO - __main__ - epoch 3/20 done , tot steps 4812 , lr 5.3E-05 , loss 0.75 , qa loss 0.75 , lm loss 0.00 , avg batch size 4.0
2023-06-29 18:02:11,896 - 1:33:52 - 256.1s - INFO - __main__ - progress 3.624 , lr 5.1E-05 , loss 0.665 , qa loss 0.665 , lm loss 0.000 , avg batch size 4.0
2023-06-29 18:04:47,468 - 1:36:28 - 155.6s - INFO - __main__ - epoch 4/20 done , tot steps 6416 , lr 5.0E-05 , loss 0.66 , qa loss 0.66 , lm loss 0.00 , avg batch size 4.0
2023-06-29 18:09:05,801 - 1:40:46 - 258.3s - INFO - __main__ - progress 4.624 , lr 4.8E-05 , loss 0.625 , qa loss 0.625 , lm loss 0.000 , avg batch size 4.0
2023-06-29 18:11:41,457 - 1:43:22 - 155.7s - INFO - __main__ - epoch 5/20 done , tot steps 8020 , lr 4.7E-05 , loss 0.61 , qa loss 0.61 , lm loss 0.00 , avg batch size 4.0
2023-06-29 18:15:56,027 - 1:47:36 - 254.6s - INFO - __main__ - progress 5.624 , lr 4.5E-05 , loss 0.562 , qa loss 0.562 , lm loss 0.000 , avg batch size 4.0
2023-06-29 18:18:34,815 - 1:50:15 - 158.8s - INFO - __main__ - epoch 6/20 done , tot steps 9624 , lr 4.4E-05 , loss 0.56 , qa loss 0.56 , lm loss 0.00 , avg batch size 4.0
2023-06-29 18:22:48,322 - 1:54:29 - 253.5s - INFO - __main__ - progress 6.624 , lr 4.2E-05 , loss 0.515 , qa loss 0.515 , lm loss 0.000 , avg batch size 4.0
2023-06-29 18:25:32,992 - 1:57:13 - 164.7s - INFO - __main__ - epoch 7/20 done , tot steps 11228 , lr 4.1E-05 , loss 0.52 , qa loss 0.52 , lm loss 0.00 , avg batch size 4.0
2023-06-29 18:29:44,674 - 2:01:25 - 251.7s - INFO - __main__ - progress 7.624 , lr 3.9E-05 , loss 0.497 , qa loss 0.497 , lm loss 0.000 , avg batch size 4.0
2023-06-29 18:32:26,482 - 2:04:07 - 161.8s - INFO - __main__ - epoch 8/20 done , tot steps 12832 , lr 3.8E-05 , loss 0.51 , qa loss 0.51 , lm loss 0.00 , avg batch size 4.0
2023-06-29 18:36:38,093 - 2:08:18 - 251.6s - INFO - __main__ - progress 8.624 , lr 3.6E-05 , loss 0.461 , qa loss 0.461 , lm loss 0.000 , avg batch size 4.0
2023-06-29 18:39:20,637 - 2:11:01 - 162.5s - INFO - __main__ - epoch 9/20 done , tot steps 14436 , lr 3.4E-05 , loss 0.47 , qa loss 0.47 , lm loss 0.00 , avg batch size 4.0
2023-06-29 18:43:36,492 - 2:15:17 - 255.9s - INFO - __main__ - progress 9.624 , lr 3.2E-05 , loss 0.446 , qa loss 0.446 , lm loss 0.000 , avg batch size 4.0
2023-06-29 18:46:13,399 - 2:17:54 - 156.9s - INFO - __main__ - epoch 10/20 done , tot steps 16040 , lr 3.1E-05 , loss 0.45 , qa loss 0.45 , lm loss 0.00 , avg batch size 4.0
2023-06-29 18:50:32,707 - 2:22:13 - 259.3s - INFO - __main__ - progress 10.624 , lr 2.9E-05 , loss 0.434 , qa loss 0.434 , lm loss 0.000 , avg batch size 4.0
2023-06-29 18:53:06,534 - 2:24:47 - 153.8s - INFO - __main__ - epoch 11/20 done , tot steps 17644 , lr 2.8E-05 , loss 0.44 , qa loss 0.44 , lm loss 0.00 , avg batch size 4.0
2023-06-29 18:57:19,965 - 2:29:00 - 253.4s - INFO - __main__ - progress 11.624 , lr 2.6E-05 , loss 0.405 , qa loss 0.405 , lm loss 0.000 , avg batch size 4.0
2023-06-29 19:00:00,162 - 2:31:40 - 160.2s - INFO - __main__ - epoch 12/20 done , tot steps 19248 , lr 2.5E-05 , loss 0.42 , qa loss 0.42 , lm loss 0.00 , avg batch size 4.0
2023-06-29 19:04:10,060 - 2:35:50 - 249.9s - INFO - __main__ - progress 12.624 , lr 2.3E-05 , loss 0.386 , qa loss 0.386 , lm loss 0.000 , avg batch size 4.0
2023-06-29 19:06:51,755 - 2:38:32 - 161.7s - INFO - __main__ - epoch 13/20 done , tot steps 20852 , lr 2.2E-05 , loss 0.39 , qa loss 0.39 , lm loss 0.00 , avg batch size 4.0
2023-06-29 19:11:10,187 - 2:42:50 - 258.4s - INFO - __main__ - progress 13.624 , lr 2.0E-05 , loss 0.387 , qa loss 0.387 , lm loss 0.000 , avg batch size 4.0
2023-06-29 19:13:44,761 - 2:45:25 - 154.6s - INFO - __main__ - epoch 14/20 done , tot steps 22456 , lr 1.9E-05 , loss 0.39 , qa loss 0.39 , lm loss 0.00 , avg batch size 4.0
2023-06-29 19:18:02,976 - 2:49:43 - 258.2s - INFO - __main__ - progress 14.624 , lr 1.7E-05 , loss 0.378 , qa loss 0.378 , lm loss 0.000 , avg batch size 4.0
2023-06-29 19:20:39,359 - 2:52:20 - 156.4s - INFO - __main__ - epoch 15/20 done , tot steps 24060 , lr 1.6E-05 , loss 0.38 , qa loss 0.38 , lm loss 0.00 , avg batch size 4.0
2023-06-29 19:24:55,042 - 2:56:35 - 255.7s - INFO - __main__ - progress 15.624 , lr 1.4E-05 , loss 0.367 , qa loss 0.367 , lm loss 0.000 , avg batch size 4.0
2023-06-29 19:27:33,955 - 2:59:14 - 158.9s - INFO - __main__ - epoch 16/20 done , tot steps 25664 , lr 1.3E-05 , loss 0.36 , qa loss 0.36 , lm loss 0.00 , avg batch size 4.0
2023-06-29 19:31:52,314 - 3:03:33 - 258.4s - INFO - __main__ - progress 16.624 , lr 1.1E-05 , loss 0.354 , qa loss 0.354 , lm loss 0.000 , avg batch size 4.0
2023-06-29 19:34:28,525 - 3:06:09 - 156.2s - INFO - __main__ - epoch 17/20 done , tot steps 27268 , lr 9.4E-06 , loss 0.36 , qa loss 0.36 , lm loss 0.00 , avg batch size 4.0
2023-06-29 19:38:44,117 - 3:10:24 - 255.6s - INFO - __main__ - progress 17.624 , lr 7.4E-06 , loss 0.361 , qa loss 0.361 , lm loss 0.000 , avg batch size 4.0
2023-06-29 19:41:19,651 - 3:13:00 - 155.5s - INFO - __main__ - epoch 18/20 done , tot steps 28872 , lr 6.3E-06 , loss 0.35 , qa loss 0.35 , lm loss 0.00 , avg batch size 4.0
2023-06-29 19:45:40,881 - 3:17:21 - 261.2s - INFO - __main__ - progress 18.624 , lr 4.3E-06 , loss 0.335 , qa loss 0.335 , lm loss 0.000 , avg batch size 4.0
2023-06-29 19:48:07,793 - 3:19:48 - 146.9s - INFO - __main__ - epoch 19/20 done , tot steps 30476 , lr 3.1E-06 , loss 0.34 , qa loss 0.34 , lm loss 0.00 , avg batch size 4.0
2023-06-29 19:52:26,160 - 3:24:06 - 258.4s - INFO - __main__ - progress 19.624 , lr 1.2E-06 , loss 0.355 , qa loss 0.355 , lm loss 0.000 , avg batch size 4.0
2023-06-29 19:55:12,087 - 3:26:52 - 165.9s - INFO - __main__ - epoch 20/20 done , tot steps 32080 , lr 1.6E-08 , loss 0.34 , qa loss 0.34 , lm loss 0.00 , avg batch size 4.0
The current active adapter is Stack[srl]
The task with which model is saved srl
2023-06-29 19:55:21,581 - 3:27:02 - 9.5s - INFO - __main__ - start to train { task: ['woz.en'], seq train type: lll }
2023-06-29 19:55:25,162 - 3:27:05 - 3.6s - INFO - utils - writing extra data in ../../model_cm40/gpt2/lll/sst_srl_woz.en_0.0/srl/lm.csv ...
2023-06-29 19:55:25,226 - 3:27:05 - 0.1s - INFO - __main__ - extra training data size: 0
2023-06-29 19:55:25,674 - 3:27:06 - 0.4s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
2023-06-29 19:55:28,462 - 3:27:09 - 2.8s - INFO - __main__ - len of train dataset: 2536 , max train batch size 4 , num of opt steps: 50720
2023-06-29 19:56:52,973 - 3:28:33 - 84.5s - INFO - __main__ - epoch 1/20 done , tot steps 634 , lr 5.9E-05 , loss 2.94 , qa loss 2.94 , lm loss 0.00 , avg batch size 4.0
2023-06-29 19:58:17,862 - 3:29:58 - 84.9s - INFO - __main__ - epoch 2/20 done , tot steps 1268 , lr 5.6E-05 , loss 0.62 , qa loss 0.62 , lm loss 0.00 , avg batch size 4.0
2023-06-29 19:59:11,436 - 3:30:52 - 53.6s - INFO - __main__ - epoch 3/20 done , tot steps 1902 , lr 5.3E-05 , loss 0.42 , qa loss 0.42 , lm loss 0.00 , avg batch size 4.0
2023-06-29 20:00:18,025 - 3:31:58 - 66.6s - INFO - __main__ - epoch 4/20 done , tot steps 2536 , lr 5.0E-05 , loss 0.34 , qa loss 0.34 , lm loss 0.00 , avg batch size 4.0
2023-06-29 20:01:27,512 - 3:33:08 - 69.5s - INFO - __main__ - epoch 5/20 done , tot steps 3170 , lr 4.7E-05 , loss 0.28 , qa loss 0.28 , lm loss 0.00 , avg batch size 4.0
2023-06-29 20:02:33,949 - 3:34:14 - 66.4s - INFO - __main__ - epoch 6/20 done , tot steps 3804 , lr 4.4E-05 , loss 0.25 , qa loss 0.25 , lm loss 0.00 , avg batch size 4.0
2023-06-29 20:03:40,971 - 3:35:21 - 67.0s - INFO - __main__ - epoch 7/20 done , tot steps 4438 , lr 4.1E-05 , loss 0.24 , qa loss 0.24 , lm loss 0.00 , avg batch size 4.0
2023-06-29 20:04:46,797 - 3:36:27 - 65.8s - INFO - __main__ - epoch 8/20 done , tot steps 5072 , lr 3.8E-05 , loss 0.21 , qa loss 0.21 , lm loss 0.00 , avg batch size 4.0
2023-06-29 20:05:54,153 - 3:37:34 - 67.4s - INFO - __main__ - epoch 9/20 done , tot steps 5706 , lr 3.4E-05 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-06-29 20:07:01,164 - 3:38:41 - 67.0s - INFO - __main__ - epoch 10/20 done , tot steps 6340 , lr 3.1E-05 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-06-29 20:08:08,464 - 3:39:49 - 67.3s - INFO - __main__ - epoch 11/20 done , tot steps 6974 , lr 2.8E-05 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-06-29 20:09:16,458 - 3:40:57 - 68.0s - INFO - __main__ - epoch 12/20 done , tot steps 7608 , lr 2.5E-05 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
2023-06-29 20:10:24,924 - 3:42:05 - 68.5s - INFO - __main__ - epoch 13/20 done , tot steps 8242 , lr 2.2E-05 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-06-29 20:11:34,410 - 3:43:15 - 69.5s - INFO - __main__ - epoch 14/20 done , tot steps 8876 , lr 1.9E-05 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-06-29 20:12:45,353 - 3:44:26 - 70.9s - INFO - __main__ - epoch 15/20 done , tot steps 9510 , lr 1.6E-05 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-06-29 20:13:56,539 - 3:45:37 - 71.2s - INFO - __main__ - epoch 16/20 done , tot steps 10144 , lr 1.3E-05 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-06-29 20:15:08,060 - 3:46:48 - 71.5s - INFO - __main__ - epoch 17/20 done , tot steps 10778 , lr 9.4E-06 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
2023-06-29 20:16:17,559 - 3:47:58 - 69.5s - INFO - __main__ - epoch 18/20 done , tot steps 11412 , lr 6.3E-06 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
2023-06-29 20:17:26,698 - 3:49:07 - 69.1s - INFO - __main__ - epoch 19/20 done , tot steps 12046 , lr 3.1E-06 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-06-29 20:18:43,645 - 3:50:24 - 76.9s - INFO - __main__ - epoch 20/20 done , tot steps 12680 , lr 1.5E-08 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
The current active adapter is Stack[woz_en]
The task with which model is saved woz_en
Wall Execution time: 03:50:22
CPU Execution time: 03:51:44