Not all gpus support fp16 training! Will use fp32 instead.
2023-06-29 16:49:21,609 - 0:00:05 - 0.0s - INFO - __main__ - args = Namespace(REG_TYPE_KEYS=['mas', 'ewc'], adam_epsilon=0.0001, add_task_tokens=False, data_dir='../../data', debug=False, decay_style='linear', device_ids=[2], dynamic_epochs=False, fp32=True, gen_lm_sample_percentage=0.0, learning_rate=6.25e-05, lm_lambda=0.0, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[40537.0], min_batch_size=4, min_n_steps=1500, model_dir_root='../../model_aamm1/gpt2/lll/sst_srl_woz.en_0.0', model_name='gpt2', n_gpus=1, n_train_epochs={'sst': 20, 'srl': 20, 'woz.en': 20}, n_warmup_ratio=0.005, n_workers=75, qp_margin=0.5, real_sample=False, reg_lambda=1.0, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[14187], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[14187], unbound=0, use_sep=False, weight_decay=0.01)
2023-06-29 16:49:21,610 - 0:00:05 - 0.0s - INFO - __main__ - start to train { task: ['sst'], seq train type: lll }
2023-06-29 16:49:21,618 - 0:00:05 - 0.0s - INFO - __main__ - extra training data size: 0
2023-06-29 16:49:25,091 - 0:00:09 - 3.5s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
2023-06-29 16:49:27,053 - 0:00:11 - 2.0s - INFO - __main__ - len of train dataset: 6920 , max train batch size 4 , num of opt steps: 138400
/home/student/2021/cs21mtech11006/anaconda3/envs/lamol/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
2023-06-29 16:50:46,924 - 0:01:31 - 79.9s - INFO - __main__ - progress 0.578 , lr 6.1E-05 , loss 2.729 , qa loss 2.729 , lm loss 0.000 , avg batch size 4.0
2023-06-29 16:52:19,456 - 0:03:03 - 92.5s - INFO - __main__ - epoch 1/20 done , tot steps 1730 , lr 5.9E-05 , loss 1.69 , qa loss 1.69 , lm loss 0.00 , avg batch size 4.0
2023-06-29 16:54:19,684 - 0:05:03 - 120.2s - INFO - __main__ - progress 1.578 , lr 5.8E-05 , loss 0.208 , qa loss 0.208 , lm loss 0.000 , avg batch size 4.0
2023-06-29 16:55:50,593 - 0:06:34 - 90.9s - INFO - __main__ - epoch 2/20 done , tot steps 3460 , lr 5.6E-05 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-06-29 16:57:48,776 - 0:08:33 - 118.2s - INFO - __main__ - progress 2.578 , lr 5.4E-05 , loss 0.183 , qa loss 0.183 , lm loss 0.000 , avg batch size 4.0
2023-06-29 16:59:21,544 - 0:10:05 - 92.8s - INFO - __main__ - epoch 3/20 done , tot steps 5190 , lr 5.3E-05 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-06-29 17:01:21,674 - 0:12:05 - 120.1s - INFO - __main__ - progress 3.578 , lr 5.1E-05 , loss 0.159 , qa loss 0.159 , lm loss 0.000 , avg batch size 4.0
2023-06-29 17:02:52,865 - 0:13:37 - 91.2s - INFO - __main__ - epoch 4/20 done , tot steps 6920 , lr 5.0E-05 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-06-29 17:04:53,182 - 0:15:37 - 120.3s - INFO - __main__ - progress 4.578 , lr 4.8E-05 , loss 0.142 , qa loss 0.142 , lm loss 0.000 , avg batch size 4.0
2023-06-29 17:06:23,907 - 0:17:08 - 90.7s - INFO - __main__ - epoch 5/20 done , tot steps 8650 , lr 4.7E-05 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
2023-06-29 17:08:22,691 - 0:19:07 - 118.8s - INFO - __main__ - progress 5.578 , lr 4.5E-05 , loss 0.127 , qa loss 0.127 , lm loss 0.000 , avg batch size 4.0
2023-06-29 17:09:54,063 - 0:20:38 - 91.4s - INFO - __main__ - epoch 6/20 done , tot steps 10380 , lr 4.4E-05 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-06-29 17:11:52,998 - 0:22:37 - 118.9s - INFO - __main__ - progress 6.578 , lr 4.2E-05 , loss 0.114 , qa loss 0.114 , lm loss 0.000 , avg batch size 4.0
2023-06-29 17:13:25,148 - 0:24:09 - 92.2s - INFO - __main__ - epoch 7/20 done , tot steps 12110 , lr 4.1E-05 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2023-06-29 17:15:24,137 - 0:26:08 - 119.0s - INFO - __main__ - progress 7.578 , lr 3.9E-05 , loss 0.100 , qa loss 0.100 , lm loss 0.000 , avg batch size 4.0
2023-06-29 17:16:55,863 - 0:27:40 - 91.7s - INFO - __main__ - epoch 8/20 done , tot steps 13840 , lr 3.8E-05 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-06-29 17:18:55,936 - 0:29:40 - 120.1s - INFO - __main__ - progress 8.578 , lr 3.6E-05 , loss 0.104 , qa loss 0.104 , lm loss 0.000 , avg batch size 4.0
2023-06-29 17:20:27,326 - 0:31:11 - 91.4s - INFO - __main__ - epoch 9/20 done , tot steps 15570 , lr 3.4E-05 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-06-29 17:22:28,953 - 0:33:13 - 121.6s - INFO - __main__ - progress 9.578 , lr 3.3E-05 , loss 0.108 , qa loss 0.108 , lm loss 0.000 , avg batch size 4.0
2023-06-29 17:23:59,025 - 0:34:43 - 90.1s - INFO - __main__ - epoch 10/20 done , tot steps 17300 , lr 3.1E-05 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 4.0
2023-06-29 17:26:00,100 - 0:36:44 - 121.1s - INFO - __main__ - progress 10.578 , lr 2.9E-05 , loss 0.082 , qa loss 0.082 , lm loss 0.000 , avg batch size 4.0
2023-06-29 17:27:30,478 - 0:38:14 - 90.4s - INFO - __main__ - epoch 11/20 done , tot steps 19030 , lr 2.8E-05 , loss 0.08 , qa loss 0.08 , lm loss 0.00 , avg batch size 4.0
2023-06-29 17:29:29,229 - 0:40:13 - 118.8s - INFO - __main__ - progress 11.578 , lr 2.6E-05 , loss 0.076 , qa loss 0.076 , lm loss 0.000 , avg batch size 4.0
2023-06-29 17:31:01,752 - 0:41:46 - 92.5s - INFO - __main__ - epoch 12/20 done , tot steps 20760 , lr 2.5E-05 , loss 0.08 , qa loss 0.08 , lm loss 0.00 , avg batch size 4.0
2023-06-29 17:33:01,447 - 0:43:45 - 119.7s - INFO - __main__ - progress 12.578 , lr 2.3E-05 , loss 0.068 , qa loss 0.068 , lm loss 0.000 , avg batch size 4.0
2023-06-29 17:34:33,967 - 0:45:18 - 92.5s - INFO - __main__ - epoch 13/20 done , tot steps 22490 , lr 2.2E-05 , loss 0.07 , qa loss 0.07 , lm loss 0.00 , avg batch size 4.0
2023-06-29 17:36:37,247 - 0:47:21 - 123.3s - INFO - __main__ - progress 13.578 , lr 2.0E-05 , loss 0.073 , qa loss 0.073 , lm loss 0.000 , avg batch size 4.0
2023-06-29 17:38:04,875 - 0:48:49 - 87.6s - INFO - __main__ - epoch 14/20 done , tot steps 24220 , lr 1.9E-05 , loss 0.07 , qa loss 0.07 , lm loss 0.00 , avg batch size 4.0
2023-06-29 17:40:07,330 - 0:50:51 - 122.5s - INFO - __main__ - progress 14.578 , lr 1.7E-05 , loss 0.056 , qa loss 0.056 , lm loss 0.000 , avg batch size 4.0
2023-06-29 17:41:35,674 - 0:52:19 - 88.3s - INFO - __main__ - epoch 15/20 done , tot steps 25950 , lr 1.6E-05 , loss 0.06 , qa loss 0.06 , lm loss 0.00 , avg batch size 4.0
2023-06-29 17:43:41,135 - 0:54:25 - 125.5s - INFO - __main__ - progress 15.578 , lr 1.4E-05 , loss 0.061 , qa loss 0.061 , lm loss 0.000 , avg batch size 4.0
2023-06-29 17:45:06,966 - 0:55:51 - 85.8s - INFO - __main__ - epoch 16/20 done , tot steps 27680 , lr 1.3E-05 , loss 0.06 , qa loss 0.06 , lm loss 0.00 , avg batch size 4.0
2023-06-29 17:47:12,662 - 0:57:56 - 125.7s - INFO - __main__ - progress 16.578 , lr 1.1E-05 , loss 0.044 , qa loss 0.044 , lm loss 0.000 , avg batch size 4.0
2023-06-29 17:48:38,267 - 0:59:22 - 85.6s - INFO - __main__ - epoch 17/20 done , tot steps 29410 , lr 9.4E-06 , loss 0.05 , qa loss 0.05 , lm loss 0.00 , avg batch size 4.0
2023-06-29 17:50:41,658 - 1:01:25 - 123.4s - INFO - __main__ - progress 17.578 , lr 7.6E-06 , loss 0.049 , qa loss 0.049 , lm loss 0.000 , avg batch size 4.0
2023-06-29 17:52:08,766 - 1:02:53 - 87.1s - INFO - __main__ - epoch 18/20 done , tot steps 31140 , lr 6.3E-06 , loss 0.05 , qa loss 0.05 , lm loss 0.00 , avg batch size 4.0
2023-06-29 17:54:11,770 - 1:04:56 - 123.0s - INFO - __main__ - progress 18.578 , lr 4.5E-06 , loss 0.048 , qa loss 0.048 , lm loss 0.000 , avg batch size 4.0
2023-06-29 17:55:38,808 - 1:06:23 - 87.0s - INFO - __main__ - epoch 19/20 done , tot steps 32870 , lr 3.1E-06 , loss 0.05 , qa loss 0.05 , lm loss 0.00 , avg batch size 4.0
2023-06-29 17:57:41,967 - 1:08:26 - 123.2s - INFO - __main__ - progress 19.578 , lr 1.3E-06 , loss 0.044 , qa loss 0.044 , lm loss 0.000 , avg batch size 4.0
2023-06-29 17:59:05,347 - 1:09:49 - 83.4s - INFO - __main__ - epoch 20/20 done , tot steps 34600 , lr 1.5E-08 , loss 0.04 , qa loss 0.04 , lm loss 0.00 , avg batch size 4.0
2023-06-29 17:59:14,151 - 1:09:58 - 8.8s - INFO - __main__ - start to train { task: ['srl'], seq train type: lll }
2023-06-29 17:59:18,725 - 1:10:03 - 4.6s - INFO - utils - writing extra data in ../../model_aamm1/gpt2/lll/sst_srl_woz.en_0.0/sst/lm.csv ...
2023-06-29 17:59:18,749 - 1:10:03 - 0.0s - INFO - __main__ - extra training data size: 0
2023-06-29 17:59:19,019 - 1:10:03 - 0.3s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[sst]
The task with which model is saved sst
2023-06-29 17:59:21,537 - 1:10:05 - 2.5s - INFO - __main__ - len of train dataset: 6414 , max train batch size 4 , num of opt steps: 128280
2023-06-29 18:03:42,501 - 1:14:26 - 261.0s - INFO - __main__ - progress 0.624 , lr 6.1E-05 , loss 3.960 , qa loss 3.960 , lm loss 0.000 , avg batch size 4.0
2023-06-29 18:06:13,880 - 1:16:58 - 151.4s - INFO - __main__ - epoch 1/20 done , tot steps 1604 , lr 5.9E-05 , loss 2.82 , qa loss 2.82 , lm loss 0.00 , avg batch size 4.0
2023-06-29 18:10:30,616 - 1:21:14 - 256.7s - INFO - __main__ - progress 1.624 , lr 5.7E-05 , loss 0.778 , qa loss 0.778 , lm loss 0.000 , avg batch size 4.0
2023-06-29 18:13:06,945 - 1:23:51 - 156.3s - INFO - __main__ - epoch 2/20 done , tot steps 3208 , lr 5.6E-05 , loss 0.75 , qa loss 0.75 , lm loss 0.00 , avg batch size 4.0
2023-06-29 18:17:34,782 - 1:28:19 - 267.8s - INFO - __main__ - progress 2.624 , lr 5.4E-05 , loss 0.645 , qa loss 0.645 , lm loss 0.000 , avg batch size 4.0
2023-06-29 18:20:03,913 - 1:30:48 - 149.1s - INFO - __main__ - epoch 3/20 done , tot steps 4812 , lr 5.3E-05 , loss 0.64 , qa loss 0.64 , lm loss 0.00 , avg batch size 4.0
2023-06-29 18:24:25,708 - 1:35:10 - 261.8s - INFO - __main__ - progress 3.624 , lr 5.1E-05 , loss 0.568 , qa loss 0.568 , lm loss 0.000 , avg batch size 4.0
2023-06-29 18:27:01,459 - 1:37:45 - 155.8s - INFO - __main__ - epoch 4/20 done , tot steps 6416 , lr 5.0E-05 , loss 0.56 , qa loss 0.56 , lm loss 0.00 , avg batch size 4.0
2023-06-29 18:31:21,117 - 1:42:05 - 259.7s - INFO - __main__ - progress 4.624 , lr 4.8E-05 , loss 0.517 , qa loss 0.517 , lm loss 0.000 , avg batch size 4.0
2023-06-29 18:33:56,021 - 1:44:40 - 154.9s - INFO - __main__ - epoch 5/20 done , tot steps 8020 , lr 4.7E-05 , loss 0.51 , qa loss 0.51 , lm loss 0.00 , avg batch size 4.0
2023-06-29 18:38:17,122 - 1:49:01 - 261.1s - INFO - __main__ - progress 5.624 , lr 4.5E-05 , loss 0.465 , qa loss 0.465 , lm loss 0.000 , avg batch size 4.0
2023-06-29 18:40:47,233 - 1:51:31 - 150.1s - INFO - __main__ - epoch 6/20 done , tot steps 9624 , lr 4.4E-05 , loss 0.46 , qa loss 0.46 , lm loss 0.00 , avg batch size 4.0
2023-06-29 18:45:07,431 - 1:55:51 - 260.2s - INFO - __main__ - progress 6.624 , lr 4.2E-05 , loss 0.431 , qa loss 0.431 , lm loss 0.000 , avg batch size 4.0
2023-06-29 18:47:40,211 - 1:58:24 - 152.8s - INFO - __main__ - epoch 7/20 done , tot steps 11228 , lr 4.1E-05 , loss 0.44 , qa loss 0.44 , lm loss 0.00 , avg batch size 4.0
2023-06-29 18:52:00,070 - 2:02:44 - 259.9s - INFO - __main__ - progress 7.624 , lr 3.9E-05 , loss 0.398 , qa loss 0.398 , lm loss 0.000 , avg batch size 4.0
2023-06-29 18:54:33,306 - 2:05:17 - 153.2s - INFO - __main__ - epoch 8/20 done , tot steps 12832 , lr 3.8E-05 , loss 0.40 , qa loss 0.40 , lm loss 0.00 , avg batch size 4.0
2023-06-29 18:58:56,101 - 2:09:40 - 262.8s - INFO - __main__ - progress 8.624 , lr 3.6E-05 , loss 0.382 , qa loss 0.382 , lm loss 0.000 , avg batch size 4.0
2023-06-29 19:01:33,173 - 2:12:17 - 157.1s - INFO - __main__ - epoch 9/20 done , tot steps 14436 , lr 3.4E-05 , loss 0.38 , qa loss 0.38 , lm loss 0.00 , avg batch size 4.0
2023-06-29 19:05:52,220 - 2:16:36 - 259.0s - INFO - __main__ - progress 9.624 , lr 3.2E-05 , loss 0.362 , qa loss 0.362 , lm loss 0.000 , avg batch size 4.0
2023-06-29 19:08:26,915 - 2:19:11 - 154.7s - INFO - __main__ - epoch 10/20 done , tot steps 16040 , lr 3.1E-05 , loss 0.36 , qa loss 0.36 , lm loss 0.00 , avg batch size 4.0
2023-06-29 19:12:44,384 - 2:23:28 - 257.5s - INFO - __main__ - progress 10.624 , lr 2.9E-05 , loss 0.333 , qa loss 0.333 , lm loss 0.000 , avg batch size 4.0
2023-06-29 19:15:20,684 - 2:26:04 - 156.3s - INFO - __main__ - epoch 11/20 done , tot steps 17644 , lr 2.8E-05 , loss 0.34 , qa loss 0.34 , lm loss 0.00 , avg batch size 4.0
2023-06-29 19:19:47,721 - 2:30:32 - 267.0s - INFO - __main__ - progress 11.624 , lr 2.6E-05 , loss 0.324 , qa loss 0.324 , lm loss 0.000 , avg batch size 4.0
2023-06-29 19:22:21,027 - 2:33:05 - 153.3s - INFO - __main__ - epoch 12/20 done , tot steps 19248 , lr 2.5E-05 , loss 0.32 , qa loss 0.32 , lm loss 0.00 , avg batch size 4.0
2023-06-29 19:26:43,391 - 2:37:27 - 262.4s - INFO - __main__ - progress 12.624 , lr 2.3E-05 , loss 0.305 , qa loss 0.305 , lm loss 0.000 , avg batch size 4.0
2023-06-29 19:29:18,854 - 2:40:03 - 155.5s - INFO - __main__ - epoch 13/20 done , tot steps 20852 , lr 2.2E-05 , loss 0.30 , qa loss 0.30 , lm loss 0.00 , avg batch size 4.0
2023-06-29 19:33:43,574 - 2:44:27 - 264.7s - INFO - __main__ - progress 13.624 , lr 2.0E-05 , loss 0.286 , qa loss 0.286 , lm loss 0.000 , avg batch size 4.0
2023-06-29 19:36:15,106 - 2:46:59 - 151.5s - INFO - __main__ - epoch 14/20 done , tot steps 22456 , lr 1.9E-05 , loss 0.28 , qa loss 0.28 , lm loss 0.00 , avg batch size 4.0
2023-06-29 19:40:32,298 - 2:51:16 - 257.2s - INFO - __main__ - progress 14.624 , lr 1.7E-05 , loss 0.263 , qa loss 0.263 , lm loss 0.000 , avg batch size 4.0
2023-06-29 19:43:12,398 - 2:53:56 - 160.1s - INFO - __main__ - epoch 15/20 done , tot steps 24060 , lr 1.6E-05 , loss 0.27 , qa loss 0.27 , lm loss 0.00 , avg batch size 4.0
2023-06-29 19:47:25,149 - 2:58:09 - 252.8s - INFO - __main__ - progress 15.624 , lr 1.4E-05 , loss 0.262 , qa loss 0.262 , lm loss 0.000 , avg batch size 4.0
2023-06-29 19:50:06,822 - 3:00:51 - 161.7s - INFO - __main__ - epoch 16/20 done , tot steps 25664 , lr 1.3E-05 , loss 0.27 , qa loss 0.27 , lm loss 0.00 , avg batch size 4.0
2023-06-29 19:54:29,105 - 3:05:13 - 262.3s - INFO - __main__ - progress 16.624 , lr 1.1E-05 , loss 0.258 , qa loss 0.258 , lm loss 0.000 , avg batch size 4.0
2023-06-29 19:57:02,642 - 3:07:46 - 153.5s - INFO - __main__ - epoch 17/20 done , tot steps 27268 , lr 9.4E-06 , loss 0.26 , qa loss 0.26 , lm loss 0.00 , avg batch size 4.0
2023-06-29 20:01:20,101 - 3:12:04 - 257.5s - INFO - __main__ - progress 17.624 , lr 7.4E-06 , loss 0.239 , qa loss 0.239 , lm loss 0.000 , avg batch size 4.0
2023-06-29 20:03:55,229 - 3:14:39 - 155.1s - INFO - __main__ - epoch 18/20 done , tot steps 28872 , lr 6.3E-06 , loss 0.24 , qa loss 0.24 , lm loss 0.00 , avg batch size 4.0
2023-06-29 20:08:09,489 - 3:18:53 - 254.3s - INFO - __main__ - progress 18.624 , lr 4.3E-06 , loss 0.245 , qa loss 0.245 , lm loss 0.000 , avg batch size 4.0
2023-06-29 20:10:51,625 - 3:21:35 - 162.1s - INFO - __main__ - epoch 19/20 done , tot steps 30476 , lr 3.1E-06 , loss 0.24 , qa loss 0.24 , lm loss 0.00 , avg batch size 4.0
2023-06-29 20:14:56,806 - 3:25:41 - 245.2s - INFO - __main__ - progress 19.624 , lr 1.2E-06 , loss 0.233 , qa loss 0.233 , lm loss 0.000 , avg batch size 4.0
2023-06-29 20:17:06,375 - 3:27:50 - 129.6s - INFO - __main__ - epoch 20/20 done , tot steps 32080 , lr 1.6E-08 , loss 0.24 , qa loss 0.24 , lm loss 0.00 , avg batch size 4.0
2023-06-29 20:17:14,963 - 3:27:59 - 8.6s - INFO - __main__ - start to train { task: ['woz.en'], seq train type: lll }
2023-06-29 20:17:19,190 - 3:28:03 - 4.2s - INFO - utils - writing extra data in ../../model_aamm1/gpt2/lll/sst_srl_woz.en_0.0/srl/lm.csv ...
2023-06-29 20:17:19,262 - 3:28:03 - 0.1s - INFO - __main__ - extra training data size: 0
2023-06-29 20:17:19,554 - 3:28:03 - 0.3s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[srl]
The task with which model is saved srl
2023-06-29 20:17:23,361 - 3:28:07 - 3.8s - INFO - __main__ - len of train dataset: 2536 , max train batch size 4 , num of opt steps: 50720
2023-06-29 20:18:34,383 - 3:29:18 - 71.0s - INFO - __main__ - epoch 1/20 done , tot steps 634 , lr 5.9E-05 , loss 4.58 , qa loss 4.58 , lm loss 0.00 , avg batch size 4.0
2023-06-29 20:19:43,696 - 3:30:28 - 69.3s - INFO - __main__ - epoch 2/20 done , tot steps 1268 , lr 5.6E-05 , loss 0.51 , qa loss 0.51 , lm loss 0.00 , avg batch size 4.0
2023-06-29 20:20:56,509 - 3:31:40 - 72.8s - INFO - __main__ - epoch 3/20 done , tot steps 1902 , lr 5.3E-05 , loss 0.34 , qa loss 0.34 , lm loss 0.00 , avg batch size 4.0
2023-06-29 20:22:07,176 - 3:32:51 - 70.7s - INFO - __main__ - epoch 4/20 done , tot steps 2536 , lr 5.0E-05 , loss 0.28 , qa loss 0.28 , lm loss 0.00 , avg batch size 4.0
2023-06-29 20:23:17,226 - 3:34:01 - 70.1s - INFO - __main__ - epoch 5/20 done , tot steps 3170 , lr 4.7E-05 , loss 0.23 , qa loss 0.23 , lm loss 0.00 , avg batch size 4.0
2023-06-29 20:24:26,162 - 3:35:10 - 68.9s - INFO - __main__ - epoch 6/20 done , tot steps 3804 , lr 4.4E-05 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-06-29 20:25:35,803 - 3:36:20 - 69.6s - INFO - __main__ - epoch 7/20 done , tot steps 4438 , lr 4.1E-05 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-06-29 20:26:43,759 - 3:37:28 - 68.0s - INFO - __main__ - epoch 8/20 done , tot steps 5072 , lr 3.8E-05 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-06-29 20:27:51,954 - 3:38:36 - 68.2s - INFO - __main__ - epoch 9/20 done , tot steps 5706 , lr 3.4E-05 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-06-29 20:29:01,321 - 3:39:45 - 69.4s - INFO - __main__ - epoch 10/20 done , tot steps 6340 , lr 3.1E-05 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-06-29 20:30:09,491 - 3:40:53 - 68.2s - INFO - __main__ - epoch 11/20 done , tot steps 6974 , lr 2.8E-05 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
2023-06-29 20:31:18,221 - 3:42:02 - 68.7s - INFO - __main__ - epoch 12/20 done , tot steps 7608 , lr 2.5E-05 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-06-29 20:32:26,958 - 3:43:11 - 68.7s - INFO - __main__ - epoch 13/20 done , tot steps 8242 , lr 2.2E-05 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-06-29 20:33:36,236 - 3:44:20 - 69.3s - INFO - __main__ - epoch 14/20 done , tot steps 8876 , lr 1.9E-05 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-06-29 20:34:44,834 - 3:45:29 - 68.6s - INFO - __main__ - epoch 15/20 done , tot steps 9510 , lr 1.6E-05 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-06-29 20:35:53,681 - 3:46:37 - 68.8s - INFO - __main__ - epoch 16/20 done , tot steps 10144 , lr 1.3E-05 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-06-29 20:37:02,585 - 3:47:46 - 68.9s - INFO - __main__ - epoch 17/20 done , tot steps 10778 , lr 9.4E-06 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-06-29 20:37:57,523 - 3:48:41 - 54.9s - INFO - __main__ - epoch 18/20 done , tot steps 11412 , lr 6.3E-06 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 4.0
2023-06-29 20:38:48,032 - 3:49:32 - 50.5s - INFO - __main__ - epoch 19/20 done , tot steps 12046 , lr 3.1E-06 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 4.0
2023-06-29 20:39:43,384 - 3:50:27 - 55.4s - INFO - __main__ - epoch 20/20 done , tot steps 12680 , lr 1.5E-08 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 4.0
The current active adapter is Stack[woz_en]
The task with which model is saved woz_en
Wall Execution time: 03:50:30
CPU Execution time: 03:52:35
