Not all gpus support fp16 training! Will use fp32 instead.
2023-04-24 18:11:24,915 - 0:00:04 - 0.0s - INFO - __main__ - args = Namespace(REG_TYPE_KEYS=['mas', 'ewc'], adam_epsilon=0.0001, add_task_tokens=False, data_dir='../../data', debug=False, decay_style='linear', device_ids=[1, 2, 3, 4, 5, 6], dynamic_epochs=False, fp32=True, gen_lm_sample_percentage=0.0, learning_rate=6.25e-05, lm_lambda=0.0, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[32429.600000000002, 42158.48, 42158.48, 42158.48, 42158.48, 42158.48], min_batch_size=4, min_n_steps=1500, model_dir_root='../../model_conf2/gpt2/lll/sst_srl_woz.en_0.0', model_name='gpt2', n_gpus=6, n_train_epochs={'sst': 12, 'srl': 12, 'woz.en': 12}, n_warmup_ratio=0.005, n_workers=75, qp_margin=0.5, real_sample=False, reg_lambda=1.0, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[11350, 14755, 14755, 14755, 14755, 14755], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[11350, 14755, 14755, 14755, 14755, 14755], unbound=0, use_sep=False, weight_decay=0.01)
2023-04-24 18:11:24,915 - 0:00:04 - 0.0s - INFO - __main__ - start to train { task: ['sst'], seq train type: lll }
2023-04-24 18:11:24,915 - 0:00:04 - 0.0s - INFO - __main__ - extra training data size: 0
2023-04-24 18:11:28,968 - 0:00:08 - 4.1s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
2023-04-24 18:11:31,811 - 0:00:10 - 2.8s - INFO - __main__ - len of train dataset: 6920 , max train batch size 4 , num of opt steps: 83040
/home/student/2021/cs21mtech11006/anaconda3/envs/lamol/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/home/student/2021/cs21mtech11006/anaconda3/envs/lamol/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
2023-04-24 18:13:33,744 - 0:02:12 - 121.9s - INFO - __main__ - progress 0.578 , lr 6.0E-05 , loss 3.182 , qa loss 3.182 , lm loss 0.000 , avg batch size 4.0
2023-04-24 18:15:06,665 - 0:03:45 - 92.9s - INFO - __main__ - epoch 1/12 done , tot steps 1730 , lr 5.7E-05 , loss 1.96 , qa loss 1.96 , lm loss 0.00 , avg batch size 4.0
2023-04-24 18:17:05,476 - 0:05:44 - 118.8s - INFO - __main__ - progress 1.578 , lr 5.4E-05 , loss 0.287 , qa loss 0.287 , lm loss 0.000 , avg batch size 4.0
2023-04-24 18:18:31,218 - 0:07:10 - 85.7s - INFO - __main__ - epoch 2/12 done , tot steps 3460 , lr 5.2E-05 , loss 0.27 , qa loss 0.27 , lm loss 0.00 , avg batch size 4.0
2023-04-24 18:20:30,135 - 0:09:09 - 118.9s - INFO - __main__ - progress 2.578 , lr 4.9E-05 , loss 0.251 , qa loss 0.251 , lm loss 0.000 , avg batch size 4.0
2023-04-24 18:21:57,584 - 0:10:36 - 87.4s - INFO - __main__ - epoch 3/12 done , tot steps 5190 , lr 4.7E-05 , loss 0.25 , qa loss 0.25 , lm loss 0.00 , avg batch size 4.0
2023-04-24 18:23:58,793 - 0:12:37 - 121.2s - INFO - __main__ - progress 3.578 , lr 4.4E-05 , loss 0.234 , qa loss 0.234 , lm loss 0.000 , avg batch size 4.0
2023-04-24 18:25:26,617 - 0:14:05 - 87.8s - INFO - __main__ - epoch 4/12 done , tot steps 6920 , lr 4.2E-05 , loss 0.24 , qa loss 0.24 , lm loss 0.00 , avg batch size 4.0
2023-04-24 18:27:26,879 - 0:16:06 - 120.3s - INFO - __main__ - progress 4.578 , lr 3.9E-05 , loss 0.221 , qa loss 0.221 , lm loss 0.000 , avg batch size 4.0
2023-04-24 18:28:55,120 - 0:17:34 - 88.2s - INFO - __main__ - epoch 5/12 done , tot steps 8650 , lr 3.6E-05 , loss 0.23 , qa loss 0.23 , lm loss 0.00 , avg batch size 4.0
2023-04-24 18:30:53,552 - 0:19:32 - 118.4s - INFO - __main__ - progress 5.578 , lr 3.3E-05 , loss 0.214 , qa loss 0.214 , lm loss 0.000 , avg batch size 4.0
2023-04-24 18:32:19,834 - 0:20:59 - 86.3s - INFO - __main__ - epoch 6/12 done , tot steps 10380 , lr 3.1E-05 , loss 0.22 , qa loss 0.22 , lm loss 0.00 , avg batch size 4.0
2023-04-24 18:34:19,005 - 0:22:58 - 119.2s - INFO - __main__ - progress 6.578 , lr 2.8E-05 , loss 0.207 , qa loss 0.207 , lm loss 0.000 , avg batch size 4.0
2023-04-24 18:35:44,979 - 0:24:24 - 86.0s - INFO - __main__ - epoch 7/12 done , tot steps 12110 , lr 2.6E-05 , loss 0.21 , qa loss 0.21 , lm loss 0.00 , avg batch size 4.0
2023-04-24 18:37:45,771 - 0:26:24 - 120.8s - INFO - __main__ - progress 7.578 , lr 2.3E-05 , loss 0.216 , qa loss 0.216 , lm loss 0.000 , avg batch size 4.0
2023-04-24 18:39:13,876 - 0:27:53 - 88.1s - INFO - __main__ - epoch 8/12 done , tot steps 13840 , lr 2.1E-05 , loss 0.22 , qa loss 0.22 , lm loss 0.00 , avg batch size 4.0
2023-04-24 18:41:14,170 - 0:29:53 - 120.3s - INFO - __main__ - progress 8.578 , lr 1.8E-05 , loss 0.195 , qa loss 0.195 , lm loss 0.000 , avg batch size 4.0
2023-04-24 18:42:42,153 - 0:31:21 - 88.0s - INFO - __main__ - epoch 9/12 done , tot steps 15570 , lr 1.6E-05 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-04-24 18:44:42,951 - 0:33:22 - 120.8s - INFO - __main__ - progress 9.578 , lr 1.3E-05 , loss 0.191 , qa loss 0.191 , lm loss 0.000 , avg batch size 4.0
2023-04-24 18:46:11,022 - 0:34:50 - 88.1s - INFO - __main__ - epoch 10/12 done , tot steps 17300 , lr 1.0E-05 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-04-24 18:48:10,907 - 0:36:50 - 119.9s - INFO - __main__ - progress 10.578 , lr 7.4E-06 , loss 0.182 , qa loss 0.182 , lm loss 0.000 , avg batch size 4.0
2023-04-24 18:49:39,321 - 0:38:18 - 88.4s - INFO - __main__ - epoch 11/12 done , tot steps 19030 , lr 5.2E-06 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-04-24 18:51:40,547 - 0:40:19 - 121.2s - INFO - __main__ - progress 11.578 , lr 2.2E-06 , loss 0.181 , qa loss 0.181 , lm loss 0.000 , avg batch size 4.0
2023-04-24 18:53:09,078 - 0:41:48 - 88.5s - INFO - __main__ - epoch 12/12 done , tot steps 20760 , lr 2.6E-08 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-04-24 18:53:16,012 - 0:41:55 - 6.9s - INFO - __main__ - start to train { task: ['srl'], seq train type: lll }
2023-04-24 18:53:18,267 - 0:41:57 - 2.3s - INFO - utils - writing extra data in ../../model_conf2/gpt2/lll/sst_srl_woz.en_0.0/sst/lm.csv ...
2023-04-24 18:53:18,268 - 0:41:57 - 0.0s - INFO - __main__ - extra training data size: 0
2023-04-24 18:53:18,385 - 0:41:57 - 0.1s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[sst]
The task with which model is saved sst
2023-04-24 18:53:20,563 - 0:41:59 - 2.2s - INFO - __main__ - len of train dataset: 6414 , max train batch size 4 , num of opt steps: 76968
2023-04-24 18:56:20,398 - 0:44:59 - 179.8s - INFO - __main__ - progress 0.624 , lr 5.9E-05 , loss 6.883 , qa loss 6.883 , lm loss 0.000 , avg batch size 4.0
2023-04-24 18:58:16,161 - 0:46:55 - 115.8s - INFO - __main__ - epoch 1/12 done , tot steps 1604 , lr 5.7E-05 , loss 4.85 , qa loss 4.85 , lm loss 0.00 , avg batch size 4.0
2023-04-24 19:01:17,907 - 0:49:57 - 181.7s - INFO - __main__ - progress 1.624 , lr 5.4E-05 , loss 1.050 , qa loss 1.050 , lm loss 0.000 , avg batch size 4.0
2023-04-24 19:03:04,963 - 0:51:44 - 107.1s - INFO - __main__ - epoch 2/12 done , tot steps 3208 , lr 5.2E-05 , loss 1.00 , qa loss 1.00 , lm loss 0.00 , avg batch size 4.0
2023-04-24 19:06:07,303 - 0:54:46 - 182.3s - INFO - __main__ - progress 2.624 , lr 4.9E-05 , loss 0.815 , qa loss 0.815 , lm loss 0.000 , avg batch size 4.0
2023-04-24 19:07:53,504 - 0:56:32 - 106.2s - INFO - __main__ - epoch 3/12 done , tot steps 4812 , lr 4.7E-05 , loss 0.79 , qa loss 0.79 , lm loss 0.00 , avg batch size 4.0
2023-04-24 19:10:54,527 - 0:59:33 - 181.0s - INFO - __main__ - progress 3.624 , lr 4.4E-05 , loss 0.669 , qa loss 0.669 , lm loss 0.000 , avg batch size 4.0
2023-04-24 19:12:43,664 - 1:01:22 - 109.1s - INFO - __main__ - epoch 4/12 done , tot steps 6416 , lr 4.2E-05 , loss 0.67 , qa loss 0.67 , lm loss 0.00 , avg batch size 4.0
2023-04-24 19:15:43,041 - 1:04:22 - 179.4s - INFO - __main__ - progress 4.624 , lr 3.8E-05 , loss 0.604 , qa loss 0.604 , lm loss 0.000 , avg batch size 4.0
2023-04-24 19:17:33,953 - 1:06:13 - 110.9s - INFO - __main__ - epoch 5/12 done , tot steps 8020 , lr 3.6E-05 , loss 0.60 , qa loss 0.60 , lm loss 0.00 , avg batch size 4.0
2023-04-24 19:20:32,754 - 1:09:11 - 178.8s - INFO - __main__ - progress 5.624 , lr 3.3E-05 , loss 0.561 , qa loss 0.561 , lm loss 0.000 , avg batch size 4.0
2023-04-24 19:22:23,699 - 1:11:02 - 110.9s - INFO - __main__ - epoch 6/12 done , tot steps 9624 , lr 3.1E-05 , loss 0.55 , qa loss 0.55 , lm loss 0.00 , avg batch size 4.0
2023-04-24 19:25:21,926 - 1:14:01 - 178.2s - INFO - __main__ - progress 6.624 , lr 2.8E-05 , loss 0.498 , qa loss 0.498 , lm loss 0.000 , avg batch size 4.0
2023-04-24 19:27:12,995 - 1:15:52 - 111.1s - INFO - __main__ - epoch 7/12 done , tot steps 11228 , lr 2.6E-05 , loss 0.50 , qa loss 0.50 , lm loss 0.00 , avg batch size 4.0
2023-04-24 19:30:16,940 - 1:18:56 - 183.9s - INFO - __main__ - progress 7.624 , lr 2.3E-05 , loss 0.453 , qa loss 0.453 , lm loss 0.000 , avg batch size 4.0
2023-04-24 19:32:04,273 - 1:20:43 - 107.3s - INFO - __main__ - epoch 8/12 done , tot steps 12832 , lr 2.1E-05 , loss 0.45 , qa loss 0.45 , lm loss 0.00 , avg batch size 4.0
2023-04-24 19:35:03,061 - 1:23:42 - 178.8s - INFO - __main__ - progress 8.624 , lr 1.8E-05 , loss 0.429 , qa loss 0.429 , lm loss 0.000 , avg batch size 4.0
2023-04-24 19:36:55,520 - 1:25:34 - 112.5s - INFO - __main__ - epoch 9/12 done , tot steps 14436 , lr 1.6E-05 , loss 0.42 , qa loss 0.42 , lm loss 0.00 , avg batch size 4.0
2023-04-24 19:39:54,165 - 1:28:33 - 178.6s - INFO - __main__ - progress 9.624 , lr 1.2E-05 , loss 0.409 , qa loss 0.409 , lm loss 0.000 , avg batch size 4.0
2023-04-24 19:41:45,009 - 1:30:24 - 110.8s - INFO - __main__ - epoch 10/12 done , tot steps 16040 , lr 1.0E-05 , loss 0.40 , qa loss 0.40 , lm loss 0.00 , avg batch size 4.0
2023-04-24 19:44:47,457 - 1:33:26 - 182.4s - INFO - __main__ - progress 10.624 , lr 7.2E-06 , loss 0.386 , qa loss 0.386 , lm loss 0.000 , avg batch size 4.0
2023-04-24 19:46:34,455 - 1:35:13 - 107.0s - INFO - __main__ - epoch 11/12 done , tot steps 17644 , lr 5.2E-06 , loss 0.39 , qa loss 0.39 , lm loss 0.00 , avg batch size 4.0
2023-04-24 19:49:36,197 - 1:38:15 - 181.7s - INFO - __main__ - progress 11.624 , lr 2.0E-06 , loss 0.383 , qa loss 0.383 , lm loss 0.000 , avg batch size 4.0
2023-04-24 19:51:25,444 - 1:40:04 - 109.2s - INFO - __main__ - epoch 12/12 done , tot steps 19248 , lr 2.6E-08 , loss 0.38 , qa loss 0.38 , lm loss 0.00 , avg batch size 4.0
2023-04-24 19:51:32,263 - 1:40:11 - 6.8s - INFO - __main__ - start to train { task: ['woz.en'], seq train type: lll }
2023-04-24 19:51:36,019 - 1:40:15 - 3.8s - INFO - utils - writing extra data in ../../model_conf2/gpt2/lll/sst_srl_woz.en_0.0/srl/lm.csv ...
2023-04-24 19:51:36,020 - 1:40:15 - 0.0s - INFO - __main__ - extra training data size: 0
2023-04-24 19:51:36,147 - 1:40:15 - 0.1s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[sst]
The task with which model is saved sst
2023-04-24 19:51:39,174 - 1:40:18 - 3.0s - INFO - __main__ - len of train dataset: 2536 , max train batch size 4 , num of opt steps: 30432
2023-04-24 19:53:02,381 - 1:41:41 - 83.2s - INFO - __main__ - epoch 1/12 done , tot steps 634 , lr 5.7E-05 , loss 5.73 , qa loss 5.73 , lm loss 0.00 , avg batch size 4.0
2023-04-24 19:54:22,913 - 1:43:02 - 80.5s - INFO - __main__ - epoch 2/12 done , tot steps 1268 , lr 5.2E-05 , loss 0.83 , qa loss 0.83 , lm loss 0.00 , avg batch size 4.0
2023-04-24 19:55:41,255 - 1:44:20 - 78.3s - INFO - __main__ - epoch 3/12 done , tot steps 1902 , lr 4.7E-05 , loss 0.53 , qa loss 0.53 , lm loss 0.00 , avg batch size 4.0
2023-04-24 19:57:00,278 - 1:45:39 - 79.0s - INFO - __main__ - epoch 4/12 done , tot steps 2536 , lr 4.2E-05 , loss 0.40 , qa loss 0.40 , lm loss 0.00 , avg batch size 4.0
2023-04-24 19:58:18,743 - 1:46:57 - 78.5s - INFO - __main__ - epoch 5/12 done , tot steps 3170 , lr 3.6E-05 , loss 0.33 , qa loss 0.33 , lm loss 0.00 , avg batch size 4.0
2023-04-24 19:59:38,270 - 1:48:17 - 79.5s - INFO - __main__ - epoch 6/12 done , tot steps 3804 , lr 3.1E-05 , loss 0.30 , qa loss 0.30 , lm loss 0.00 , avg batch size 4.0
2023-04-24 20:00:56,421 - 1:49:35 - 78.2s - INFO - __main__ - epoch 7/12 done , tot steps 4438 , lr 2.6E-05 , loss 0.26 , qa loss 0.26 , lm loss 0.00 , avg batch size 4.0
2023-04-24 20:02:15,892 - 1:50:55 - 79.5s - INFO - __main__ - epoch 8/12 done , tot steps 5072 , lr 2.1E-05 , loss 0.24 , qa loss 0.24 , lm loss 0.00 , avg batch size 4.0
2023-04-24 20:03:34,787 - 1:52:13 - 78.9s - INFO - __main__ - epoch 9/12 done , tot steps 5706 , lr 1.6E-05 , loss 0.23 , qa loss 0.23 , lm loss 0.00 , avg batch size 4.0
2023-04-24 20:04:53,776 - 1:53:32 - 79.0s - INFO - __main__ - epoch 10/12 done , tot steps 6340 , lr 1.0E-05 , loss 0.21 , qa loss 0.21 , lm loss 0.00 , avg batch size 4.0
2023-04-24 20:06:12,043 - 1:54:51 - 78.3s - INFO - __main__ - epoch 11/12 done , tot steps 6974 , lr 5.2E-06 , loss 0.21 , qa loss 0.21 , lm loss 0.00 , avg batch size 4.0
2023-04-24 20:07:31,504 - 1:56:10 - 79.5s - INFO - __main__ - epoch 12/12 done , tot steps 7608 , lr 2.5E-08 , loss 0.21 , qa loss 0.21 , lm loss 0.00 , avg batch size 4.0
The current active adapter is Stack[sst]
The task with which model is saved sst
Wall Execution time: 01:56:13
CPU Execution time: 01:59:39
