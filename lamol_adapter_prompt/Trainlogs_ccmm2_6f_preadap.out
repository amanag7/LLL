Not all gpus support fp16 training! Will use fp32 instead.
2023-06-29 23:32:46,754 - 0:00:06 - 0.0s - INFO - __main__ - args = Namespace(REG_TYPE_KEYS=['mas', 'ewc'], adam_epsilon=0.0001, add_task_tokens=False, data_dir='../../data', debug=False, decay_style='linear', device_ids=[1, 2], dynamic_epochs=False, fp32=True, gen_lm_sample_percentage=0.0, learning_rate=6.25e-05, lm_lambda=0.0, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[38915.52, 42158.48], min_batch_size=4, min_n_steps=1500, model_dir_root='../../model_ccmm2/gpt2/lll/wikisql_ag_amazon_sst_srl_woz.en_0.0', model_name='gpt2', n_gpus=2, n_train_epochs={'wikisql': 20, 'ag': 20, 'amazon': 20, 'sst': 20, 'srl': 20, 'woz.en': 20}, n_warmup_ratio=0.005, n_workers=75, qp_margin=0.5, real_sample=False, reg_lambda=1.0, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['wikisql', 'ag', 'amazon', 'sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[13620, 14755], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[13620, 14755], unbound=0, use_sep=False, weight_decay=0.01)
2023-06-29 23:32:46,754 - 0:00:06 - 0.0s - INFO - __main__ - start to train { task: ['wikisql'], seq train type: lll }
2023-06-29 23:32:46,795 - 0:00:06 - 0.0s - INFO - __main__ - extra training data size: 0
2023-06-29 23:32:50,110 - 0:00:09 - 3.3s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
2023-06-29 23:36:04,549 - 0:03:23 - 194.4s - INFO - __main__ - len of train dataset: 56355 , max train batch size 37 , num of opt steps: 1127100
/home/student/2021/cs21mtech11006/anaconda3/envs/lamol/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/home/student/2021/cs21mtech11006/anaconda3/envs/lamol/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
2023-06-29 23:56:21,354 - 0:23:40 - 1216.8s - INFO - __main__ - progress 0.656 , lr 6.0E-05 , loss 1.607 , qa loss 1.607 , lm loss 0.000 , avg batch size 37.0
2023-06-30 00:06:54,460 - 0:34:13 - 633.1s - INFO - __main__ - epoch 1/20 done , tot steps 1524 , lr 5.9E-05 , loss 1.19 , qa loss 1.19 , lm loss 0.00 , avg batch size 37.0
2023-06-30 00:27:04,180 - 0:54:23 - 1209.7s - INFO - __main__ - progress 1.656 , lr 5.7E-05 , loss 0.316 , qa loss 0.316 , lm loss 0.000 , avg batch size 37.0
2023-06-30 00:37:36,933 - 1:04:56 - 632.8s - INFO - __main__ - epoch 2/20 done , tot steps 3048 , lr 5.6E-05 , loss 0.30 , qa loss 0.30 , lm loss 0.00 , avg batch size 37.0
2023-06-30 00:58:58,950 - 1:26:18 - 1282.0s - INFO - __main__ - progress 2.656 , lr 5.4E-05 , loss 0.245 , qa loss 0.245 , lm loss 0.000 , avg batch size 37.0
2023-06-30 01:10:58,039 - 1:38:17 - 719.1s - INFO - __main__ - epoch 3/20 done , tot steps 4572 , lr 5.3E-05 , loss 0.24 , qa loss 0.24 , lm loss 0.00 , avg batch size 37.0
2023-06-30 01:33:50,005 - 2:01:09 - 1372.0s - INFO - __main__ - progress 3.656 , lr 5.1E-05 , loss 0.214 , qa loss 0.214 , lm loss 0.000 , avg batch size 37.0
2023-06-30 01:45:47,103 - 2:13:06 - 717.1s - INFO - __main__ - epoch 4/20 done , tot steps 6096 , lr 5.0E-05 , loss 0.21 , qa loss 0.21 , lm loss 0.00 , avg batch size 37.0
2023-06-30 02:08:37,145 - 2:35:56 - 1370.0s - INFO - __main__ - progress 4.656 , lr 4.8E-05 , loss 0.192 , qa loss 0.192 , lm loss 0.000 , avg batch size 37.0
2023-06-30 02:20:37,217 - 2:47:56 - 720.1s - INFO - __main__ - epoch 5/20 done , tot steps 7620 , lr 4.7E-05 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 37.0
2023-06-30 02:43:26,985 - 3:10:46 - 1369.8s - INFO - __main__ - progress 5.656 , lr 4.5E-05 , loss 0.177 , qa loss 0.177 , lm loss 0.000 , avg batch size 37.0
2023-06-30 02:55:24,503 - 3:22:43 - 717.5s - INFO - __main__ - epoch 6/20 done , tot steps 9144 , lr 4.4E-05 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 37.0
2023-06-30 03:18:14,433 - 3:45:33 - 1369.9s - INFO - __main__ - progress 6.656 , lr 4.2E-05 , loss 0.169 , qa loss 0.169 , lm loss 0.000 , avg batch size 37.0
2023-06-30 03:30:11,281 - 3:57:30 - 716.8s - INFO - __main__ - epoch 7/20 done , tot steps 10668 , lr 4.1E-05 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 37.0
2023-06-30 03:50:32,152 - 4:17:51 - 1220.9s - INFO - __main__ - progress 7.657 , lr 3.9E-05 , loss 0.160 , qa loss 0.160 , lm loss 0.000 , avg batch size 37.0
2023-06-30 04:00:26,649 - 4:27:46 - 594.5s - INFO - __main__ - epoch 8/20 done , tot steps 12192 , lr 3.8E-05 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 37.0
2023-06-30 04:18:42,739 - 4:46:02 - 1096.1s - INFO - __main__ - progress 8.657 , lr 3.5E-05 , loss 0.158 , qa loss 0.158 , lm loss 0.000 , avg batch size 37.0
2023-06-30 04:25:09,118 - 4:52:28 - 386.4s - INFO - __main__ - epoch 9/20 done , tot steps 13716 , lr 3.4E-05 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 37.0
2023-06-30 04:37:28,720 - 5:04:48 - 739.6s - INFO - __main__ - progress 9.657 , lr 3.2E-05 , loss 0.148 , qa loss 0.148 , lm loss 0.000 , avg batch size 37.0
2023-06-30 04:43:54,861 - 5:11:14 - 386.1s - INFO - __main__ - epoch 10/20 done , tot steps 15240 , lr 3.1E-05 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 37.0
2023-06-30 04:56:13,275 - 5:23:32 - 738.4s - INFO - __main__ - progress 10.656 , lr 2.9E-05 , loss 0.142 , qa loss 0.142 , lm loss 0.000 , avg batch size 37.0
2023-06-30 05:02:39,568 - 5:29:58 - 386.3s - INFO - __main__ - epoch 11/20 done , tot steps 16764 , lr 2.8E-05 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 37.0
2023-06-30 05:14:59,424 - 5:42:18 - 739.9s - INFO - __main__ - progress 11.656 , lr 2.6E-05 , loss 0.140 , qa loss 0.140 , lm loss 0.000 , avg batch size 37.0
2023-06-30 05:21:24,584 - 5:48:43 - 385.2s - INFO - __main__ - epoch 12/20 done , tot steps 18288 , lr 2.5E-05 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 37.0
2023-06-30 05:33:42,812 - 6:01:02 - 738.2s - INFO - __main__ - progress 12.656 , lr 2.3E-05 , loss 0.134 , qa loss 0.134 , lm loss 0.000 , avg batch size 37.0
2023-06-30 05:40:08,354 - 6:07:27 - 385.5s - INFO - __main__ - epoch 13/20 done , tot steps 19812 , lr 2.2E-05 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 37.0
2023-06-30 05:52:25,762 - 6:19:45 - 737.4s - INFO - __main__ - progress 13.657 , lr 2.0E-05 , loss 0.132 , qa loss 0.132 , lm loss 0.000 , avg batch size 37.0
2023-06-30 05:58:51,042 - 6:26:10 - 385.3s - INFO - __main__ - epoch 14/20 done , tot steps 21336 , lr 1.9E-05 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 37.0
2023-06-30 06:11:10,967 - 6:38:30 - 739.9s - INFO - __main__ - progress 14.657 , lr 1.7E-05 , loss 0.128 , qa loss 0.128 , lm loss 0.000 , avg batch size 37.0
2023-06-30 06:17:37,209 - 6:44:56 - 386.2s - INFO - __main__ - epoch 15/20 done , tot steps 22860 , lr 1.6E-05 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 37.0
2023-06-30 06:29:58,022 - 6:57:17 - 740.8s - INFO - __main__ - progress 15.656 , lr 1.4E-05 , loss 0.126 , qa loss 0.126 , lm loss 0.000 , avg batch size 37.0
2023-06-30 06:36:22,455 - 7:03:41 - 384.4s - INFO - __main__ - epoch 16/20 done , tot steps 24384 , lr 1.3E-05 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 37.0
2023-06-30 06:48:42,032 - 7:16:01 - 739.6s - INFO - __main__ - progress 16.657 , lr 1.0E-05 , loss 0.124 , qa loss 0.124 , lm loss 0.000 , avg batch size 37.0
2023-06-30 06:55:07,802 - 7:22:27 - 385.8s - INFO - __main__ - epoch 17/20 done , tot steps 25908 , lr 9.4E-06 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 37.0
2023-06-30 07:07:27,777 - 7:34:47 - 740.0s - INFO - __main__ - progress 17.656 , lr 7.3E-06 , loss 0.123 , qa loss 0.123 , lm loss 0.000 , avg batch size 37.0
2023-06-30 07:13:53,281 - 7:41:12 - 385.5s - INFO - __main__ - epoch 18/20 done , tot steps 27432 , lr 6.3E-06 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 37.0
2023-06-30 07:26:13,256 - 7:53:32 - 740.0s - INFO - __main__ - progress 18.656 , lr 4.2E-06 , loss 0.121 , qa loss 0.121 , lm loss 0.000 , avg batch size 37.0
2023-06-30 07:32:39,011 - 7:59:58 - 385.8s - INFO - __main__ - epoch 19/20 done , tot steps 28956 , lr 3.1E-06 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 37.0
2023-06-30 07:44:59,674 - 8:12:19 - 740.7s - INFO - __main__ - progress 19.656 , lr 1.1E-06 , loss 0.117 , qa loss 0.117 , lm loss 0.000 , avg batch size 37.0
2023-06-30 07:51:32,087 - 8:18:51 - 392.4s - INFO - __main__ - epoch 20/20 done , tot steps 30480 , lr 1.6E-08 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 37.0
2023-06-30 07:51:41,460 - 8:19:00 - 9.4s - INFO - __main__ - start to train { task: ['ag'], seq train type: lll }
2023-06-30 07:51:45,337 - 8:19:04 - 3.9s - INFO - utils - writing extra data in ../../model_ccmm2/gpt2/lll/wikisql_ag_amazon_sst_srl_woz.en_0.0/wikisql/lm.csv ...
2023-06-30 07:51:45,376 - 8:19:04 - 0.0s - INFO - __main__ - extra training data size: 0
2023-06-30 07:51:45,609 - 8:19:04 - 0.2s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[wikisql]
The task with which model is saved wikisql
2023-06-30 07:51:56,797 - 8:19:16 - 11.2s - INFO - __main__ - len of train dataset: 115000 , max train batch size 76 , num of opt steps: 2300000
2023-06-30 08:04:58,492 - 8:32:17 - 781.7s - INFO - __main__ - progress 0.648 , lr 6.0E-05 , loss 1.095 , qa loss 1.095 , lm loss 0.000 , avg batch size 74.5
2023-06-30 08:11:59,229 - 8:39:18 - 420.7s - INFO - __main__ - epoch 1/20 done , tot steps 1541 , lr 5.9E-05 , loss 0.75 , qa loss 0.75 , lm loss 0.00 , avg batch size 74.6
2023-06-30 08:24:59,808 - 8:52:19 - 780.6s - INFO - __main__ - progress 1.648 , lr 5.7E-05 , loss 0.115 , qa loss 0.115 , lm loss 0.000 , avg batch size 74.5
2023-06-30 08:32:03,044 - 8:59:22 - 423.2s - INFO - __main__ - epoch 2/20 done , tot steps 3084 , lr 5.6E-05 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 74.5
2023-06-30 08:45:03,851 - 9:12:23 - 780.8s - INFO - __main__ - progress 2.649 , lr 5.4E-05 , loss 0.100 , qa loss 0.100 , lm loss 0.000 , avg batch size 74.7
2023-06-30 08:52:03,057 - 9:19:22 - 419.2s - INFO - __main__ - epoch 3/20 done , tot steps 4624 , lr 5.3E-05 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 74.7
2023-06-30 09:05:02,538 - 9:32:21 - 779.5s - INFO - __main__ - progress 3.649 , lr 5.1E-05 , loss 0.088 , qa loss 0.088 , lm loss 0.000 , avg batch size 74.6
2023-06-30 09:12:04,549 - 9:39:23 - 422.0s - INFO - __main__ - epoch 4/20 done , tot steps 6166 , lr 5.0E-05 , loss 0.09 , qa loss 0.09 , lm loss 0.00 , avg batch size 74.6
2023-06-30 09:25:05,750 - 9:52:25 - 781.2s - INFO - __main__ - progress 4.649 , lr 4.8E-05 , loss 0.084 , qa loss 0.084 , lm loss 0.000 , avg batch size 74.6
2023-06-30 09:32:07,023 - 9:59:26 - 421.3s - INFO - __main__ - epoch 5/20 done , tot steps 7708 , lr 4.7E-05 , loss 0.08 , qa loss 0.08 , lm loss 0.00 , avg batch size 74.6
2023-06-30 09:45:07,153 - 10:12:26 - 780.1s - INFO - __main__ - progress 5.649 , lr 4.5E-05 , loss 0.079 , qa loss 0.079 , lm loss 0.000 , avg batch size 74.6
2023-06-30 09:52:07,456 - 10:19:26 - 420.3s - INFO - __main__ - epoch 6/20 done , tot steps 9248 , lr 4.4E-05 , loss 0.08 , qa loss 0.08 , lm loss 0.00 , avg batch size 74.7
2023-06-30 10:05:06,401 - 10:32:25 - 778.9s - INFO - __main__ - progress 6.650 , lr 4.2E-05 , loss 0.077 , qa loss 0.077 , lm loss 0.000 , avg batch size 74.8
2023-06-30 10:12:05,370 - 10:39:24 - 419.0s - INFO - __main__ - epoch 7/20 done , tot steps 10788 , lr 4.1E-05 , loss 0.08 , qa loss 0.08 , lm loss 0.00 , avg batch size 74.7
2023-06-30 10:25:04,403 - 10:52:23 - 779.0s - INFO - __main__ - progress 7.649 , lr 3.9E-05 , loss 0.074 , qa loss 0.074 , lm loss 0.000 , avg batch size 74.7
2023-06-30 10:32:04,832 - 10:59:24 - 420.4s - INFO - __main__ - epoch 8/20 done , tot steps 12328 , lr 3.8E-05 , loss 0.07 , qa loss 0.07 , lm loss 0.00 , avg batch size 74.7
2023-06-30 10:45:04,062 - 11:12:23 - 779.2s - INFO - __main__ - progress 8.648 , lr 3.5E-05 , loss 0.071 , qa loss 0.071 , lm loss 0.000 , avg batch size 74.5
2023-06-30 10:52:03,858 - 11:19:23 - 419.8s - INFO - __main__ - epoch 9/20 done , tot steps 13870 , lr 3.4E-05 , loss 0.07 , qa loss 0.07 , lm loss 0.00 , avg batch size 74.6
2023-06-30 11:05:03,806 - 11:32:23 - 779.9s - INFO - __main__ - progress 9.649 , lr 3.2E-05 , loss 0.069 , qa loss 0.069 , lm loss 0.000 , avg batch size 74.6
2023-06-30 11:12:03,764 - 11:39:23 - 420.0s - INFO - __main__ - epoch 10/20 done , tot steps 15411 , lr 3.1E-05 , loss 0.07 , qa loss 0.07 , lm loss 0.00 , avg batch size 74.6
2023-06-30 11:25:03,736 - 11:52:23 - 780.0s - INFO - __main__ - progress 10.648 , lr 2.9E-05 , loss 0.068 , qa loss 0.068 , lm loss 0.000 , avg batch size 74.5
2023-06-30 11:32:06,522 - 11:59:25 - 422.8s - INFO - __main__ - epoch 11/20 done , tot steps 16958 , lr 2.8E-05 , loss 0.07 , qa loss 0.07 , lm loss 0.00 , avg batch size 74.3
2023-06-30 11:45:06,039 - 12:12:25 - 779.5s - INFO - __main__ - progress 11.650 , lr 2.6E-05 , loss 0.067 , qa loss 0.067 , lm loss 0.000 , avg batch size 74.7
2023-06-30 11:52:04,053 - 12:19:23 - 418.0s - INFO - __main__ - epoch 12/20 done , tot steps 18497 , lr 2.5E-05 , loss 0.07 , qa loss 0.07 , lm loss 0.00 , avg batch size 74.7
2023-06-30 12:05:06,131 - 12:32:25 - 782.1s - INFO - __main__ - progress 12.649 , lr 2.3E-05 , loss 0.066 , qa loss 0.066 , lm loss 0.000 , avg batch size 74.6
2023-06-30 12:12:08,220 - 12:39:27 - 422.1s - INFO - __main__ - epoch 13/20 done , tot steps 20040 , lr 2.2E-05 , loss 0.07 , qa loss 0.07 , lm loss 0.00 , avg batch size 74.5
2023-06-30 12:25:07,573 - 12:52:26 - 779.4s - INFO - __main__ - progress 13.650 , lr 2.0E-05 , loss 0.064 , qa loss 0.064 , lm loss 0.000 , avg batch size 74.7
2023-06-30 12:32:08,033 - 12:59:27 - 420.5s - INFO - __main__ - epoch 14/20 done , tot steps 21582 , lr 1.9E-05 , loss 0.06 , qa loss 0.06 , lm loss 0.00 , avg batch size 74.6
2023-06-30 12:45:06,809 - 13:12:26 - 778.8s - INFO - __main__ - progress 14.649 , lr 1.7E-05 , loss 0.064 , qa loss 0.064 , lm loss 0.000 , avg batch size 74.6
2023-06-30 12:52:08,682 - 13:19:28 - 421.9s - INFO - __main__ - epoch 15/20 done , tot steps 23125 , lr 1.6E-05 , loss 0.06 , qa loss 0.06 , lm loss 0.00 , avg batch size 74.5
2023-06-30 13:05:06,476 - 13:32:25 - 777.8s - INFO - __main__ - progress 15.649 , lr 1.4E-05 , loss 0.063 , qa loss 0.063 , lm loss 0.000 , avg batch size 74.6
2023-06-30 13:12:06,178 - 13:39:25 - 419.7s - INFO - __main__ - epoch 16/20 done , tot steps 24665 , lr 1.3E-05 , loss 0.06 , qa loss 0.06 , lm loss 0.00 , avg batch size 74.7
2023-06-30 13:25:04,292 - 13:52:23 - 778.1s - INFO - __main__ - progress 16.649 , lr 1.0E-05 , loss 0.063 , qa loss 0.063 , lm loss 0.000 , avg batch size 74.6
2023-06-30 13:32:03,314 - 13:59:22 - 419.0s - INFO - __main__ - epoch 17/20 done , tot steps 26206 , lr 9.4E-06 , loss 0.06 , qa loss 0.06 , lm loss 0.00 , avg batch size 74.6
2023-06-30 13:45:01,877 - 14:12:21 - 778.6s - INFO - __main__ - progress 17.648 , lr 7.4E-06 , loss 0.060 , qa loss 0.060 , lm loss 0.000 , avg batch size 74.5
2023-06-30 13:52:02,381 - 14:19:21 - 420.5s - INFO - __main__ - epoch 18/20 done , tot steps 27749 , lr 6.3E-06 , loss 0.06 , qa loss 0.06 , lm loss 0.00 , avg batch size 74.5
2023-06-30 14:05:03,558 - 14:32:22 - 781.2s - INFO - __main__ - progress 18.648 , lr 4.2E-06 , loss 0.061 , qa loss 0.061 , lm loss 0.000 , avg batch size 74.5
2023-06-30 14:12:05,475 - 14:39:24 - 421.9s - INFO - __main__ - epoch 19/20 done , tot steps 29293 , lr 3.1E-06 , loss 0.06 , qa loss 0.06 , lm loss 0.00 , avg batch size 74.5
2023-06-30 14:25:05,080 - 14:52:24 - 779.6s - INFO - __main__ - progress 19.649 , lr 1.1E-06 , loss 0.059 , qa loss 0.059 , lm loss 0.000 , avg batch size 74.6
2023-06-30 14:32:12,781 - 14:59:32 - 427.7s - INFO - __main__ - epoch 20/20 done , tot steps 30835 , lr 1.6E-08 , loss 0.06 , qa loss 0.06 , lm loss 0.00 , avg batch size 74.6
2023-06-30 14:32:22,362 - 14:59:41 - 9.6s - INFO - __main__ - start to train { task: ['amazon'], seq train type: lll }
2023-06-30 14:32:25,235 - 14:59:44 - 2.9s - INFO - utils - writing extra data in ../../model_ccmm2/gpt2/lll/wikisql_ag_amazon_sst_srl_woz.en_0.0/ag/lm.csv ...
2023-06-30 14:32:25,261 - 14:59:44 - 0.0s - INFO - __main__ - extra training data size: 0
2023-06-30 14:32:25,668 - 14:59:45 - 0.4s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[ag]
The task with which model is saved ag
2023-06-30 14:32:37,815 - 14:59:57 - 12.1s - INFO - __main__ - len of train dataset: 115000 , max train batch size 76 , num of opt steps: 2300000
2023-06-30 14:46:03,543 - 15:13:22 - 805.7s - INFO - __main__ - progress 0.428 , lr 6.1E-05 , loss 1.463 , qa loss 1.463 , lm loss 0.000 , avg batch size 49.2
2023-06-30 14:59:19,993 - 15:26:39 - 796.5s - INFO - __main__ - progress 0.857 , lr 6.0E-05 , loss 0.972 , qa loss 0.972 , lm loss 0.000 , avg batch size 49.3
2023-06-30 15:03:49,648 - 15:31:09 - 269.7s - INFO - __main__ - epoch 1/20 done , tot steps 2336 , lr 5.9E-05 , loss 0.90 , qa loss 0.90 , lm loss 0.00 , avg batch size 49.2
2023-06-30 15:17:14,420 - 15:44:33 - 804.8s - INFO - __main__ - progress 1.428 , lr 5.8E-05 , loss 0.450 , qa loss 0.450 , lm loss 0.000 , avg batch size 49.3
2023-06-30 15:30:30,496 - 15:57:49 - 796.1s - INFO - __main__ - progress 1.857 , lr 5.7E-05 , loss 0.444 , qa loss 0.444 , lm loss 0.000 , avg batch size 49.3
2023-06-30 15:34:57,567 - 16:02:16 - 267.1s - INFO - __main__ - epoch 2/20 done , tot steps 4668 , lr 5.6E-05 , loss 0.44 , qa loss 0.44 , lm loss 0.00 , avg batch size 49.3
2023-06-30 15:48:23,667 - 16:15:43 - 806.1s - INFO - __main__ - progress 2.428 , lr 5.5E-05 , loss 0.426 , qa loss 0.426 , lm loss 0.000 , avg batch size 49.3
2023-06-30 16:01:39,204 - 16:28:58 - 795.5s - INFO - __main__ - progress 2.857 , lr 5.4E-05 , loss 0.421 , qa loss 0.421 , lm loss 0.000 , avg batch size 49.3
2023-06-30 16:06:08,845 - 16:33:28 - 269.6s - INFO - __main__ - epoch 3/20 done , tot steps 7003 , lr 5.3E-05 , loss 0.42 , qa loss 0.42 , lm loss 0.00 , avg batch size 49.3
2023-06-30 16:19:35,923 - 16:46:55 - 807.1s - INFO - __main__ - progress 3.427 , lr 5.2E-05 , loss 0.411 , qa loss 0.411 , lm loss 0.000 , avg batch size 49.1
2023-06-30 16:32:52,513 - 17:00:11 - 796.6s - INFO - __main__ - progress 3.856 , lr 5.0E-05 , loss 0.409 , qa loss 0.409 , lm loss 0.000 , avg batch size 49.2
2023-06-30 16:37:21,274 - 17:04:40 - 268.8s - INFO - __main__ - epoch 4/20 done , tot steps 9338 , lr 5.0E-05 , loss 0.41 , qa loss 0.41 , lm loss 0.00 , avg batch size 49.3
2023-06-30 16:50:45,332 - 17:18:04 - 804.1s - INFO - __main__ - progress 4.430 , lr 4.9E-05 , loss 0.402 , qa loss 0.402 , lm loss 0.000 , avg batch size 49.5
2023-06-30 17:04:01,814 - 17:31:21 - 796.5s - INFO - __main__ - progress 4.859 , lr 4.7E-05 , loss 0.400 , qa loss 0.400 , lm loss 0.000 , avg batch size 49.4
2023-06-30 17:08:28,433 - 17:35:47 - 266.6s - INFO - __main__ - epoch 5/20 done , tot steps 11670 , lr 4.7E-05 , loss 0.40 , qa loss 0.40 , lm loss 0.00 , avg batch size 49.3
2023-06-30 17:21:55,214 - 17:49:14 - 806.8s - INFO - __main__ - progress 5.429 , lr 4.6E-05 , loss 0.393 , qa loss 0.393 , lm loss 0.000 , avg batch size 49.4
2023-06-30 17:35:10,808 - 18:02:30 - 795.6s - INFO - __main__ - progress 5.859 , lr 4.4E-05 , loss 0.393 , qa loss 0.393 , lm loss 0.000 , avg batch size 49.4
2023-06-30 17:39:37,319 - 18:06:56 - 266.5s - INFO - __main__ - epoch 6/20 done , tot steps 14002 , lr 4.4E-05 , loss 0.39 , qa loss 0.39 , lm loss 0.00 , avg batch size 49.3
2023-06-30 17:53:02,848 - 18:20:22 - 805.5s - INFO - __main__ - progress 6.430 , lr 4.2E-05 , loss 0.388 , qa loss 0.388 , lm loss 0.000 , avg batch size 49.4
2023-06-30 18:06:17,589 - 18:33:36 - 794.7s - INFO - __main__ - progress 6.858 , lr 4.1E-05 , loss 0.388 , qa loss 0.388 , lm loss 0.000 , avg batch size 49.3
2023-06-30 18:10:46,095 - 18:38:05 - 268.5s - INFO - __main__ - epoch 7/20 done , tot steps 16336 , lr 4.1E-05 , loss 0.39 , qa loss 0.39 , lm loss 0.00 , avg batch size 49.3
2023-06-30 18:24:09,578 - 18:51:28 - 803.5s - INFO - __main__ - progress 7.428 , lr 3.9E-05 , loss 0.384 , qa loss 0.384 , lm loss 0.000 , avg batch size 49.2
2023-06-30 18:37:24,465 - 19:04:43 - 794.9s - INFO - __main__ - progress 7.857 , lr 3.8E-05 , loss 0.383 , qa loss 0.383 , lm loss 0.000 , avg batch size 49.3
2023-06-30 18:41:53,649 - 19:09:13 - 269.2s - INFO - __main__ - epoch 8/20 done , tot steps 18672 , lr 3.8E-05 , loss 0.38 , qa loss 0.38 , lm loss 0.00 , avg batch size 49.2
2023-06-30 18:55:16,832 - 19:22:36 - 803.2s - INFO - __main__ - progress 8.428 , lr 3.6E-05 , loss 0.379 , qa loss 0.379 , lm loss 0.000 , avg batch size 49.2
2023-06-30 19:08:31,410 - 19:35:50 - 794.6s - INFO - __main__ - progress 8.856 , lr 3.5E-05 , loss 0.380 , qa loss 0.380 , lm loss 0.000 , avg batch size 49.2
2023-06-30 19:13:00,616 - 19:40:19 - 269.2s - INFO - __main__ - epoch 9/20 done , tot steps 21008 , lr 3.4E-05 , loss 0.38 , qa loss 0.38 , lm loss 0.00 , avg batch size 49.2
2023-06-30 19:26:24,786 - 19:53:44 - 804.2s - INFO - __main__ - progress 9.430 , lr 3.3E-05 , loss 0.377 , qa loss 0.377 , lm loss 0.000 , avg batch size 49.4
2023-06-30 19:39:38,973 - 20:06:58 - 794.2s - INFO - __main__ - progress 9.856 , lr 3.2E-05 , loss 0.377 , qa loss 0.377 , lm loss 0.000 , avg batch size 49.2
2023-06-30 19:44:09,637 - 20:11:28 - 270.7s - INFO - __main__ - epoch 10/20 done , tot steps 23345 , lr 3.1E-05 , loss 0.38 , qa loss 0.38 , lm loss 0.00 , avg batch size 49.2
2023-06-30 19:57:33,700 - 20:24:53 - 804.1s - INFO - __main__ - progress 10.428 , lr 3.0E-05 , loss 0.374 , qa loss 0.374 , lm loss 0.000 , avg batch size 49.2
2023-06-30 20:10:48,745 - 20:38:08 - 795.0s - INFO - __main__ - progress 10.857 , lr 2.9E-05 , loss 0.374 , qa loss 0.374 , lm loss 0.000 , avg batch size 49.3
2023-06-30 20:15:18,158 - 20:42:37 - 269.4s - INFO - __main__ - epoch 11/20 done , tot steps 25681 , lr 2.8E-05 , loss 0.37 , qa loss 0.37 , lm loss 0.00 , avg batch size 49.2
2023-06-30 20:28:42,304 - 20:56:01 - 804.1s - INFO - __main__ - progress 11.429 , lr 2.7E-05 , loss 0.372 , qa loss 0.372 , lm loss 0.000 , avg batch size 49.3
2023-06-30 20:41:56,830 - 21:09:16 - 794.5s - INFO - __main__ - progress 11.858 , lr 2.5E-05 , loss 0.371 , qa loss 0.371 , lm loss 0.000 , avg batch size 49.3
2023-06-30 20:46:23,239 - 21:13:42 - 266.4s - INFO - __main__ - epoch 12/20 done , tot steps 28013 , lr 2.5E-05 , loss 0.37 , qa loss 0.37 , lm loss 0.00 , avg batch size 49.3
2023-06-30 20:59:47,742 - 21:27:07 - 804.5s - INFO - __main__ - progress 12.428 , lr 2.4E-05 , loss 0.369 , qa loss 0.369 , lm loss 0.000 , avg batch size 49.2
2023-06-30 21:13:03,169 - 21:40:22 - 795.4s - INFO - __main__ - progress 12.856 , lr 2.2E-05 , loss 0.369 , qa loss 0.369 , lm loss 0.000 , avg batch size 49.2
2023-06-30 21:17:33,078 - 21:44:52 - 269.9s - INFO - __main__ - epoch 13/20 done , tot steps 30348 , lr 2.2E-05 , loss 0.37 , qa loss 0.37 , lm loss 0.00 , avg batch size 49.3
2023-06-30 21:30:57,999 - 21:58:17 - 804.9s - INFO - __main__ - progress 13.428 , lr 2.1E-05 , loss 0.365 , qa loss 0.365 , lm loss 0.000 , avg batch size 49.2
2023-06-30 21:44:14,426 - 22:11:33 - 796.4s - INFO - __main__ - progress 13.858 , lr 1.9E-05 , loss 0.366 , qa loss 0.366 , lm loss 0.000 , avg batch size 49.3
2023-06-30 21:48:41,390 - 22:16:00 - 267.0s - INFO - __main__ - epoch 14/20 done , tot steps 32680 , lr 1.9E-05 , loss 0.37 , qa loss 0.37 , lm loss 0.00 , avg batch size 49.3
2023-06-30 22:02:05,375 - 22:29:24 - 804.0s - INFO - __main__ - progress 14.428 , lr 1.7E-05 , loss 0.364 , qa loss 0.364 , lm loss 0.000 , avg batch size 49.2
2023-06-30 22:15:20,917 - 22:42:40 - 795.5s - INFO - __main__ - progress 14.856 , lr 1.6E-05 , loss 0.364 , qa loss 0.364 , lm loss 0.000 , avg batch size 49.2
2023-06-30 22:19:51,583 - 22:47:10 - 270.7s - INFO - __main__ - epoch 15/20 done , tot steps 35017 , lr 1.6E-05 , loss 0.36 , qa loss 0.36 , lm loss 0.00 , avg batch size 49.2
2023-06-30 22:33:16,129 - 23:00:35 - 804.5s - INFO - __main__ - progress 15.428 , lr 1.4E-05 , loss 0.362 , qa loss 0.362 , lm loss 0.000 , avg batch size 49.2
2023-06-30 22:46:31,857 - 23:13:51 - 795.7s - INFO - __main__ - progress 15.856 , lr 1.3E-05 , loss 0.364 , qa loss 0.364 , lm loss 0.000 , avg batch size 49.2
2023-06-30 22:51:01,173 - 23:18:20 - 269.3s - INFO - __main__ - epoch 16/20 done , tot steps 37351 , lr 1.3E-05 , loss 0.36 , qa loss 0.36 , lm loss 0.00 , avg batch size 49.3
2023-06-30 23:04:26,616 - 23:31:45 - 805.4s - INFO - __main__ - progress 16.427 , lr 1.1E-05 , loss 0.362 , qa loss 0.362 , lm loss 0.000 , avg batch size 49.2
2023-06-30 23:17:42,419 - 23:45:01 - 795.8s - INFO - __main__ - progress 16.857 , lr 9.8E-06 , loss 0.360 , qa loss 0.360 , lm loss 0.000 , avg batch size 49.3
2023-06-30 23:22:11,539 - 23:49:30 - 269.1s - INFO - __main__ - epoch 17/20 done , tot steps 39686 , lr 9.4E-06 , loss 0.36 , qa loss 0.36 , lm loss 0.00 , avg batch size 49.3
2023-06-30 23:35:37,946 - 1 day, 0:02:57 - 806.4s - INFO - __main__ - progress 17.428 , lr 8.1E-06 , loss 0.364 , qa loss 0.364 , lm loss 0.000 , avg batch size 49.3
2023-06-30 23:48:53,081 - 1 day, 0:16:12 - 795.1s - INFO - __main__ - progress 17.857 , lr 6.7E-06 , loss 0.361 , qa loss 0.361 , lm loss 0.000 , avg batch size 49.3
2023-06-30 23:53:20,311 - 1 day, 0:20:39 - 267.2s - INFO - __main__ - epoch 18/20 done , tot steps 42019 , lr 6.3E-06 , loss 0.36 , qa loss 0.36 , lm loss 0.00 , avg batch size 49.3
2023-07-01 00:06:44,885 - 1 day, 0:34:04 - 804.6s - INFO - __main__ - progress 18.429 , lr 4.9E-06 , loss 0.359 , qa loss 0.359 , lm loss 0.000 , avg batch size 49.3
2023-07-01 00:20:01,392 - 1 day, 0:47:20 - 796.5s - INFO - __main__ - progress 18.857 , lr 3.6E-06 , loss 0.359 , qa loss 0.359 , lm loss 0.000 , avg batch size 49.3
2023-07-01 00:24:28,746 - 1 day, 0:51:48 - 267.4s - INFO - __main__ - epoch 19/20 done , tot steps 44352 , lr 3.1E-06 , loss 0.36 , qa loss 0.36 , lm loss 0.00 , avg batch size 49.3
2023-07-01 00:37:53,459 - 1 day, 1:05:12 - 804.7s - INFO - __main__ - progress 19.429 , lr 1.8E-06 , loss 0.361 , qa loss 0.361 , lm loss 0.000 , avg batch size 49.4
2023-07-01 00:51:10,073 - 1 day, 1:18:29 - 796.6s - INFO - __main__ - progress 19.858 , lr 4.6E-07 , loss 0.360 , qa loss 0.360 , lm loss 0.000 , avg batch size 49.3
2023-07-01 00:55:42,811 - 1 day, 1:23:02 - 272.7s - INFO - __main__ - epoch 20/20 done , tot steps 46683 , lr 1.6E-08 , loss 0.36 , qa loss 0.36 , lm loss 0.00 , avg batch size 49.3
2023-07-01 00:55:52,687 - 1 day, 1:23:12 - 9.9s - INFO - __main__ - start to train { task: ['sst'], seq train type: lll }
2023-07-01 00:55:57,187 - 1 day, 1:23:16 - 4.5s - INFO - utils - writing extra data in ../../model_ccmm2/gpt2/lll/wikisql_ag_amazon_sst_srl_woz.en_0.0/amazon/lm.csv ...
2023-07-01 00:55:57,276 - 1 day, 1:23:16 - 0.1s - INFO - __main__ - extra training data size: 0
2023-07-01 00:55:57,527 - 1 day, 1:23:16 - 0.3s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[amazon]
The task with which model is saved amazon
2023-07-01 00:56:03,769 - 1 day, 1:23:23 - 6.2s - INFO - __main__ - len of train dataset: 6920 , max train batch size 4 , num of opt steps: 138400
2023-07-01 00:57:44,620 - 1 day, 1:25:03 - 100.9s - INFO - __main__ - progress 0.578 , lr 6.1E-05 , loss 2.530 , qa loss 2.530 , lm loss 0.000 , avg batch size 4.0
2023-07-01 00:58:54,723 - 1 day, 1:26:14 - 70.1s - INFO - __main__ - epoch 1/20 done , tot steps 1730 , lr 5.9E-05 , loss 1.59 , qa loss 1.59 , lm loss 0.00 , avg batch size 4.0
2023-07-01 01:00:36,210 - 1 day, 1:27:55 - 101.5s - INFO - __main__ - progress 1.578 , lr 5.8E-05 , loss 0.272 , qa loss 0.272 , lm loss 0.000 , avg batch size 4.0
2023-07-01 01:01:46,594 - 1 day, 1:29:05 - 70.4s - INFO - __main__ - epoch 2/20 done , tot steps 3460 , lr 5.6E-05 , loss 0.26 , qa loss 0.26 , lm loss 0.00 , avg batch size 4.0
2023-07-01 01:03:25,757 - 1 day, 1:30:45 - 99.2s - INFO - __main__ - progress 2.578 , lr 5.4E-05 , loss 0.218 , qa loss 0.218 , lm loss 0.000 , avg batch size 4.0
2023-07-01 01:04:37,475 - 1 day, 1:31:56 - 71.7s - INFO - __main__ - epoch 3/20 done , tot steps 5190 , lr 5.3E-05 , loss 0.21 , qa loss 0.21 , lm loss 0.00 , avg batch size 4.0
2023-07-01 01:06:19,045 - 1 day, 1:33:38 - 101.6s - INFO - __main__ - progress 3.578 , lr 5.1E-05 , loss 0.195 , qa loss 0.195 , lm loss 0.000 , avg batch size 4.0
2023-07-01 01:07:30,102 - 1 day, 1:34:49 - 71.1s - INFO - __main__ - epoch 4/20 done , tot steps 6920 , lr 5.0E-05 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-07-01 01:09:11,585 - 1 day, 1:36:30 - 101.5s - INFO - __main__ - progress 4.578 , lr 4.8E-05 , loss 0.172 , qa loss 0.172 , lm loss 0.000 , avg batch size 4.0
2023-07-01 01:10:22,426 - 1 day, 1:37:41 - 70.8s - INFO - __main__ - epoch 5/20 done , tot steps 8650 , lr 4.7E-05 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
2023-07-01 01:12:03,595 - 1 day, 1:39:22 - 101.2s - INFO - __main__ - progress 5.578 , lr 4.5E-05 , loss 0.152 , qa loss 0.152 , lm loss 0.000 , avg batch size 4.0
2023-07-01 01:13:13,850 - 1 day, 1:40:33 - 70.3s - INFO - __main__ - epoch 6/20 done , tot steps 10380 , lr 4.4E-05 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-07-01 01:14:55,543 - 1 day, 1:42:14 - 101.7s - INFO - __main__ - progress 6.578 , lr 4.2E-05 , loss 0.149 , qa loss 0.149 , lm loss 0.000 , avg batch size 4.0
2023-07-01 01:16:05,580 - 1 day, 1:43:24 - 70.0s - INFO - __main__ - epoch 7/20 done , tot steps 12110 , lr 4.1E-05 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-07-01 01:17:46,602 - 1 day, 1:45:05 - 101.0s - INFO - __main__ - progress 7.578 , lr 3.9E-05 , loss 0.133 , qa loss 0.133 , lm loss 0.000 , avg batch size 4.0
2023-07-01 01:18:56,620 - 1 day, 1:46:15 - 70.0s - INFO - __main__ - epoch 8/20 done , tot steps 13840 , lr 3.8E-05 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
2023-07-01 01:20:37,692 - 1 day, 1:47:57 - 101.1s - INFO - __main__ - progress 8.578 , lr 3.6E-05 , loss 0.127 , qa loss 0.127 , lm loss 0.000 , avg batch size 4.0
2023-07-01 01:21:48,323 - 1 day, 1:49:07 - 70.6s - INFO - __main__ - epoch 9/20 done , tot steps 15570 , lr 3.4E-05 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-07-01 01:23:27,551 - 1 day, 1:50:46 - 99.2s - INFO - __main__ - progress 9.578 , lr 3.3E-05 , loss 0.117 , qa loss 0.117 , lm loss 0.000 , avg batch size 4.0
2023-07-01 01:24:38,731 - 1 day, 1:51:58 - 71.2s - INFO - __main__ - epoch 10/20 done , tot steps 17300 , lr 3.1E-05 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2023-07-01 01:26:19,269 - 1 day, 1:53:38 - 100.5s - INFO - __main__ - progress 10.578 , lr 2.9E-05 , loss 0.119 , qa loss 0.119 , lm loss 0.000 , avg batch size 4.0
2023-07-01 01:27:29,985 - 1 day, 1:54:49 - 70.7s - INFO - __main__ - epoch 11/20 done , tot steps 19030 , lr 2.8E-05 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2023-07-01 01:29:10,152 - 1 day, 1:56:29 - 100.2s - INFO - __main__ - progress 11.578 , lr 2.6E-05 , loss 0.108 , qa loss 0.108 , lm loss 0.000 , avg batch size 4.0
2023-07-01 01:30:20,949 - 1 day, 1:57:40 - 70.8s - INFO - __main__ - epoch 12/20 done , tot steps 20760 , lr 2.5E-05 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-07-01 01:32:01,311 - 1 day, 1:59:20 - 100.4s - INFO - __main__ - progress 12.578 , lr 2.3E-05 , loss 0.099 , qa loss 0.099 , lm loss 0.000 , avg batch size 4.0
2023-07-01 01:33:12,153 - 1 day, 2:00:31 - 70.8s - INFO - __main__ - epoch 13/20 done , tot steps 22490 , lr 2.2E-05 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 4.0
2023-07-01 01:34:52,981 - 1 day, 2:02:12 - 100.8s - INFO - __main__ - progress 13.578 , lr 2.0E-05 , loss 0.100 , qa loss 0.100 , lm loss 0.000 , avg batch size 4.0
2023-07-01 01:36:03,008 - 1 day, 2:03:22 - 70.0s - INFO - __main__ - epoch 14/20 done , tot steps 24220 , lr 1.9E-05 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 4.0
2023-07-01 01:37:43,551 - 1 day, 2:05:02 - 100.5s - INFO - __main__ - progress 14.578 , lr 1.7E-05 , loss 0.090 , qa loss 0.090 , lm loss 0.000 , avg batch size 4.0
2023-07-01 01:38:54,044 - 1 day, 2:06:13 - 70.5s - INFO - __main__ - epoch 15/20 done , tot steps 25950 , lr 1.6E-05 , loss 0.09 , qa loss 0.09 , lm loss 0.00 , avg batch size 4.0
2023-07-01 01:40:34,322 - 1 day, 2:07:53 - 100.3s - INFO - __main__ - progress 15.578 , lr 1.4E-05 , loss 0.078 , qa loss 0.078 , lm loss 0.000 , avg batch size 4.0
2023-07-01 01:41:44,858 - 1 day, 2:09:04 - 70.5s - INFO - __main__ - epoch 16/20 done , tot steps 27680 , lr 1.3E-05 , loss 0.08 , qa loss 0.08 , lm loss 0.00 , avg batch size 4.0
2023-07-01 01:43:24,784 - 1 day, 2:10:44 - 99.9s - INFO - __main__ - progress 16.578 , lr 1.1E-05 , loss 0.105 , qa loss 0.105 , lm loss 0.000 , avg batch size 4.0
2023-07-01 01:44:35,928 - 1 day, 2:11:55 - 71.1s - INFO - __main__ - epoch 17/20 done , tot steps 29410 , lr 9.4E-06 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 4.0
2023-07-01 01:46:17,568 - 1 day, 2:13:36 - 101.6s - INFO - __main__ - progress 17.578 , lr 7.6E-06 , loss 0.081 , qa loss 0.081 , lm loss 0.000 , avg batch size 4.0
2023-07-01 01:47:27,818 - 1 day, 2:14:47 - 70.3s - INFO - __main__ - epoch 18/20 done , tot steps 31140 , lr 6.3E-06 , loss 0.08 , qa loss 0.08 , lm loss 0.00 , avg batch size 4.0
2023-07-01 01:49:08,148 - 1 day, 2:16:27 - 100.3s - INFO - __main__ - progress 18.578 , lr 4.5E-06 , loss 0.076 , qa loss 0.076 , lm loss 0.000 , avg batch size 4.0
2023-07-01 01:50:18,356 - 1 day, 2:17:37 - 70.2s - INFO - __main__ - epoch 19/20 done , tot steps 32870 , lr 3.1E-06 , loss 0.08 , qa loss 0.08 , lm loss 0.00 , avg batch size 4.0
2023-07-01 01:51:59,142 - 1 day, 2:19:18 - 100.8s - INFO - __main__ - progress 19.578 , lr 1.3E-06 , loss 0.072 , qa loss 0.072 , lm loss 0.000 , avg batch size 4.0
2023-07-01 01:53:15,913 - 1 day, 2:20:35 - 76.8s - INFO - __main__ - epoch 20/20 done , tot steps 34600 , lr 1.5E-08 , loss 0.08 , qa loss 0.08 , lm loss 0.00 , avg batch size 4.0
2023-07-01 01:53:25,075 - 1 day, 2:20:44 - 9.2s - INFO - __main__ - start to train { task: ['srl'], seq train type: lll }
2023-07-01 01:53:27,948 - 1 day, 2:20:47 - 2.9s - INFO - utils - writing extra data in ../../model_ccmm2/gpt2/lll/wikisql_ag_amazon_sst_srl_woz.en_0.0/sst/lm.csv ...
2023-07-01 01:53:27,995 - 1 day, 2:20:47 - 0.0s - INFO - __main__ - extra training data size: 0
2023-07-01 01:53:28,303 - 1 day, 2:20:47 - 0.3s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[sst]
The task with which model is saved sst
2023-07-01 01:53:34,630 - 1 day, 2:20:53 - 6.3s - INFO - __main__ - len of train dataset: 6414 , max train batch size 4 , num of opt steps: 128280
2023-07-01 01:56:12,193 - 1 day, 2:23:31 - 157.6s - INFO - __main__ - progress 0.624 , lr 6.1E-05 , loss 3.000 , qa loss 3.000 , lm loss 0.000 , avg batch size 4.0
2023-07-01 01:57:51,561 - 1 day, 2:25:10 - 99.4s - INFO - __main__ - epoch 1/20 done , tot steps 1604 , lr 5.9E-05 , loss 2.27 , qa loss 2.27 , lm loss 0.00 , avg batch size 4.0
2023-07-01 02:00:34,085 - 1 day, 2:27:53 - 162.5s - INFO - __main__ - progress 1.624 , lr 5.7E-05 , loss 0.904 , qa loss 0.904 , lm loss 0.000 , avg batch size 4.0
2023-07-01 02:02:11,398 - 1 day, 2:29:30 - 97.3s - INFO - __main__ - epoch 2/20 done , tot steps 3208 , lr 5.6E-05 , loss 0.87 , qa loss 0.87 , lm loss 0.00 , avg batch size 4.0
2023-07-01 02:04:53,818 - 1 day, 2:32:13 - 162.4s - INFO - __main__ - progress 2.624 , lr 5.4E-05 , loss 0.739 , qa loss 0.739 , lm loss 0.000 , avg batch size 4.0
2023-07-01 02:06:29,362 - 1 day, 2:33:48 - 95.5s - INFO - __main__ - epoch 3/20 done , tot steps 4812 , lr 5.3E-05 , loss 0.73 , qa loss 0.73 , lm loss 0.00 , avg batch size 4.0
2023-07-01 02:09:09,668 - 1 day, 2:36:29 - 160.3s - INFO - __main__ - progress 3.624 , lr 5.1E-05 , loss 0.666 , qa loss 0.666 , lm loss 0.000 , avg batch size 4.0
2023-07-01 02:10:44,853 - 1 day, 2:38:04 - 95.2s - INFO - __main__ - epoch 4/20 done , tot steps 6416 , lr 5.0E-05 , loss 0.66 , qa loss 0.66 , lm loss 0.00 , avg batch size 4.0
2023-07-01 02:13:24,109 - 1 day, 2:40:43 - 159.3s - INFO - __main__ - progress 4.624 , lr 4.8E-05 , loss 0.601 , qa loss 0.601 , lm loss 0.000 , avg batch size 4.0
2023-07-01 02:15:02,817 - 1 day, 2:42:22 - 98.7s - INFO - __main__ - epoch 5/20 done , tot steps 8020 , lr 4.7E-05 , loss 0.59 , qa loss 0.59 , lm loss 0.00 , avg batch size 4.0
2023-07-01 02:17:45,476 - 1 day, 2:45:04 - 162.7s - INFO - __main__ - progress 5.624 , lr 4.5E-05 , loss 0.550 , qa loss 0.550 , lm loss 0.000 , avg batch size 4.0
2023-07-01 02:19:19,706 - 1 day, 2:46:39 - 94.2s - INFO - __main__ - epoch 6/20 done , tot steps 9624 , lr 4.4E-05 , loss 0.56 , qa loss 0.56 , lm loss 0.00 , avg batch size 4.0
2023-07-01 02:22:02,574 - 1 day, 2:49:21 - 162.9s - INFO - __main__ - progress 6.624 , lr 4.2E-05 , loss 0.516 , qa loss 0.516 , lm loss 0.000 , avg batch size 4.0
2023-07-01 02:23:39,983 - 1 day, 2:50:59 - 97.4s - INFO - __main__ - epoch 7/20 done , tot steps 11228 , lr 4.1E-05 , loss 0.52 , qa loss 0.52 , lm loss 0.00 , avg batch size 4.0
2023-07-01 02:26:22,376 - 1 day, 2:53:41 - 162.4s - INFO - __main__ - progress 7.624 , lr 3.9E-05 , loss 0.488 , qa loss 0.488 , lm loss 0.000 , avg batch size 4.0
2023-07-01 02:27:58,803 - 1 day, 2:55:18 - 96.4s - INFO - __main__ - epoch 8/20 done , tot steps 12832 , lr 3.8E-05 , loss 0.50 , qa loss 0.50 , lm loss 0.00 , avg batch size 4.0
2023-07-01 02:30:45,021 - 1 day, 2:58:04 - 166.2s - INFO - __main__ - progress 8.624 , lr 3.6E-05 , loss 0.465 , qa loss 0.465 , lm loss 0.000 , avg batch size 4.0
2023-07-01 02:32:19,875 - 1 day, 2:59:39 - 94.9s - INFO - __main__ - epoch 9/20 done , tot steps 14436 , lr 3.4E-05 , loss 0.46 , qa loss 0.46 , lm loss 0.00 , avg batch size 4.0
2023-07-01 02:35:04,716 - 1 day, 3:02:24 - 164.8s - INFO - __main__ - progress 9.624 , lr 3.2E-05 , loss 0.446 , qa loss 0.446 , lm loss 0.000 , avg batch size 4.0
2023-07-01 02:36:39,844 - 1 day, 3:03:59 - 95.1s - INFO - __main__ - epoch 10/20 done , tot steps 16040 , lr 3.1E-05 , loss 0.44 , qa loss 0.44 , lm loss 0.00 , avg batch size 4.0
2023-07-01 02:39:20,658 - 1 day, 3:06:40 - 160.8s - INFO - __main__ - progress 10.624 , lr 2.9E-05 , loss 0.419 , qa loss 0.419 , lm loss 0.000 , avg batch size 4.0
2023-07-01 02:40:58,786 - 1 day, 3:08:18 - 98.1s - INFO - __main__ - epoch 11/20 done , tot steps 17644 , lr 2.8E-05 , loss 0.42 , qa loss 0.42 , lm loss 0.00 , avg batch size 4.0
2023-07-01 02:43:38,874 - 1 day, 3:10:58 - 160.1s - INFO - __main__ - progress 11.624 , lr 2.6E-05 , loss 0.401 , qa loss 0.401 , lm loss 0.000 , avg batch size 4.0
2023-07-01 02:45:17,567 - 1 day, 3:12:36 - 98.7s - INFO - __main__ - epoch 12/20 done , tot steps 19248 , lr 2.5E-05 , loss 0.40 , qa loss 0.40 , lm loss 0.00 , avg batch size 4.0
2023-07-01 02:48:00,010 - 1 day, 3:15:19 - 162.4s - INFO - __main__ - progress 12.624 , lr 2.3E-05 , loss 0.410 , qa loss 0.410 , lm loss 0.000 , avg batch size 4.0
2023-07-01 02:49:36,558 - 1 day, 3:16:55 - 96.5s - INFO - __main__ - epoch 13/20 done , tot steps 20852 , lr 2.2E-05 , loss 0.39 , qa loss 0.39 , lm loss 0.00 , avg batch size 4.0
2023-07-01 02:52:19,877 - 1 day, 3:19:39 - 163.3s - INFO - __main__ - progress 13.624 , lr 2.0E-05 , loss 0.373 , qa loss 0.373 , lm loss 0.000 , avg batch size 4.0
2023-07-01 02:53:53,909 - 1 day, 3:21:13 - 94.0s - INFO - __main__ - epoch 14/20 done , tot steps 22456 , lr 1.9E-05 , loss 0.38 , qa loss 0.38 , lm loss 0.00 , avg batch size 4.0
2023-07-01 02:56:38,769 - 1 day, 3:23:58 - 164.9s - INFO - __main__ - progress 14.624 , lr 1.7E-05 , loss 0.364 , qa loss 0.364 , lm loss 0.000 , avg batch size 4.0
2023-07-01 02:58:13,747 - 1 day, 3:25:33 - 95.0s - INFO - __main__ - epoch 15/20 done , tot steps 24060 , lr 1.6E-05 , loss 0.37 , qa loss 0.37 , lm loss 0.00 , avg batch size 4.0
2023-07-01 03:00:58,625 - 1 day, 3:28:17 - 164.9s - INFO - __main__ - progress 15.624 , lr 1.4E-05 , loss 0.358 , qa loss 0.358 , lm loss 0.000 , avg batch size 4.0
2023-07-01 03:02:33,572 - 1 day, 3:29:52 - 94.9s - INFO - __main__ - epoch 16/20 done , tot steps 25664 , lr 1.3E-05 , loss 0.36 , qa loss 0.36 , lm loss 0.00 , avg batch size 4.0
2023-07-01 03:05:14,546 - 1 day, 3:32:33 - 161.0s - INFO - __main__ - progress 16.624 , lr 1.1E-05 , loss 0.343 , qa loss 0.343 , lm loss 0.000 , avg batch size 4.0
2023-07-01 03:06:51,691 - 1 day, 3:34:11 - 97.1s - INFO - __main__ - epoch 17/20 done , tot steps 27268 , lr 9.4E-06 , loss 0.35 , qa loss 0.35 , lm loss 0.00 , avg batch size 4.0
2023-07-01 03:09:34,537 - 1 day, 3:36:53 - 162.8s - INFO - __main__ - progress 17.624 , lr 7.4E-06 , loss 0.330 , qa loss 0.330 , lm loss 0.000 , avg batch size 4.0
2023-07-01 03:11:12,047 - 1 day, 3:38:31 - 97.5s - INFO - __main__ - epoch 18/20 done , tot steps 28872 , lr 6.3E-06 , loss 0.33 , qa loss 0.33 , lm loss 0.00 , avg batch size 4.0
2023-07-01 03:13:54,973 - 1 day, 3:41:14 - 162.9s - INFO - __main__ - progress 18.624 , lr 4.3E-06 , loss 0.319 , qa loss 0.319 , lm loss 0.000 , avg batch size 4.0
2023-07-01 03:15:32,842 - 1 day, 3:42:52 - 97.9s - INFO - __main__ - epoch 19/20 done , tot steps 30476 , lr 3.1E-06 , loss 0.33 , qa loss 0.33 , lm loss 0.00 , avg batch size 4.0
2023-07-01 03:18:16,008 - 1 day, 3:45:35 - 163.2s - INFO - __main__ - progress 19.624 , lr 1.2E-06 , loss 0.323 , qa loss 0.323 , lm loss 0.000 , avg batch size 4.0
2023-07-01 03:19:58,627 - 1 day, 3:47:17 - 102.6s - INFO - __main__ - epoch 20/20 done , tot steps 32080 , lr 1.6E-08 , loss 0.32 , qa loss 0.32 , lm loss 0.00 , avg batch size 4.0
2023-07-01 03:20:07,307 - 1 day, 3:47:26 - 8.7s - INFO - __main__ - start to train { task: ['woz.en'], seq train type: lll }
2023-07-01 03:20:11,722 - 1 day, 3:47:31 - 4.4s - INFO - utils - writing extra data in ../../model_ccmm2/gpt2/lll/wikisql_ag_amazon_sst_srl_woz.en_0.0/srl/lm.csv ...
2023-07-01 03:20:11,751 - 1 day, 3:47:31 - 0.0s - INFO - __main__ - extra training data size: 0
2023-07-01 03:20:12,121 - 1 day, 3:47:31 - 0.4s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[srl]
The task with which model is saved srl
2023-07-01 03:20:17,632 - 1 day, 3:47:36 - 5.5s - INFO - __main__ - len of train dataset: 2536 , max train batch size 4 , num of opt steps: 50720
2023-07-01 03:21:22,367 - 1 day, 3:48:41 - 64.7s - INFO - __main__ - epoch 1/20 done , tot steps 634 , lr 5.9E-05 , loss 3.33 , qa loss 3.33 , lm loss 0.00 , avg batch size 4.0
2023-07-01 03:22:28,204 - 1 day, 3:49:47 - 65.8s - INFO - __main__ - epoch 2/20 done , tot steps 1268 , lr 5.6E-05 , loss 0.58 , qa loss 0.58 , lm loss 0.00 , avg batch size 4.0
2023-07-01 03:23:33,434 - 1 day, 3:50:52 - 65.2s - INFO - __main__ - epoch 3/20 done , tot steps 1902 , lr 5.3E-05 , loss 0.43 , qa loss 0.43 , lm loss 0.00 , avg batch size 4.0
2023-07-01 03:24:38,832 - 1 day, 3:51:58 - 65.4s - INFO - __main__ - epoch 4/20 done , tot steps 2536 , lr 5.0E-05 , loss 0.34 , qa loss 0.34 , lm loss 0.00 , avg batch size 4.0
2023-07-01 03:25:45,122 - 1 day, 3:53:04 - 66.3s - INFO - __main__ - epoch 5/20 done , tot steps 3170 , lr 4.7E-05 , loss 0.28 , qa loss 0.28 , lm loss 0.00 , avg batch size 4.0
2023-07-01 03:26:50,624 - 1 day, 3:54:09 - 65.5s - INFO - __main__ - epoch 6/20 done , tot steps 3804 , lr 4.4E-05 , loss 0.25 , qa loss 0.25 , lm loss 0.00 , avg batch size 4.0
2023-07-01 03:27:55,849 - 1 day, 3:55:15 - 65.2s - INFO - __main__ - epoch 7/20 done , tot steps 4438 , lr 4.1E-05 , loss 0.23 , qa loss 0.23 , lm loss 0.00 , avg batch size 4.0
2023-07-01 03:29:01,077 - 1 day, 3:56:20 - 65.2s - INFO - __main__ - epoch 8/20 done , tot steps 5072 , lr 3.8E-05 , loss 0.21 , qa loss 0.21 , lm loss 0.00 , avg batch size 4.0
2023-07-01 03:30:06,724 - 1 day, 3:57:26 - 65.6s - INFO - __main__ - epoch 9/20 done , tot steps 5706 , lr 3.4E-05 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-07-01 03:31:12,260 - 1 day, 3:58:31 - 65.5s - INFO - __main__ - epoch 10/20 done , tot steps 6340 , lr 3.1E-05 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-07-01 03:32:18,391 - 1 day, 3:59:37 - 66.1s - INFO - __main__ - epoch 11/20 done , tot steps 6974 , lr 2.8E-05 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
2023-07-01 03:33:22,901 - 1 day, 4:00:42 - 64.5s - INFO - __main__ - epoch 12/20 done , tot steps 7608 , lr 2.5E-05 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-07-01 03:34:28,251 - 1 day, 4:01:47 - 65.4s - INFO - __main__ - epoch 13/20 done , tot steps 8242 , lr 2.2E-05 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-07-01 03:35:33,501 - 1 day, 4:02:52 - 65.3s - INFO - __main__ - epoch 14/20 done , tot steps 8876 , lr 1.9E-05 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-07-01 03:36:39,582 - 1 day, 4:03:58 - 66.1s - INFO - __main__ - epoch 15/20 done , tot steps 9510 , lr 1.6E-05 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-07-01 03:37:44,775 - 1 day, 4:05:04 - 65.2s - INFO - __main__ - epoch 16/20 done , tot steps 10144 , lr 1.3E-05 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
2023-07-01 03:38:50,572 - 1 day, 4:06:09 - 65.8s - INFO - __main__ - epoch 17/20 done , tot steps 10778 , lr 9.4E-06 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
2023-07-01 03:39:55,307 - 1 day, 4:07:14 - 64.7s - INFO - __main__ - epoch 18/20 done , tot steps 11412 , lr 6.3E-06 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-07-01 03:41:01,139 - 1 day, 4:08:20 - 65.8s - INFO - __main__ - epoch 19/20 done , tot steps 12046 , lr 3.1E-06 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-07-01 03:42:12,833 - 1 day, 4:09:32 - 71.7s - INFO - __main__ - epoch 20/20 done , tot steps 12680 , lr 1.5E-08 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
The current active adapter is Stack[woz_en]
The task with which model is saved woz_en
Wall Execution time: 04:09:35
CPU Execution time: 14:07:11
