Not all gpus support fp16 training! Will use fp32 instead.
2023-06-29 23:14:25,162 - 0:00:04 - 0.0s - INFO - __main__ - args = Namespace(REG_TYPE_KEYS=['mas', 'ewc'], adam_epsilon=0.0001, add_task_tokens=False, data_dir='../../data', debug=False, decay_style='linear', device_ids=[0, 1], dynamic_epochs=False, fp32=True, gen_lm_sample_percentage=0.0, learning_rate=6.25e-05, lm_lambda=0.0, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[38915.52, 42158.48], min_batch_size=4, min_n_steps=1500, model_dir_root='../../model_ccmr2/gpt2/lll/sst_srl_woz.en_0.0', model_name='gpt2', n_gpus=2, n_train_epochs={'sst': 20, 'srl': 20, 'woz.en': 20}, n_warmup_ratio=0.005, n_workers=75, qp_margin=0.5, real_sample=False, reg_lambda=1.0, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[13620, 14755], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[13620, 14755], unbound=0, use_sep=False, weight_decay=0.01)
2023-06-29 23:14:25,163 - 0:00:04 - 0.0s - INFO - __main__ - start to train { task: ['sst'], seq train type: lll }
2023-06-29 23:14:25,170 - 0:00:04 - 0.0s - INFO - __main__ - extra training data size: 0
2023-06-29 23:14:28,108 - 0:00:07 - 2.9s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
2023-06-29 23:14:31,080 - 0:00:10 - 3.0s - INFO - __main__ - len of train dataset: 6920 , max train batch size 4 , num of opt steps: 138400
/home/student/2021/cs21mtech11006/anaconda3/envs/lamol/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/home/student/2021/cs21mtech11006/anaconda3/envs/lamol/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
2023-06-29 23:16:54,400 - 0:02:34 - 143.3s - INFO - __main__ - progress 0.578 , lr 6.1E-05 , loss 2.214 , qa loss 2.214 , lm loss 0.000 , avg batch size 4.0
2023-06-29 23:18:37,373 - 0:04:17 - 103.0s - INFO - __main__ - epoch 1/20 done , tot steps 1730 , lr 5.9E-05 , loss 1.42 , qa loss 1.42 , lm loss 0.00 , avg batch size 4.0
2023-06-29 23:20:59,018 - 0:06:38 - 141.6s - INFO - __main__ - progress 1.578 , lr 5.8E-05 , loss 0.281 , qa loss 0.281 , lm loss 0.000 , avg batch size 4.0
2023-06-29 23:22:40,256 - 0:08:19 - 101.2s - INFO - __main__ - epoch 2/20 done , tot steps 3460 , lr 5.6E-05 , loss 0.26 , qa loss 0.26 , lm loss 0.00 , avg batch size 4.0
2023-06-29 23:25:01,944 - 0:10:41 - 141.7s - INFO - __main__ - progress 2.578 , lr 5.4E-05 , loss 0.226 , qa loss 0.226 , lm loss 0.000 , avg batch size 4.0
2023-06-29 23:26:43,562 - 0:12:23 - 101.6s - INFO - __main__ - epoch 3/20 done , tot steps 5190 , lr 5.3E-05 , loss 0.22 , qa loss 0.22 , lm loss 0.00 , avg batch size 4.0
2023-06-29 23:29:06,371 - 0:14:46 - 142.8s - INFO - __main__ - progress 3.578 , lr 5.1E-05 , loss 0.200 , qa loss 0.200 , lm loss 0.000 , avg batch size 4.0
2023-06-29 23:30:46,976 - 0:16:26 - 100.6s - INFO - __main__ - epoch 4/20 done , tot steps 6920 , lr 5.0E-05 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-06-29 23:33:10,913 - 0:18:50 - 143.9s - INFO - __main__ - progress 4.578 , lr 4.8E-05 , loss 0.184 , qa loss 0.184 , lm loss 0.000 , avg batch size 4.0
2023-06-29 23:34:51,107 - 0:20:30 - 100.2s - INFO - __main__ - epoch 5/20 done , tot steps 8650 , lr 4.7E-05 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-06-29 23:37:13,840 - 0:22:53 - 142.7s - INFO - __main__ - progress 5.578 , lr 4.5E-05 , loss 0.169 , qa loss 0.169 , lm loss 0.000 , avg batch size 4.0
2023-06-29 23:38:55,050 - 0:24:34 - 101.2s - INFO - __main__ - epoch 6/20 done , tot steps 10380 , lr 4.4E-05 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
2023-06-29 23:41:17,400 - 0:26:57 - 142.4s - INFO - __main__ - progress 6.578 , lr 4.2E-05 , loss 0.155 , qa loss 0.155 , lm loss 0.000 , avg batch size 4.0
2023-06-29 23:42:59,055 - 0:28:38 - 101.7s - INFO - __main__ - epoch 7/20 done , tot steps 12110 , lr 4.1E-05 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-06-29 23:45:22,120 - 0:31:01 - 143.1s - INFO - __main__ - progress 7.578 , lr 3.9E-05 , loss 0.146 , qa loss 0.146 , lm loss 0.000 , avg batch size 4.0
2023-06-29 23:47:03,453 - 0:32:43 - 101.3s - INFO - __main__ - epoch 8/20 done , tot steps 13840 , lr 3.8E-05 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-06-29 23:49:27,408 - 0:35:07 - 144.0s - INFO - __main__ - progress 8.578 , lr 3.6E-05 , loss 0.140 , qa loss 0.140 , lm loss 0.000 , avg batch size 4.0
2023-06-29 23:51:08,474 - 0:36:48 - 101.1s - INFO - __main__ - epoch 9/20 done , tot steps 15570 , lr 3.4E-05 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
2023-06-29 23:53:33,388 - 0:39:13 - 144.9s - INFO - __main__ - progress 9.578 , lr 3.3E-05 , loss 0.133 , qa loss 0.133 , lm loss 0.000 , avg batch size 4.0
2023-06-29 23:55:12,552 - 0:40:52 - 99.2s - INFO - __main__ - epoch 10/20 done , tot steps 17300 , lr 3.1E-05 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
2023-06-29 23:57:35,951 - 0:43:15 - 143.4s - INFO - __main__ - progress 10.578 , lr 2.9E-05 , loss 0.130 , qa loss 0.130 , lm loss 0.000 , avg batch size 4.0
2023-06-29 23:59:16,915 - 0:44:56 - 101.0s - INFO - __main__ - epoch 11/20 done , tot steps 19030 , lr 2.8E-05 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-06-30 00:01:39,154 - 0:47:18 - 142.2s - INFO - __main__ - progress 11.578 , lr 2.6E-05 , loss 0.119 , qa loss 0.119 , lm loss 0.000 , avg batch size 4.0
2023-06-30 00:03:20,582 - 0:49:00 - 101.4s - INFO - __main__ - epoch 12/20 done , tot steps 20760 , lr 2.5E-05 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2023-06-30 00:05:43,066 - 0:51:22 - 142.5s - INFO - __main__ - progress 12.578 , lr 2.3E-05 , loss 0.123 , qa loss 0.123 , lm loss 0.000 , avg batch size 4.0
2023-06-30 00:07:23,538 - 0:53:03 - 100.5s - INFO - __main__ - epoch 13/20 done , tot steps 22490 , lr 2.2E-05 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2023-06-30 00:09:47,191 - 0:55:26 - 143.7s - INFO - __main__ - progress 13.578 , lr 2.0E-05 , loss 0.115 , qa loss 0.115 , lm loss 0.000 , avg batch size 4.0
2023-06-30 00:11:28,325 - 0:57:08 - 101.1s - INFO - __main__ - epoch 14/20 done , tot steps 24220 , lr 1.9E-05 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-06-30 00:13:53,175 - 0:59:32 - 144.9s - INFO - __main__ - progress 14.578 , lr 1.7E-05 , loss 0.109 , qa loss 0.109 , lm loss 0.000 , avg batch size 4.0
2023-06-30 00:15:32,459 - 1:01:12 - 99.3s - INFO - __main__ - epoch 15/20 done , tot steps 25950 , lr 1.6E-05 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2023-06-30 00:17:55,745 - 1:03:35 - 143.3s - INFO - __main__ - progress 15.578 , lr 1.4E-05 , loss 0.108 , qa loss 0.108 , lm loss 0.000 , avg batch size 4.0
2023-06-30 00:19:36,858 - 1:05:16 - 101.1s - INFO - __main__ - epoch 16/20 done , tot steps 27680 , lr 1.3E-05 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-06-30 00:21:59,044 - 1:07:38 - 142.2s - INFO - __main__ - progress 16.578 , lr 1.1E-05 , loss 0.102 , qa loss 0.102 , lm loss 0.000 , avg batch size 4.0
2023-06-30 00:23:41,117 - 1:09:20 - 102.1s - INFO - __main__ - epoch 17/20 done , tot steps 29410 , lr 9.4E-06 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 4.0
2023-06-30 00:26:05,476 - 1:11:45 - 144.4s - INFO - __main__ - progress 17.578 , lr 7.6E-06 , loss 0.103 , qa loss 0.103 , lm loss 0.000 , avg batch size 4.0
2023-06-30 00:27:45,406 - 1:13:25 - 99.9s - INFO - __main__ - epoch 18/20 done , tot steps 31140 , lr 6.3E-06 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 4.0
2023-06-30 00:30:07,086 - 1:15:46 - 141.7s - INFO - __main__ - progress 18.578 , lr 4.5E-06 , loss 0.107 , qa loss 0.107 , lm loss 0.000 , avg batch size 4.0
2023-06-30 00:31:49,856 - 1:17:29 - 102.8s - INFO - __main__ - epoch 19/20 done , tot steps 32870 , lr 3.1E-06 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 4.0
2023-06-30 00:34:11,993 - 1:19:51 - 142.1s - INFO - __main__ - progress 19.578 , lr 1.3E-06 , loss 0.101 , qa loss 0.101 , lm loss 0.000 , avg batch size 4.0
2023-06-30 00:35:50,622 - 1:21:30 - 98.6s - INFO - __main__ - epoch 20/20 done , tot steps 34600 , lr 1.5E-08 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 4.0
2023-06-30 00:36:00,322 - 1:21:40 - 9.7s - INFO - __main__ - start to train { task: ['srl'], seq train type: lll }
2023-06-30 00:36:03,068 - 1:21:42 - 2.7s - INFO - utils - writing extra data in ../../model_ccmr2/gpt2/lll/sst_srl_woz.en_0.0/sst/lm.csv ...
2023-06-30 00:36:03,158 - 1:21:42 - 0.1s - INFO - __main__ - extra training data size: 0
2023-06-30 00:36:03,476 - 1:21:43 - 0.3s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[sst]
The task with which model is saved sst
2023-06-30 00:36:06,697 - 1:21:46 - 3.2s - INFO - __main__ - len of train dataset: 6414 , max train batch size 4 , num of opt steps: 128280
2023-06-30 00:40:46,990 - 1:26:26 - 280.3s - INFO - __main__ - progress 0.624 , lr 6.1E-05 , loss 3.128 , qa loss 3.128 , lm loss 0.000 , avg batch size 4.0
2023-06-30 00:43:32,869 - 1:29:12 - 165.9s - INFO - __main__ - epoch 1/20 done , tot steps 1604 , lr 5.9E-05 , loss 2.37 , qa loss 2.37 , lm loss 0.00 , avg batch size 4.0
2023-06-30 00:48:16,415 - 1:33:56 - 283.5s - INFO - __main__ - progress 1.624 , lr 5.7E-05 , loss 0.927 , qa loss 0.927 , lm loss 0.000 , avg batch size 4.0
2023-06-30 00:50:54,609 - 1:36:34 - 158.2s - INFO - __main__ - epoch 2/20 done , tot steps 3208 , lr 5.6E-05 , loss 0.90 , qa loss 0.90 , lm loss 0.00 , avg batch size 4.0
2023-06-30 00:55:40,247 - 1:41:19 - 285.6s - INFO - __main__ - progress 2.624 , lr 5.4E-05 , loss 0.791 , qa loss 0.791 , lm loss 0.000 , avg batch size 4.0
2023-06-30 00:58:19,493 - 1:43:59 - 159.2s - INFO - __main__ - epoch 3/20 done , tot steps 4812 , lr 5.3E-05 , loss 0.76 , qa loss 0.76 , lm loss 0.00 , avg batch size 4.0
2023-06-30 01:03:03,704 - 1:48:43 - 284.2s - INFO - __main__ - progress 3.624 , lr 5.1E-05 , loss 0.678 , qa loss 0.678 , lm loss 0.000 , avg batch size 4.0
2023-06-30 01:05:44,974 - 1:51:24 - 161.3s - INFO - __main__ - epoch 4/20 done , tot steps 6416 , lr 5.0E-05 , loss 0.68 , qa loss 0.68 , lm loss 0.00 , avg batch size 4.0
2023-06-30 01:10:22,996 - 1:56:02 - 278.0s - INFO - __main__ - progress 4.624 , lr 4.8E-05 , loss 0.599 , qa loss 0.599 , lm loss 0.000 , avg batch size 4.0
2023-06-30 01:13:09,596 - 1:58:49 - 166.6s - INFO - __main__ - epoch 5/20 done , tot steps 8020 , lr 4.7E-05 , loss 0.61 , qa loss 0.61 , lm loss 0.00 , avg batch size 4.0
2023-06-30 01:17:49,982 - 2:03:29 - 280.4s - INFO - __main__ - progress 5.624 , lr 4.5E-05 , loss 0.585 , qa loss 0.585 , lm loss 0.000 , avg batch size 4.0
2023-06-30 01:20:33,153 - 2:06:12 - 163.2s - INFO - __main__ - epoch 6/20 done , tot steps 9624 , lr 4.4E-05 , loss 0.58 , qa loss 0.58 , lm loss 0.00 , avg batch size 4.0
2023-06-30 01:25:09,375 - 2:10:49 - 276.2s - INFO - __main__ - progress 6.624 , lr 4.2E-05 , loss 0.555 , qa loss 0.555 , lm loss 0.000 , avg batch size 4.0
2023-06-30 01:27:57,937 - 2:13:37 - 168.6s - INFO - __main__ - epoch 7/20 done , tot steps 11228 , lr 4.1E-05 , loss 0.55 , qa loss 0.55 , lm loss 0.00 , avg batch size 4.0
2023-06-30 01:32:39,421 - 2:18:19 - 281.5s - INFO - __main__ - progress 7.624 , lr 3.9E-05 , loss 0.513 , qa loss 0.513 , lm loss 0.000 , avg batch size 4.0
2023-06-30 01:35:23,047 - 2:21:02 - 163.6s - INFO - __main__ - epoch 8/20 done , tot steps 12832 , lr 3.8E-05 , loss 0.51 , qa loss 0.51 , lm loss 0.00 , avg batch size 4.0
2023-06-30 01:40:03,483 - 2:25:43 - 280.4s - INFO - __main__ - progress 8.624 , lr 3.6E-05 , loss 0.506 , qa loss 0.506 , lm loss 0.000 , avg batch size 4.0
2023-06-30 01:42:44,155 - 2:28:23 - 160.7s - INFO - __main__ - epoch 9/20 done , tot steps 14436 , lr 3.4E-05 , loss 0.50 , qa loss 0.50 , lm loss 0.00 , avg batch size 4.0
2023-06-30 01:47:24,734 - 2:33:04 - 280.6s - INFO - __main__ - progress 9.624 , lr 3.2E-05 , loss 0.472 , qa loss 0.472 , lm loss 0.000 , avg batch size 4.0
2023-06-30 01:50:07,021 - 2:35:46 - 162.3s - INFO - __main__ - epoch 10/20 done , tot steps 16040 , lr 3.1E-05 , loss 0.48 , qa loss 0.48 , lm loss 0.00 , avg batch size 4.0
2023-06-30 01:54:45,392 - 2:40:25 - 278.4s - INFO - __main__ - progress 10.624 , lr 2.9E-05 , loss 0.457 , qa loss 0.457 , lm loss 0.000 , avg batch size 4.0
2023-06-30 01:57:28,895 - 2:43:08 - 163.5s - INFO - __main__ - epoch 11/20 done , tot steps 17644 , lr 2.8E-05 , loss 0.47 , qa loss 0.47 , lm loss 0.00 , avg batch size 4.0
2023-06-30 02:02:09,440 - 2:47:49 - 280.5s - INFO - __main__ - progress 11.624 , lr 2.6E-05 , loss 0.433 , qa loss 0.433 , lm loss 0.000 , avg batch size 4.0
2023-06-30 02:04:54,334 - 2:50:34 - 164.9s - INFO - __main__ - epoch 12/20 done , tot steps 19248 , lr 2.5E-05 , loss 0.44 , qa loss 0.44 , lm loss 0.00 , avg batch size 4.0
2023-06-30 02:09:32,176 - 2:55:11 - 277.8s - INFO - __main__ - progress 12.624 , lr 2.3E-05 , loss 0.428 , qa loss 0.428 , lm loss 0.000 , avg batch size 4.0
2023-06-30 02:12:19,042 - 2:57:58 - 166.9s - INFO - __main__ - epoch 13/20 done , tot steps 20852 , lr 2.2E-05 , loss 0.43 , qa loss 0.43 , lm loss 0.00 , avg batch size 4.0
2023-06-30 02:17:04,252 - 3:02:43 - 285.2s - INFO - __main__ - progress 13.624 , lr 2.0E-05 , loss 0.409 , qa loss 0.409 , lm loss 0.000 , avg batch size 4.0
2023-06-30 02:19:47,365 - 3:05:27 - 163.1s - INFO - __main__ - epoch 14/20 done , tot steps 22456 , lr 1.9E-05 , loss 0.42 , qa loss 0.42 , lm loss 0.00 , avg batch size 4.0
2023-06-30 02:24:26,977 - 3:10:06 - 279.6s - INFO - __main__ - progress 14.624 , lr 1.7E-05 , loss 0.416 , qa loss 0.416 , lm loss 0.000 , avg batch size 4.0
2023-06-30 02:27:10,316 - 3:12:50 - 163.3s - INFO - __main__ - epoch 15/20 done , tot steps 24060 , lr 1.6E-05 , loss 0.41 , qa loss 0.41 , lm loss 0.00 , avg batch size 4.0
2023-06-30 02:31:43,848 - 3:17:23 - 273.5s - INFO - __main__ - progress 15.624 , lr 1.4E-05 , loss 0.397 , qa loss 0.397 , lm loss 0.000 , avg batch size 4.0
2023-06-30 02:34:29,626 - 3:20:09 - 165.8s - INFO - __main__ - epoch 16/20 done , tot steps 25664 , lr 1.3E-05 , loss 0.39 , qa loss 0.39 , lm loss 0.00 , avg batch size 4.0
2023-06-30 02:39:07,069 - 3:24:46 - 277.4s - INFO - __main__ - progress 16.624 , lr 1.1E-05 , loss 0.390 , qa loss 0.390 , lm loss 0.000 , avg batch size 4.0
2023-06-30 02:41:53,670 - 3:27:33 - 166.6s - INFO - __main__ - epoch 17/20 done , tot steps 27268 , lr 9.4E-06 , loss 0.39 , qa loss 0.39 , lm loss 0.00 , avg batch size 4.0
2023-06-30 02:46:36,250 - 3:32:15 - 282.6s - INFO - __main__ - progress 17.624 , lr 7.4E-06 , loss 0.381 , qa loss 0.381 , lm loss 0.000 , avg batch size 4.0
2023-06-30 02:49:18,806 - 3:34:58 - 162.6s - INFO - __main__ - epoch 18/20 done , tot steps 28872 , lr 6.3E-06 , loss 0.38 , qa loss 0.38 , lm loss 0.00 , avg batch size 4.0
2023-06-30 02:53:53,693 - 3:39:33 - 274.9s - INFO - __main__ - progress 18.624 , lr 4.3E-06 , loss 0.381 , qa loss 0.381 , lm loss 0.000 , avg batch size 4.0
2023-06-30 02:56:40,975 - 3:42:20 - 167.3s - INFO - __main__ - epoch 19/20 done , tot steps 30476 , lr 3.1E-06 , loss 0.38 , qa loss 0.38 , lm loss 0.00 , avg batch size 4.0
2023-06-30 03:01:20,486 - 3:47:00 - 279.5s - INFO - __main__ - progress 19.624 , lr 1.2E-06 , loss 0.372 , qa loss 0.372 , lm loss 0.000 , avg batch size 4.0
2023-06-30 03:03:54,455 - 3:49:34 - 154.0s - INFO - __main__ - epoch 20/20 done , tot steps 32080 , lr 1.6E-08 , loss 0.36 , qa loss 0.36 , lm loss 0.00 , avg batch size 4.0
2023-06-30 03:04:02,748 - 3:49:42 - 8.3s - INFO - __main__ - start to train { task: ['woz.en'], seq train type: lll }
2023-06-30 03:04:06,002 - 3:49:45 - 3.3s - INFO - utils - writing extra data in ../../model_ccmr2/gpt2/lll/sst_srl_woz.en_0.0/srl/lm.csv ...
2023-06-30 03:04:06,023 - 3:49:45 - 0.0s - INFO - __main__ - extra training data size: 0
2023-06-30 03:04:06,400 - 3:49:46 - 0.4s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[srl]
The task with which model is saved srl
2023-06-30 03:04:09,923 - 3:49:49 - 3.5s - INFO - __main__ - len of train dataset: 2536 , max train batch size 4 , num of opt steps: 50720
2023-06-30 03:05:32,019 - 3:51:11 - 82.1s - INFO - __main__ - epoch 1/20 done , tot steps 634 , lr 5.9E-05 , loss 4.24 , qa loss 4.24 , lm loss 0.00 , avg batch size 4.0
2023-06-30 03:06:54,159 - 3:52:33 - 82.1s - INFO - __main__ - epoch 2/20 done , tot steps 1268 , lr 5.6E-05 , loss 0.61 , qa loss 0.61 , lm loss 0.00 , avg batch size 4.0
2023-06-30 03:08:15,491 - 3:53:55 - 81.3s - INFO - __main__ - epoch 3/20 done , tot steps 1902 , lr 5.3E-05 , loss 0.45 , qa loss 0.45 , lm loss 0.00 , avg batch size 4.0
2023-06-30 03:09:37,578 - 3:55:17 - 82.1s - INFO - __main__ - epoch 4/20 done , tot steps 2536 , lr 5.0E-05 , loss 0.36 , qa loss 0.36 , lm loss 0.00 , avg batch size 4.0
2023-06-30 03:10:59,350 - 3:56:39 - 81.8s - INFO - __main__ - epoch 5/20 done , tot steps 3170 , lr 4.7E-05 , loss 0.31 , qa loss 0.31 , lm loss 0.00 , avg batch size 4.0
2023-06-30 03:12:20,799 - 3:58:00 - 81.4s - INFO - __main__ - epoch 6/20 done , tot steps 3804 , lr 4.4E-05 , loss 0.27 , qa loss 0.27 , lm loss 0.00 , avg batch size 4.0
2023-06-30 03:13:42,783 - 3:59:22 - 82.0s - INFO - __main__ - epoch 7/20 done , tot steps 4438 , lr 4.1E-05 , loss 0.25 , qa loss 0.25 , lm loss 0.00 , avg batch size 4.0
2023-06-30 03:15:05,099 - 4:00:44 - 82.3s - INFO - __main__ - epoch 8/20 done , tot steps 5072 , lr 3.8E-05 , loss 0.23 , qa loss 0.23 , lm loss 0.00 , avg batch size 4.0
2023-06-30 03:16:27,158 - 4:02:06 - 82.1s - INFO - __main__ - epoch 9/20 done , tot steps 5706 , lr 3.4E-05 , loss 0.21 , qa loss 0.21 , lm loss 0.00 , avg batch size 4.0
2023-06-30 03:17:48,994 - 4:03:28 - 81.8s - INFO - __main__ - epoch 10/20 done , tot steps 6340 , lr 3.1E-05 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-06-30 03:19:10,392 - 4:04:50 - 81.4s - INFO - __main__ - epoch 11/20 done , tot steps 6974 , lr 2.8E-05 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-06-30 03:20:31,880 - 4:06:11 - 81.5s - INFO - __main__ - epoch 12/20 done , tot steps 7608 , lr 2.5E-05 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-06-30 03:21:53,282 - 4:07:33 - 81.4s - INFO - __main__ - epoch 13/20 done , tot steps 8242 , lr 2.2E-05 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-06-30 03:23:14,493 - 4:08:54 - 81.2s - INFO - __main__ - epoch 14/20 done , tot steps 8876 , lr 1.9E-05 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
2023-06-30 03:24:35,932 - 4:10:15 - 81.4s - INFO - __main__ - epoch 15/20 done , tot steps 9510 , lr 1.6E-05 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-06-30 03:25:57,008 - 4:11:36 - 81.1s - INFO - __main__ - epoch 16/20 done , tot steps 10144 , lr 1.3E-05 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-06-30 03:27:19,100 - 4:12:58 - 82.1s - INFO - __main__ - epoch 17/20 done , tot steps 10778 , lr 9.4E-06 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-06-30 03:28:40,492 - 4:14:20 - 81.4s - INFO - __main__ - epoch 18/20 done , tot steps 11412 , lr 6.3E-06 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-06-30 03:30:03,290 - 4:15:43 - 82.8s - INFO - __main__ - epoch 19/20 done , tot steps 12046 , lr 3.1E-06 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
2023-06-30 03:31:35,878 - 4:17:15 - 92.6s - INFO - __main__ - epoch 20/20 done , tot steps 12680 , lr 1.5E-08 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
The current active adapter is Stack[woz_en]
The task with which model is saved woz_en
Wall Execution time: 04:17:22
CPU Execution time: 04:15:50
