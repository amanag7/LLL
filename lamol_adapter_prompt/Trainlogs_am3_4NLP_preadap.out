Not all gpus support fp16 training! Will use fp32 instead.
2023-06-11 12:03:52,198 - 0:00:05 - 0.0s - INFO - __main__ - args = Namespace(REG_TYPE_KEYS=['mas', 'ewc'], adam_epsilon=0.0001, add_task_tokens=False, data_dir='../../data', debug=False, decay_style='linear', device_ids=[1, 2, 3, 4], dynamic_epochs=False, fp32=True, gen_lm_sample_percentage=0.0, learning_rate=6.25e-05, lm_lambda=0.0, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[35672.56, 42158.48, 42158.48, 42158.48], min_batch_size=4, min_n_steps=1500, model_dir_root='../../model_am3/gpt2/lll/sst_srl_zre_woz.en_0.0', model_name='gpt2', n_gpus=4, n_train_epochs={'sst': 20, 'srl': 20, 'zre': 20, 'woz.en': 20}, n_warmup_ratio=0.005, n_workers=75, qp_margin=0.5, real_sample=False, reg_lambda=1.0, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['sst', 'srl', 'zre', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[12485, 14755, 14755, 14755], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[12485, 14755, 14755, 14755], unbound=0, use_sep=False, weight_decay=0.01)
2023-06-11 12:03:52,198 - 0:00:05 - 0.0s - INFO - __main__ - start to train { task: ['sst'], seq train type: lll }
2023-06-11 12:03:52,198 - 0:00:05 - 0.0s - INFO - __main__ - extra training data size: 0
2023-06-11 12:04:07,808 - 0:00:21 - 15.6s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
2023-06-11 12:04:10,671 - 0:00:23 - 2.9s - INFO - __main__ - len of train dataset: 6920 , max train batch size 4 , num of opt steps: 138400
/home/student/2021/cs21mtech11006/anaconda3/envs/lamol/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/home/student/2021/cs21mtech11006/anaconda3/envs/lamol/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
2023-06-11 12:06:06,129 - 0:02:19 - 115.5s - INFO - __main__ - progress 0.578 , lr 6.1E-05 , loss 1.206 , qa loss 1.206 , lm loss 0.000 , avg batch size 4.0
2023-06-11 12:07:30,149 - 0:03:43 - 84.0s - INFO - __main__ - epoch 1/20 done , tot steps 1730 , lr 5.9E-05 , loss 0.82 , qa loss 0.82 , lm loss 0.00 , avg batch size 4.0
2023-06-11 12:09:18,716 - 0:05:31 - 108.6s - INFO - __main__ - progress 1.578 , lr 5.8E-05 , loss 0.261 , qa loss 0.261 , lm loss 0.000 , avg batch size 4.0
2023-06-11 12:10:37,091 - 0:06:50 - 78.4s - INFO - __main__ - epoch 2/20 done , tot steps 3460 , lr 5.6E-05 , loss 0.25 , qa loss 0.25 , lm loss 0.00 , avg batch size 4.0
2023-06-11 12:12:26,095 - 0:08:39 - 109.0s - INFO - __main__ - progress 2.578 , lr 5.4E-05 , loss 0.211 , qa loss 0.211 , lm loss 0.000 , avg batch size 4.0
2023-06-11 12:13:44,275 - 0:09:57 - 78.2s - INFO - __main__ - epoch 3/20 done , tot steps 5190 , lr 5.3E-05 , loss 0.21 , qa loss 0.21 , lm loss 0.00 , avg batch size 4.0
2023-06-11 12:15:33,471 - 0:11:46 - 109.2s - INFO - __main__ - progress 3.578 , lr 5.1E-05 , loss 0.178 , qa loss 0.178 , lm loss 0.000 , avg batch size 4.0
2023-06-11 12:16:51,706 - 0:13:04 - 78.2s - INFO - __main__ - epoch 4/20 done , tot steps 6920 , lr 5.0E-05 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-06-11 12:18:40,130 - 0:14:53 - 108.4s - INFO - __main__ - progress 4.578 , lr 4.8E-05 , loss 0.151 , qa loss 0.151 , lm loss 0.000 , avg batch size 4.0
2023-06-11 12:19:58,630 - 0:16:11 - 78.5s - INFO - __main__ - epoch 5/20 done , tot steps 8650 , lr 4.7E-05 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-06-11 12:21:46,770 - 0:18:00 - 108.1s - INFO - __main__ - progress 5.578 , lr 4.5E-05 , loss 0.136 , qa loss 0.136 , lm loss 0.000 , avg batch size 4.0
2023-06-11 12:23:05,286 - 0:19:18 - 78.5s - INFO - __main__ - epoch 6/20 done , tot steps 10380 , lr 4.4E-05 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
2023-06-11 12:24:53,004 - 0:21:06 - 107.7s - INFO - __main__ - progress 6.578 , lr 4.2E-05 , loss 0.110 , qa loss 0.110 , lm loss 0.000 , avg batch size 4.0
2023-06-11 12:26:11,601 - 0:22:24 - 78.6s - INFO - __main__ - epoch 7/20 done , tot steps 12110 , lr 4.1E-05 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2023-06-11 12:27:59,732 - 0:24:12 - 108.1s - INFO - __main__ - progress 7.578 , lr 3.9E-05 , loss 0.093 , qa loss 0.093 , lm loss 0.000 , avg batch size 4.0
2023-06-11 12:29:18,723 - 0:25:31 - 79.0s - INFO - __main__ - epoch 8/20 done , tot steps 13840 , lr 3.8E-05 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 4.0
2023-06-11 12:31:06,557 - 0:27:19 - 107.8s - INFO - __main__ - progress 8.578 , lr 3.6E-05 , loss 0.078 , qa loss 0.078 , lm loss 0.000 , avg batch size 4.0
2023-06-11 12:32:24,772 - 0:28:38 - 78.2s - INFO - __main__ - epoch 9/20 done , tot steps 15570 , lr 3.4E-05 , loss 0.09 , qa loss 0.09 , lm loss 0.00 , avg batch size 4.0
2023-06-11 12:34:14,350 - 0:30:27 - 109.6s - INFO - __main__ - progress 9.578 , lr 3.3E-05 , loss 0.069 , qa loss 0.069 , lm loss 0.000 , avg batch size 4.0
2023-06-11 12:35:32,655 - 0:31:45 - 78.3s - INFO - __main__ - epoch 10/20 done , tot steps 17300 , lr 3.1E-05 , loss 0.07 , qa loss 0.07 , lm loss 0.00 , avg batch size 4.0
2023-06-11 12:37:21,453 - 0:33:34 - 108.8s - INFO - __main__ - progress 10.578 , lr 2.9E-05 , loss 0.054 , qa loss 0.054 , lm loss 0.000 , avg batch size 4.0
2023-06-11 12:38:40,405 - 0:34:53 - 79.0s - INFO - __main__ - epoch 11/20 done , tot steps 19030 , lr 2.8E-05 , loss 0.05 , qa loss 0.05 , lm loss 0.00 , avg batch size 4.0
2023-06-11 12:40:29,630 - 0:36:42 - 109.2s - INFO - __main__ - progress 11.578 , lr 2.6E-05 , loss 0.039 , qa loss 0.039 , lm loss 0.000 , avg batch size 4.0
2023-06-11 12:41:47,734 - 0:38:01 - 78.1s - INFO - __main__ - epoch 12/20 done , tot steps 20760 , lr 2.5E-05 , loss 0.04 , qa loss 0.04 , lm loss 0.00 , avg batch size 4.0
2023-06-11 12:43:36,834 - 0:39:50 - 109.1s - INFO - __main__ - progress 12.578 , lr 2.3E-05 , loss 0.033 , qa loss 0.033 , lm loss 0.000 , avg batch size 4.0
2023-06-11 12:44:55,159 - 0:41:08 - 78.3s - INFO - __main__ - epoch 13/20 done , tot steps 22490 , lr 2.2E-05 , loss 0.04 , qa loss 0.04 , lm loss 0.00 , avg batch size 4.0
2023-06-11 12:46:43,609 - 0:42:56 - 108.4s - INFO - __main__ - progress 13.578 , lr 2.0E-05 , loss 0.027 , qa loss 0.027 , lm loss 0.000 , avg batch size 4.0
2023-06-11 12:48:03,095 - 0:44:16 - 79.5s - INFO - __main__ - epoch 14/20 done , tot steps 24220 , lr 1.9E-05 , loss 0.03 , qa loss 0.03 , lm loss 0.00 , avg batch size 4.0
2023-06-11 12:49:52,788 - 0:46:06 - 109.7s - INFO - __main__ - progress 14.578 , lr 1.7E-05 , loss 0.022 , qa loss 0.022 , lm loss 0.000 , avg batch size 4.0
2023-06-11 12:51:10,437 - 0:47:23 - 77.6s - INFO - __main__ - epoch 15/20 done , tot steps 25950 , lr 1.6E-05 , loss 0.02 , qa loss 0.02 , lm loss 0.00 , avg batch size 4.0
2023-06-11 12:52:58,165 - 0:49:11 - 107.7s - INFO - __main__ - progress 15.578 , lr 1.4E-05 , loss 0.021 , qa loss 0.021 , lm loss 0.000 , avg batch size 4.0
2023-06-11 12:54:17,425 - 0:50:30 - 79.3s - INFO - __main__ - epoch 16/20 done , tot steps 27680 , lr 1.3E-05 , loss 0.02 , qa loss 0.02 , lm loss 0.00 , avg batch size 4.0
2023-06-11 12:56:04,334 - 0:52:17 - 106.9s - INFO - __main__ - progress 16.578 , lr 1.1E-05 , loss 0.015 , qa loss 0.015 , lm loss 0.000 , avg batch size 4.0
2023-06-11 12:57:22,920 - 0:53:36 - 78.6s - INFO - __main__ - epoch 17/20 done , tot steps 29410 , lr 9.4E-06 , loss 0.01 , qa loss 0.01 , lm loss 0.00 , avg batch size 4.0
2023-06-11 12:59:12,181 - 0:55:25 - 109.3s - INFO - __main__ - progress 17.578 , lr 7.6E-06 , loss 0.014 , qa loss 0.014 , lm loss 0.000 , avg batch size 4.0
2023-06-11 13:00:30,401 - 0:56:43 - 78.2s - INFO - __main__ - epoch 18/20 done , tot steps 31140 , lr 6.3E-06 , loss 0.02 , qa loss 0.02 , lm loss 0.00 , avg batch size 4.0
2023-06-11 13:02:19,952 - 0:58:33 - 109.6s - INFO - __main__ - progress 18.578 , lr 4.5E-06 , loss 0.008 , qa loss 0.008 , lm loss 0.000 , avg batch size 4.0
2023-06-11 13:03:38,614 - 0:59:51 - 78.7s - INFO - __main__ - epoch 19/20 done , tot steps 32870 , lr 3.1E-06 , loss 0.01 , qa loss 0.01 , lm loss 0.00 , avg batch size 4.0
2023-06-11 13:05:28,700 - 1:01:41 - 110.1s - INFO - __main__ - progress 19.578 , lr 1.3E-06 , loss 0.010 , qa loss 0.010 , lm loss 0.000 , avg batch size 4.0
2023-06-11 13:06:45,189 - 1:02:58 - 76.5s - INFO - __main__ - epoch 20/20 done , tot steps 34600 , lr 1.5E-08 , loss 0.01 , qa loss 0.01 , lm loss 0.00 , avg batch size 4.0
2023-06-11 13:06:53,427 - 1:03:06 - 8.2s - INFO - __main__ - start to train { task: ['srl'], seq train type: lll }
2023-06-11 13:07:08,022 - 1:03:21 - 14.6s - INFO - utils - writing extra data in ../../model_am3/gpt2/lll/sst_srl_zre_woz.en_0.0/sst/lm.csv ...
2023-06-11 13:07:08,037 - 1:03:21 - 0.0s - INFO - __main__ - extra training data size: 0
2023-06-11 13:07:08,168 - 1:03:21 - 0.1s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[sst]
The task with which model is saved sst
2023-06-11 13:07:12,162 - 1:03:25 - 4.0s - INFO - __main__ - len of train dataset: 6414 , max train batch size 4 , num of opt steps: 128280
2023-06-11 13:10:03,291 - 1:06:16 - 171.1s - INFO - __main__ - progress 0.624 , lr 6.1E-05 , loss 2.573 , qa loss 2.573 , lm loss 0.000 , avg batch size 4.0
2023-06-11 13:11:51,172 - 1:08:04 - 107.9s - INFO - __main__ - epoch 1/20 done , tot steps 1604 , lr 5.9E-05 , loss 1.89 , qa loss 1.89 , lm loss 0.00 , avg batch size 4.0
2023-06-11 13:14:40,029 - 1:10:53 - 168.9s - INFO - __main__ - progress 1.624 , lr 5.7E-05 , loss 0.601 , qa loss 0.601 , lm loss 0.000 , avg batch size 4.0
2023-06-11 13:16:24,748 - 1:12:38 - 104.7s - INFO - __main__ - epoch 2/20 done , tot steps 3208 , lr 5.6E-05 , loss 0.58 , qa loss 0.58 , lm loss 0.00 , avg batch size 4.0
2023-06-11 13:19:17,374 - 1:15:30 - 172.6s - INFO - __main__ - progress 2.624 , lr 5.4E-05 , loss 0.476 , qa loss 0.476 , lm loss 0.000 , avg batch size 4.0
2023-06-11 13:20:58,911 - 1:17:12 - 101.5s - INFO - __main__ - epoch 3/20 done , tot steps 4812 , lr 5.3E-05 , loss 0.46 , qa loss 0.46 , lm loss 0.00 , avg batch size 4.0
2023-06-11 13:23:48,287 - 1:20:01 - 169.4s - INFO - __main__ - progress 3.624 , lr 5.1E-05 , loss 0.386 , qa loss 0.386 , lm loss 0.000 , avg batch size 4.0
2023-06-11 13:25:32,772 - 1:21:46 - 104.5s - INFO - __main__ - epoch 4/20 done , tot steps 6416 , lr 5.0E-05 , loss 0.38 , qa loss 0.38 , lm loss 0.00 , avg batch size 4.0
2023-06-11 13:28:25,896 - 1:24:39 - 173.1s - INFO - __main__ - progress 4.624 , lr 4.8E-05 , loss 0.329 , qa loss 0.329 , lm loss 0.000 , avg batch size 4.0
2023-06-11 13:30:08,495 - 1:26:21 - 102.6s - INFO - __main__ - epoch 5/20 done , tot steps 8020 , lr 4.7E-05 , loss 0.32 , qa loss 0.32 , lm loss 0.00 , avg batch size 4.0
2023-06-11 13:32:59,862 - 1:29:13 - 171.4s - INFO - __main__ - progress 5.624 , lr 4.5E-05 , loss 0.281 , qa loss 0.281 , lm loss 0.000 , avg batch size 4.0
2023-06-11 13:34:42,094 - 1:30:55 - 102.2s - INFO - __main__ - epoch 6/20 done , tot steps 9624 , lr 4.4E-05 , loss 0.28 , qa loss 0.28 , lm loss 0.00 , avg batch size 4.0
2023-06-11 13:37:32,173 - 1:33:45 - 170.1s - INFO - __main__ - progress 6.624 , lr 4.2E-05 , loss 0.241 , qa loss 0.241 , lm loss 0.000 , avg batch size 4.0
2023-06-11 13:39:15,014 - 1:35:28 - 102.8s - INFO - __main__ - epoch 7/20 done , tot steps 11228 , lr 4.1E-05 , loss 0.24 , qa loss 0.24 , lm loss 0.00 , avg batch size 4.0
2023-06-11 13:42:07,652 - 1:38:20 - 172.6s - INFO - __main__ - progress 7.624 , lr 3.9E-05 , loss 0.211 , qa loss 0.211 , lm loss 0.000 , avg batch size 4.0
2023-06-11 13:43:47,279 - 1:40:00 - 99.6s - INFO - __main__ - epoch 8/20 done , tot steps 12832 , lr 3.8E-05 , loss 0.21 , qa loss 0.21 , lm loss 0.00 , avg batch size 4.0
2023-06-11 13:46:37,936 - 1:42:51 - 170.7s - INFO - __main__ - progress 8.624 , lr 3.6E-05 , loss 0.166 , qa loss 0.166 , lm loss 0.000 , avg batch size 4.0
2023-06-11 13:48:23,045 - 1:44:36 - 105.1s - INFO - __main__ - epoch 9/20 done , tot steps 14436 , lr 3.4E-05 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
2023-06-11 13:51:14,194 - 1:47:27 - 171.1s - INFO - __main__ - progress 9.624 , lr 3.2E-05 , loss 0.153 , qa loss 0.153 , lm loss 0.000 , avg batch size 4.0
2023-06-11 13:52:57,557 - 1:49:10 - 103.4s - INFO - __main__ - epoch 10/20 done , tot steps 16040 , lr 3.1E-05 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-06-11 13:55:47,379 - 1:52:00 - 169.8s - INFO - __main__ - progress 10.624 , lr 2.9E-05 , loss 0.128 , qa loss 0.128 , lm loss 0.000 , avg batch size 4.0
2023-06-11 13:57:31,263 - 1:53:44 - 103.9s - INFO - __main__ - epoch 11/20 done , tot steps 17644 , lr 2.8E-05 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-06-11 14:00:22,151 - 1:56:35 - 170.9s - INFO - __main__ - progress 11.624 , lr 2.6E-05 , loss 0.118 , qa loss 0.118 , lm loss 0.000 , avg batch size 4.0
2023-06-11 14:02:06,131 - 1:58:19 - 104.0s - INFO - __main__ - epoch 12/20 done , tot steps 19248 , lr 2.5E-05 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2023-06-11 14:04:56,327 - 2:01:09 - 170.2s - INFO - __main__ - progress 12.624 , lr 2.3E-05 , loss 0.104 , qa loss 0.104 , lm loss 0.000 , avg batch size 4.0
2023-06-11 14:06:41,133 - 2:02:54 - 104.8s - INFO - __main__ - epoch 13/20 done , tot steps 20852 , lr 2.2E-05 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-06-11 14:09:30,000 - 2:05:43 - 168.9s - INFO - __main__ - progress 13.624 , lr 2.0E-05 , loss 0.086 , qa loss 0.086 , lm loss 0.000 , avg batch size 4.0
2023-06-11 14:11:16,006 - 2:07:29 - 106.0s - INFO - __main__ - epoch 14/20 done , tot steps 22456 , lr 1.9E-05 , loss 0.09 , qa loss 0.09 , lm loss 0.00 , avg batch size 4.0
2023-06-11 14:14:06,319 - 2:10:19 - 170.3s - INFO - __main__ - progress 14.624 , lr 1.7E-05 , loss 0.078 , qa loss 0.078 , lm loss 0.000 , avg batch size 4.0
2023-06-11 14:15:49,064 - 2:12:02 - 102.7s - INFO - __main__ - epoch 15/20 done , tot steps 24060 , lr 1.6E-05 , loss 0.08 , qa loss 0.08 , lm loss 0.00 , avg batch size 4.0
2023-06-11 14:18:38,762 - 2:14:52 - 169.7s - INFO - __main__ - progress 15.624 , lr 1.4E-05 , loss 0.072 , qa loss 0.072 , lm loss 0.000 , avg batch size 4.0
2023-06-11 14:20:21,696 - 2:16:34 - 102.9s - INFO - __main__ - epoch 16/20 done , tot steps 25664 , lr 1.3E-05 , loss 0.07 , qa loss 0.07 , lm loss 0.00 , avg batch size 4.0
2023-06-11 14:23:13,055 - 2:19:26 - 171.4s - INFO - __main__ - progress 16.624 , lr 1.1E-05 , loss 0.061 , qa loss 0.061 , lm loss 0.000 , avg batch size 4.0
2023-06-11 14:24:54,829 - 2:21:08 - 101.8s - INFO - __main__ - epoch 17/20 done , tot steps 27268 , lr 9.4E-06 , loss 0.06 , qa loss 0.06 , lm loss 0.00 , avg batch size 4.0
2023-06-11 14:27:46,233 - 2:23:59 - 171.4s - INFO - __main__ - progress 17.624 , lr 7.4E-06 , loss 0.051 , qa loss 0.051 , lm loss 0.000 , avg batch size 4.0
2023-06-11 14:29:27,565 - 2:25:40 - 101.3s - INFO - __main__ - epoch 18/20 done , tot steps 28872 , lr 6.3E-06 , loss 0.05 , qa loss 0.05 , lm loss 0.00 , avg batch size 4.0
2023-06-11 14:32:17,996 - 2:28:31 - 170.4s - INFO - __main__ - progress 18.624 , lr 4.3E-06 , loss 0.045 , qa loss 0.045 , lm loss 0.000 , avg batch size 4.0
2023-06-11 14:34:00,823 - 2:30:14 - 102.8s - INFO - __main__ - epoch 19/20 done , tot steps 30476 , lr 3.1E-06 , loss 0.05 , qa loss 0.05 , lm loss 0.00 , avg batch size 4.0
2023-06-11 14:36:54,324 - 2:33:07 - 173.5s - INFO - __main__ - progress 19.624 , lr 1.2E-06 , loss 0.039 , qa loss 0.039 , lm loss 0.000 , avg batch size 4.0
2023-06-11 14:38:35,162 - 2:34:48 - 100.8s - INFO - __main__ - epoch 20/20 done , tot steps 32080 , lr 1.6E-08 , loss 0.04 , qa loss 0.04 , lm loss 0.00 , avg batch size 4.0
2023-06-11 14:38:43,384 - 2:34:56 - 8.2s - INFO - __main__ - start to train { task: ['zre'], seq train type: lll }
2023-06-11 14:38:46,401 - 2:34:59 - 3.0s - INFO - utils - writing extra data in ../../model_am3/gpt2/lll/sst_srl_zre_woz.en_0.0/srl/lm.csv ...
2023-06-11 14:38:46,402 - 2:34:59 - 0.0s - INFO - __main__ - extra training data size: 0
2023-06-11 14:38:46,683 - 2:34:59 - 0.3s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[srl]
The task with which model is saved srl
2023-06-11 14:39:27,414 - 2:35:40 - 40.7s - INFO - __main__ - len of train dataset: 840000 , max train batch size 560 , num of opt steps: 16800000
2023-06-11 14:55:20,922 - 2:51:34 - 953.5s - INFO - __main__ - progress 0.190 , lr 6.2E-05 , loss 1.760 , qa loss 1.760 , lm loss 0.000 , avg batch size 159.6
2023-06-11 15:10:46,980 - 3:07:00 - 926.1s - INFO - __main__ - progress 0.380 , lr 6.1E-05 , loss 1.036 , qa loss 1.036 , lm loss 0.000 , avg batch size 159.5
2023-06-11 15:26:09,938 - 3:22:23 - 923.0s - INFO - __main__ - progress 0.570 , lr 6.1E-05 , loss 0.773 , qa loss 0.773 , lm loss 0.000 , avg batch size 159.6
2023-06-11 15:41:37,393 - 3:37:50 - 927.5s - INFO - __main__ - progress 0.761 , lr 6.0E-05 , loss 0.632 , qa loss 0.632 , lm loss 0.000 , avg batch size 159.8
2023-06-11 15:57:00,797 - 3:53:14 - 923.4s - INFO - __main__ - progress 0.951 , lr 6.0E-05 , loss 0.543 , qa loss 0.543 , lm loss 0.000 , avg batch size 159.8
2023-06-11 16:01:09,062 - 3:57:22 - 248.3s - INFO - __main__ - epoch 1/20 done , tot steps 5258 , lr 5.9E-05 , loss 0.52 , qa loss 0.52 , lm loss 0.00 , avg batch size 159.8
2023-06-11 16:16:51,900 - 4:13:05 - 942.8s - INFO - __main__ - progress 1.190 , lr 5.9E-05 , loss 0.159 , qa loss 0.159 , lm loss 0.000 , avg batch size 159.5
2023-06-11 16:32:16,888 - 4:28:30 - 925.0s - INFO - __main__ - progress 1.381 , lr 5.8E-05 , loss 0.152 , qa loss 0.152 , lm loss 0.000 , avg batch size 160.1
2023-06-11 16:47:42,264 - 4:43:55 - 925.4s - INFO - __main__ - progress 1.571 , lr 5.8E-05 , loss 0.147 , qa loss 0.147 , lm loss 0.000 , avg batch size 160.0
2023-06-11 17:03:04,273 - 4:59:17 - 922.0s - INFO - __main__ - progress 1.761 , lr 5.7E-05 , loss 0.142 , qa loss 0.142 , lm loss 0.000 , avg batch size 159.9
2023-06-11 17:18:23,308 - 5:14:36 - 919.0s - INFO - __main__ - progress 1.951 , lr 5.6E-05 , loss 0.138 , qa loss 0.138 , lm loss 0.000 , avg batch size 159.7
2023-06-11 17:22:27,016 - 5:18:40 - 243.7s - INFO - __main__ - epoch 2/20 done , tot steps 10516 , lr 5.6E-05 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 159.8
2023-06-11 17:38:06,561 - 5:34:19 - 939.5s - INFO - __main__ - progress 2.190 , lr 5.6E-05 , loss 0.107 , qa loss 0.107 , lm loss 0.000 , avg batch size 159.9
2023-06-11 17:53:30,402 - 5:49:43 - 923.8s - INFO - __main__ - progress 2.381 , lr 5.5E-05 , loss 0.105 , qa loss 0.105 , lm loss 0.000 , avg batch size 160.1
2023-06-11 18:08:53,985 - 6:05:07 - 923.6s - INFO - __main__ - progress 2.571 , lr 5.4E-05 , loss 0.103 , qa loss 0.103 , lm loss 0.000 , avg batch size 160.0
2023-06-11 18:24:16,735 - 6:20:30 - 922.7s - INFO - __main__ - progress 2.760 , lr 5.4E-05 , loss 0.100 , qa loss 0.100 , lm loss 0.000 , avg batch size 159.5
2023-06-11 18:39:40,069 - 6:35:53 - 923.3s - INFO - __main__ - progress 2.950 , lr 5.3E-05 , loss 0.098 , qa loss 0.098 , lm loss 0.000 , avg batch size 159.5
2023-06-11 18:43:45,648 - 6:39:58 - 245.6s - INFO - __main__ - epoch 3/20 done , tot steps 15778 , lr 5.3E-05 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 159.6
2023-06-11 18:59:25,162 - 6:55:38 - 939.5s - INFO - __main__ - progress 3.188 , lr 5.3E-05 , loss 0.080 , qa loss 0.080 , lm loss 0.000 , avg batch size 158.1
2023-06-11 19:14:45,847 - 7:10:59 - 920.7s - INFO - __main__ - progress 3.379 , lr 5.2E-05 , loss 0.079 , qa loss 0.079 , lm loss 0.000 , avg batch size 159.4
2023-06-11 19:30:08,690 - 7:26:21 - 922.8s - INFO - __main__ - progress 3.569 , lr 5.1E-05 , loss 0.078 , qa loss 0.078 , lm loss 0.000 , avg batch size 159.3
2023-06-11 19:45:32,569 - 7:41:45 - 923.9s - INFO - __main__ - progress 3.760 , lr 5.1E-05 , loss 0.077 , qa loss 0.077 , lm loss 0.000 , avg batch size 159.7
