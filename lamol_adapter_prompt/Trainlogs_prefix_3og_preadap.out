Not all gpus support fp16 training! Will use fp32 instead.
2023-04-25 09:59:42,274 - 0:00:04 - 0.0s - INFO - __main__ - args = Namespace(REG_TYPE_KEYS=['mas', 'ewc'], adam_epsilon=0.0001, add_task_tokens=False, data_dir='../../data', debug=False, decay_style='linear', device_ids=[1, 2, 3, 4, 5, 6], dynamic_epochs=False, fp32=True, gen_lm_sample_percentage=0.0, learning_rate=6.25e-05, lm_lambda=0.0, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[32429.600000000002, 42158.48, 42158.48, 42158.48, 42158.48, 42158.48], min_batch_size=4, min_n_steps=1500, model_dir_root='../../model_prefix/gpt2/lll/sst_srl_woz.en_0.0', model_name='gpt2', n_gpus=6, n_train_epochs={'sst': 12, 'srl': 12, 'woz.en': 12}, n_warmup_ratio=0.005, n_workers=75, qp_margin=0.5, real_sample=False, reg_lambda=1.0, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[11350, 14755, 14755, 14755, 14755, 14755], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[11350, 14755, 14755, 14755, 14755, 14755], unbound=0, use_sep=False, weight_decay=0.01)
2023-04-25 09:59:42,274 - 0:00:04 - 0.0s - INFO - __main__ - start to train { task: ['sst'], seq train type: lll }
2023-04-25 09:59:42,274 - 0:00:04 - 0.0s - INFO - __main__ - extra training data size: 0
2023-04-25 09:59:44,550 - 0:00:07 - 2.3s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
2023-04-25 09:59:47,029 - 0:00:09 - 2.5s - INFO - __main__ - len of train dataset: 6920 , max train batch size 4 , num of opt steps: 83040
/home/student/2021/cs21mtech11006/anaconda3/envs/lamol/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/home/student/2021/cs21mtech11006/anaconda3/envs/lamol/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
2023-04-25 10:01:19,861 - 0:01:42 - 92.8s - INFO - __main__ - progress 0.578 , lr 6.0E-05 , loss 10.434 , qa loss 10.434 , lm loss 0.000 , avg batch size 4.0
2023-04-25 10:02:30,210 - 0:02:52 - 70.3s - INFO - __main__ - epoch 1/12 done , tot steps 1730 , lr 5.7E-05 , loss 6.27 , qa loss 6.27 , lm loss 0.00 , avg batch size 4.0
2023-04-25 10:03:58,353 - 0:04:20 - 88.1s - INFO - __main__ - progress 1.578 , lr 5.4E-05 , loss 0.405 , qa loss 0.405 , lm loss 0.000 , avg batch size 4.0
2023-04-25 10:05:02,699 - 0:05:25 - 64.3s - INFO - __main__ - epoch 2/12 done , tot steps 3460 , lr 5.2E-05 , loss 0.44 , qa loss 0.44 , lm loss 0.00 , avg batch size 4.0
2023-04-25 10:06:31,073 - 0:06:53 - 88.4s - INFO - __main__ - progress 2.578 , lr 4.9E-05 , loss 0.419 , qa loss 0.419 , lm loss 0.000 , avg batch size 4.0
2023-04-25 10:07:36,709 - 0:07:59 - 65.6s - INFO - __main__ - epoch 3/12 done , tot steps 5190 , lr 4.7E-05 , loss 0.40 , qa loss 0.40 , lm loss 0.00 , avg batch size 4.0
2023-04-25 10:09:03,865 - 0:09:26 - 87.2s - INFO - __main__ - progress 3.578 , lr 4.4E-05 , loss 0.351 , qa loss 0.351 , lm loss 0.000 , avg batch size 4.0
2023-04-25 10:10:08,554 - 0:10:31 - 64.7s - INFO - __main__ - epoch 4/12 done , tot steps 6920 , lr 4.2E-05 , loss 0.35 , qa loss 0.35 , lm loss 0.00 , avg batch size 4.0
2023-04-25 10:11:35,901 - 0:11:58 - 87.3s - INFO - __main__ - progress 4.578 , lr 3.9E-05 , loss 0.310 , qa loss 0.310 , lm loss 0.000 , avg batch size 4.0
2023-04-25 10:12:41,385 - 0:13:03 - 65.5s - INFO - __main__ - epoch 5/12 done , tot steps 8650 , lr 3.6E-05 , loss 0.31 , qa loss 0.31 , lm loss 0.00 , avg batch size 4.0
2023-04-25 10:14:09,514 - 0:14:32 - 88.1s - INFO - __main__ - progress 5.578 , lr 3.3E-05 , loss 0.306 , qa loss 0.306 , lm loss 0.000 , avg batch size 4.0
2023-04-25 10:15:14,373 - 0:15:36 - 64.9s - INFO - __main__ - epoch 6/12 done , tot steps 10380 , lr 3.1E-05 , loss 0.31 , qa loss 0.31 , lm loss 0.00 , avg batch size 4.0
2023-04-25 10:16:41,618 - 0:17:04 - 87.2s - INFO - __main__ - progress 6.578 , lr 2.8E-05 , loss 0.290 , qa loss 0.290 , lm loss 0.000 , avg batch size 4.0
2023-04-25 10:17:47,015 - 0:18:09 - 65.4s - INFO - __main__ - epoch 7/12 done , tot steps 12110 , lr 2.6E-05 , loss 0.29 , qa loss 0.29 , lm loss 0.00 , avg batch size 4.0
2023-04-25 10:19:16,762 - 0:19:39 - 89.7s - INFO - __main__ - progress 7.578 , lr 2.3E-05 , loss 0.295 , qa loss 0.295 , lm loss 0.000 , avg batch size 4.0
2023-04-25 10:20:21,171 - 0:20:43 - 64.4s - INFO - __main__ - epoch 8/12 done , tot steps 13840 , lr 2.1E-05 , loss 0.29 , qa loss 0.29 , lm loss 0.00 , avg batch size 4.0
2023-04-25 10:21:48,801 - 0:22:11 - 87.6s - INFO - __main__ - progress 8.578 , lr 1.8E-05 , loss 0.277 , qa loss 0.277 , lm loss 0.000 , avg batch size 4.0
2023-04-25 10:22:53,484 - 0:23:15 - 64.7s - INFO - __main__ - epoch 9/12 done , tot steps 15570 , lr 1.6E-05 , loss 0.27 , qa loss 0.27 , lm loss 0.00 , avg batch size 4.0
2023-04-25 10:24:22,723 - 0:24:45 - 89.2s - INFO - __main__ - progress 9.578 , lr 1.3E-05 , loss 0.288 , qa loss 0.288 , lm loss 0.000 , avg batch size 4.0
2023-04-25 10:25:27,327 - 0:25:49 - 64.6s - INFO - __main__ - epoch 10/12 done , tot steps 17300 , lr 1.0E-05 , loss 0.28 , qa loss 0.28 , lm loss 0.00 , avg batch size 4.0
2023-04-25 10:26:54,233 - 0:27:16 - 86.9s - INFO - __main__ - progress 10.578 , lr 7.4E-06 , loss 0.273 , qa loss 0.273 , lm loss 0.000 , avg batch size 4.0
2023-04-25 10:27:59,709 - 0:28:22 - 65.5s - INFO - __main__ - epoch 11/12 done , tot steps 19030 , lr 5.2E-06 , loss 0.27 , qa loss 0.27 , lm loss 0.00 , avg batch size 4.0
2023-04-25 10:29:29,067 - 0:29:51 - 89.4s - INFO - __main__ - progress 11.578 , lr 2.2E-06 , loss 0.267 , qa loss 0.267 , lm loss 0.000 , avg batch size 4.0
2023-04-25 10:30:33,514 - 0:30:56 - 64.4s - INFO - __main__ - epoch 12/12 done , tot steps 20760 , lr 2.6E-08 , loss 0.27 , qa loss 0.27 , lm loss 0.00 , avg batch size 4.0
2023-04-25 10:30:40,034 - 0:31:02 - 6.5s - INFO - __main__ - start to train { task: ['srl'], seq train type: lll }
2023-04-25 10:30:43,734 - 0:31:06 - 3.7s - INFO - utils - writing extra data in ../../model_prefix/gpt2/lll/sst_srl_woz.en_0.0/sst/lm.csv ...
2023-04-25 10:30:43,735 - 0:31:06 - 0.0s - INFO - __main__ - extra training data size: 0
2023-04-25 10:30:43,848 - 0:31:06 - 0.1s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[sst]
The task with which model is saved sst
2023-04-25 10:30:46,448 - 0:31:08 - 2.6s - INFO - __main__ - len of train dataset: 6414 , max train batch size 4 , num of opt steps: 76968
2023-04-25 10:33:17,963 - 0:33:40 - 151.5s - INFO - __main__ - progress 0.624 , lr 5.9E-05 , loss 12.910 , qa loss 12.910 , lm loss 0.000 , avg batch size 4.0
2023-04-25 10:34:54,961 - 0:35:17 - 97.0s - INFO - __main__ - epoch 1/12 done , tot steps 1604 , lr 5.7E-05 , loss 9.75 , qa loss 9.75 , lm loss 0.00 , avg batch size 4.0
2023-04-25 10:37:25,100 - 0:37:47 - 150.1s - INFO - __main__ - progress 1.624 , lr 5.4E-05 , loss 3.457 , qa loss 3.457 , lm loss 0.000 , avg batch size 4.0
2023-04-25 10:38:57,460 - 0:39:19 - 92.4s - INFO - __main__ - epoch 2/12 done , tot steps 3208 , lr 5.2E-05 , loss 3.23 , qa loss 3.23 , lm loss 0.00 , avg batch size 4.0
2023-04-25 10:41:28,174 - 0:41:50 - 150.7s - INFO - __main__ - progress 2.624 , lr 4.9E-05 , loss 2.415 , qa loss 2.415 , lm loss 0.000 , avg batch size 4.0
2023-04-25 10:43:00,205 - 0:43:22 - 92.0s - INFO - __main__ - epoch 3/12 done , tot steps 4812 , lr 4.7E-05 , loss 2.30 , qa loss 2.30 , lm loss 0.00 , avg batch size 4.0
2023-04-25 10:45:33,070 - 0:45:55 - 152.9s - INFO - __main__ - progress 3.624 , lr 4.4E-05 , loss 1.887 , qa loss 1.887 , lm loss 0.000 , avg batch size 4.0
2023-04-25 10:47:05,163 - 0:47:27 - 92.1s - INFO - __main__ - epoch 4/12 done , tot steps 6416 , lr 4.2E-05 , loss 1.83 , qa loss 1.83 , lm loss 0.00 , avg batch size 4.0
2023-04-25 10:49:41,876 - 0:50:04 - 156.7s - INFO - __main__ - progress 4.624 , lr 3.8E-05 , loss 1.625 , qa loss 1.625 , lm loss 0.000 , avg batch size 4.0
2023-04-25 10:51:09,937 - 0:51:32 - 88.1s - INFO - __main__ - epoch 5/12 done , tot steps 8020 , lr 3.6E-05 , loss 1.59 , qa loss 1.59 , lm loss 0.00 , avg batch size 4.0
2023-04-25 10:53:38,022 - 0:54:00 - 148.1s - INFO - __main__ - progress 5.624 , lr 3.3E-05 , loss 1.457 , qa loss 1.457 , lm loss 0.000 , avg batch size 4.0
2023-04-25 10:55:11,609 - 0:55:34 - 93.6s - INFO - __main__ - epoch 6/12 done , tot steps 9624 , lr 3.1E-05 , loss 1.45 , qa loss 1.45 , lm loss 0.00 , avg batch size 4.0
2023-04-25 10:57:42,784 - 0:58:05 - 151.2s - INFO - __main__ - progress 6.624 , lr 2.8E-05 , loss 1.362 , qa loss 1.362 , lm loss 0.000 , avg batch size 4.0
2023-04-25 10:59:14,874 - 0:59:37 - 92.1s - INFO - __main__ - epoch 7/12 done , tot steps 11228 , lr 2.6E-05 , loss 1.36 , qa loss 1.36 , lm loss 0.00 , avg batch size 4.0
2023-04-25 11:01:47,277 - 1:02:09 - 152.4s - INFO - __main__ - progress 7.624 , lr 2.3E-05 , loss 1.314 , qa loss 1.314 , lm loss 0.000 , avg batch size 4.0
2023-04-25 11:03:18,540 - 1:03:41 - 91.3s - INFO - __main__ - epoch 8/12 done , tot steps 12832 , lr 2.1E-05 , loss 1.30 , qa loss 1.30 , lm loss 0.00 , avg batch size 4.0
2023-04-25 11:05:52,396 - 1:06:14 - 153.9s - INFO - __main__ - progress 8.624 , lr 1.8E-05 , loss 1.244 , qa loss 1.244 , lm loss 0.000 , avg batch size 4.0
2023-04-25 11:07:22,828 - 1:07:45 - 90.4s - INFO - __main__ - epoch 9/12 done , tot steps 14436 , lr 1.6E-05 , loss 1.26 , qa loss 1.26 , lm loss 0.00 , avg batch size 4.0
2023-04-25 11:09:53,223 - 1:10:15 - 150.4s - INFO - __main__ - progress 9.624 , lr 1.2E-05 , loss 1.216 , qa loss 1.216 , lm loss 0.000 , avg batch size 4.0
2023-04-25 11:11:26,538 - 1:11:49 - 93.3s - INFO - __main__ - epoch 10/12 done , tot steps 16040 , lr 1.0E-05 , loss 1.22 , qa loss 1.22 , lm loss 0.00 , avg batch size 4.0
2023-04-25 11:13:58,070 - 1:14:20 - 151.5s - INFO - __main__ - progress 10.624 , lr 7.2E-06 , loss 1.195 , qa loss 1.195 , lm loss 0.000 , avg batch size 4.0
2023-04-25 11:15:28,683 - 1:15:51 - 90.6s - INFO - __main__ - epoch 11/12 done , tot steps 17644 , lr 5.2E-06 , loss 1.21 , qa loss 1.21 , lm loss 0.00 , avg batch size 4.0
2023-04-25 11:18:02,623 - 1:18:25 - 153.9s - INFO - __main__ - progress 11.624 , lr 2.0E-06 , loss 1.217 , qa loss 1.217 , lm loss 0.000 , avg batch size 4.0
2023-04-25 11:19:31,690 - 1:19:54 - 89.1s - INFO - __main__ - epoch 12/12 done , tot steps 19248 , lr 2.6E-08 , loss 1.21 , qa loss 1.21 , lm loss 0.00 , avg batch size 4.0
2023-04-25 11:19:38,232 - 1:20:00 - 6.5s - INFO - __main__ - start to train { task: ['woz.en'], seq train type: lll }
2023-04-25 11:19:41,458 - 1:20:03 - 3.2s - INFO - utils - writing extra data in ../../model_prefix/gpt2/lll/sst_srl_woz.en_0.0/srl/lm.csv ...
2023-04-25 11:19:41,459 - 1:20:03 - 0.0s - INFO - __main__ - extra training data size: 0
2023-04-25 11:19:41,589 - 1:20:04 - 0.1s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[sst]
The task with which model is saved sst
2023-04-25 11:19:44,935 - 1:20:07 - 3.3s - INFO - __main__ - len of train dataset: 2536 , max train batch size 4 , num of opt steps: 30432
2023-04-25 11:20:48,052 - 1:21:10 - 63.1s - INFO - __main__ - epoch 1/12 done , tot steps 634 , lr 5.7E-05 , loss 19.86 , qa loss 19.86 , lm loss 0.00 , avg batch size 4.0
2023-04-25 11:21:43,778 - 1:22:06 - 55.7s - INFO - __main__ - epoch 2/12 done , tot steps 1268 , lr 5.2E-05 , loss 3.01 , qa loss 3.01 , lm loss 0.00 , avg batch size 4.0
2023-04-25 11:22:40,560 - 1:23:03 - 56.8s - INFO - __main__ - epoch 3/12 done , tot steps 1902 , lr 4.7E-05 , loss 1.64 , qa loss 1.64 , lm loss 0.00 , avg batch size 4.0
2023-04-25 11:23:36,703 - 1:23:59 - 56.1s - INFO - __main__ - epoch 4/12 done , tot steps 2536 , lr 4.2E-05 , loss 1.30 , qa loss 1.30 , lm loss 0.00 , avg batch size 4.0
2023-04-25 11:24:32,936 - 1:24:55 - 56.2s - INFO - __main__ - epoch 5/12 done , tot steps 3170 , lr 3.6E-05 , loss 1.13 , qa loss 1.13 , lm loss 0.00 , avg batch size 4.0
2023-04-25 11:25:29,375 - 1:25:51 - 56.4s - INFO - __main__ - epoch 6/12 done , tot steps 3804 , lr 3.1E-05 , loss 1.04 , qa loss 1.04 , lm loss 0.00 , avg batch size 4.0
2023-04-25 11:26:25,740 - 1:26:48 - 56.4s - INFO - __main__ - epoch 7/12 done , tot steps 4438 , lr 2.6E-05 , loss 0.94 , qa loss 0.94 , lm loss 0.00 , avg batch size 4.0
2023-04-25 11:27:21,830 - 1:27:44 - 56.1s - INFO - __main__ - epoch 8/12 done , tot steps 5072 , lr 2.1E-05 , loss 0.91 , qa loss 0.91 , lm loss 0.00 , avg batch size 4.0
2023-04-25 11:28:18,051 - 1:28:40 - 56.2s - INFO - __main__ - epoch 9/12 done , tot steps 5706 , lr 1.6E-05 , loss 0.87 , qa loss 0.87 , lm loss 0.00 , avg batch size 4.0
2023-04-25 11:29:14,446 - 1:29:36 - 56.4s - INFO - __main__ - epoch 10/12 done , tot steps 6340 , lr 1.0E-05 , loss 0.83 , qa loss 0.83 , lm loss 0.00 , avg batch size 4.0
2023-04-25 11:30:11,090 - 1:30:33 - 56.6s - INFO - __main__ - epoch 11/12 done , tot steps 6974 , lr 5.2E-06 , loss 0.82 , qa loss 0.82 , lm loss 0.00 , avg batch size 4.0
2023-04-25 11:31:07,094 - 1:31:29 - 56.0s - INFO - __main__ - epoch 12/12 done , tot steps 7608 , lr 2.5E-08 , loss 0.83 , qa loss 0.83 , lm loss 0.00 , avg batch size 4.0
The current active adapter is Stack[sst]
The task with which model is saved sst
Wall Execution time: 01:31:31
CPU Execution time: 01:33:45
