Not all gpus support fp16 training! Will use fp32 instead.
2023-06-08 11:28:29,000 - 0:00:14 - 0.0s - INFO - __main__ - args = Namespace(REG_TYPE_KEYS=['mas', 'ewc'], adam_epsilon=0.0001, add_task_tokens=False, data_dir='../../data', debug=False, decay_style='linear', device_ids=[0, 1, 2, 3], dynamic_epochs=False, fp32=True, gen_lm_sample_percentage=0.0, learning_rate=6.25e-05, lm_lambda=0.0, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[35672.56, 42158.48, 42158.48, 42158.48], min_batch_size=4, min_n_steps=1500, model_dir_root='../../model_am2/gpt2/lll/sst_srl_zre_woz.en_0.0', model_name='gpt2', n_gpus=4, n_train_epochs={'sst': 20, 'srl': 20, 'zre': 20, 'woz.en': 20}, n_warmup_ratio=0.005, n_workers=75, qp_margin=0.5, real_sample=False, reg_lambda=1.0, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['sst', 'srl', 'zre', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[12485, 14755, 14755, 14755], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[12485, 14755, 14755, 14755], unbound=0, use_sep=False, weight_decay=0.01)
2023-06-08 11:28:29,000 - 0:00:14 - 0.0s - INFO - __main__ - start to train { task: ['sst'], seq train type: lll }
2023-06-08 11:28:29,001 - 0:00:14 - 0.0s - INFO - __main__ - extra training data size: 0
2023-06-08 11:28:31,340 - 0:00:16 - 2.3s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
2023-06-08 11:28:35,281 - 0:00:20 - 3.9s - INFO - __main__ - len of train dataset: 6920 , max train batch size 4 , num of opt steps: 138400
/home/student/2021/cs21mtech11006/anaconda3/envs/lamol/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/home/student/2021/cs21mtech11006/anaconda3/envs/lamol/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
2023-06-08 11:30:27,498 - 0:02:12 - 112.2s - INFO - __main__ - progress 0.578 , lr 6.1E-05 , loss 2.182 , qa loss 2.182 , lm loss 0.000 , avg batch size 4.0
2023-06-08 11:31:51,559 - 0:03:36 - 84.1s - INFO - __main__ - epoch 1/20 done , tot steps 1730 , lr 5.9E-05 , loss 1.39 , qa loss 1.39 , lm loss 0.00 , avg batch size 4.0
2023-06-08 11:33:42,654 - 0:05:27 - 111.1s - INFO - __main__ - progress 1.578 , lr 5.8E-05 , loss 0.262 , qa loss 0.262 , lm loss 0.000 , avg batch size 4.0
2023-06-08 11:35:03,922 - 0:06:49 - 81.3s - INFO - __main__ - epoch 2/20 done , tot steps 3460 , lr 5.6E-05 , loss 0.25 , qa loss 0.25 , lm loss 0.00 , avg batch size 4.0
2023-06-08 11:36:54,137 - 0:08:39 - 110.2s - INFO - __main__ - progress 2.578 , lr 5.4E-05 , loss 0.223 , qa loss 0.223 , lm loss 0.000 , avg batch size 4.0
2023-06-08 11:38:14,577 - 0:09:59 - 80.4s - INFO - __main__ - epoch 3/20 done , tot steps 5190 , lr 5.3E-05 , loss 0.22 , qa loss 0.22 , lm loss 0.00 , avg batch size 4.0
2023-06-08 11:40:04,947 - 0:11:50 - 110.4s - INFO - __main__ - progress 3.578 , lr 5.1E-05 , loss 0.192 , qa loss 0.192 , lm loss 0.000 , avg batch size 4.0
2023-06-08 11:41:25,061 - 0:13:10 - 80.1s - INFO - __main__ - epoch 4/20 done , tot steps 6920 , lr 5.0E-05 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-06-08 11:43:15,042 - 0:15:00 - 110.0s - INFO - __main__ - progress 4.578 , lr 4.8E-05 , loss 0.160 , qa loss 0.160 , lm loss 0.000 , avg batch size 4.0
2023-06-08 11:44:35,433 - 0:16:20 - 80.4s - INFO - __main__ - epoch 5/20 done , tot steps 8650 , lr 4.7E-05 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-06-08 11:46:25,521 - 0:18:10 - 110.1s - INFO - __main__ - progress 5.578 , lr 4.5E-05 , loss 0.135 , qa loss 0.135 , lm loss 0.000 , avg batch size 4.0
2023-06-08 11:47:46,100 - 0:19:31 - 80.6s - INFO - __main__ - epoch 6/20 done , tot steps 10380 , lr 4.4E-05 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
2023-06-08 11:49:37,148 - 0:21:22 - 111.0s - INFO - __main__ - progress 6.578 , lr 4.2E-05 , loss 0.115 , qa loss 0.115 , lm loss 0.000 , avg batch size 4.0
2023-06-08 11:50:56,414 - 0:22:41 - 79.3s - INFO - __main__ - epoch 7/20 done , tot steps 12110 , lr 4.1E-05 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2023-06-08 11:52:47,350 - 0:24:32 - 110.9s - INFO - __main__ - progress 7.578 , lr 3.9E-05 , loss 0.087 , qa loss 0.087 , lm loss 0.000 , avg batch size 4.0
2023-06-08 11:54:06,814 - 0:25:52 - 79.5s - INFO - __main__ - epoch 8/20 done , tot steps 13840 , lr 3.8E-05 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 4.0
2023-06-08 11:55:56,822 - 0:27:42 - 110.0s - INFO - __main__ - progress 8.578 , lr 3.6E-05 , loss 0.081 , qa loss 0.081 , lm loss 0.000 , avg batch size 4.0
2023-06-08 11:57:16,969 - 0:29:02 - 80.1s - INFO - __main__ - epoch 9/20 done , tot steps 15570 , lr 3.4E-05 , loss 0.08 , qa loss 0.08 , lm loss 0.00 , avg batch size 4.0
2023-06-08 11:59:08,804 - 0:30:54 - 111.8s - INFO - __main__ - progress 9.578 , lr 3.3E-05 , loss 0.061 , qa loss 0.061 , lm loss 0.000 , avg batch size 4.0
2023-06-08 12:00:29,834 - 0:32:15 - 81.0s - INFO - __main__ - epoch 10/20 done , tot steps 17300 , lr 3.1E-05 , loss 0.06 , qa loss 0.06 , lm loss 0.00 , avg batch size 4.0
2023-06-08 12:02:21,615 - 0:34:06 - 111.8s - INFO - __main__ - progress 10.578 , lr 2.9E-05 , loss 0.054 , qa loss 0.054 , lm loss 0.000 , avg batch size 4.0
2023-06-08 12:03:41,152 - 0:35:26 - 79.5s - INFO - __main__ - epoch 11/20 done , tot steps 19030 , lr 2.8E-05 , loss 0.06 , qa loss 0.06 , lm loss 0.00 , avg batch size 4.0
2023-06-08 12:05:31,567 - 0:37:16 - 110.4s - INFO - __main__ - progress 11.578 , lr 2.6E-05 , loss 0.041 , qa loss 0.041 , lm loss 0.000 , avg batch size 4.0
2023-06-08 12:06:51,171 - 0:38:36 - 79.6s - INFO - __main__ - epoch 12/20 done , tot steps 20760 , lr 2.5E-05 , loss 0.04 , qa loss 0.04 , lm loss 0.00 , avg batch size 4.0
2023-06-08 12:08:40,780 - 0:40:25 - 109.6s - INFO - __main__ - progress 12.578 , lr 2.3E-05 , loss 0.034 , qa loss 0.034 , lm loss 0.000 , avg batch size 4.0
2023-06-08 12:10:02,511 - 0:41:47 - 81.7s - INFO - __main__ - epoch 13/20 done , tot steps 22490 , lr 2.2E-05 , loss 0.03 , qa loss 0.03 , lm loss 0.00 , avg batch size 4.0
2023-06-08 12:11:54,727 - 0:43:39 - 112.2s - INFO - __main__ - progress 13.578 , lr 2.0E-05 , loss 0.036 , qa loss 0.036 , lm loss 0.000 , avg batch size 4.0
2023-06-08 12:13:14,507 - 0:44:59 - 79.8s - INFO - __main__ - epoch 14/20 done , tot steps 24220 , lr 1.9E-05 , loss 0.03 , qa loss 0.03 , lm loss 0.00 , avg batch size 4.0
2023-06-08 12:15:07,299 - 0:46:52 - 112.8s - INFO - __main__ - progress 14.578 , lr 1.7E-05 , loss 0.031 , qa loss 0.031 , lm loss 0.000 , avg batch size 4.0
2023-06-08 12:16:27,562 - 0:48:12 - 80.3s - INFO - __main__ - epoch 15/20 done , tot steps 25950 , lr 1.6E-05 , loss 0.03 , qa loss 0.03 , lm loss 0.00 , avg batch size 4.0
2023-06-08 12:18:19,203 - 0:50:04 - 111.6s - INFO - __main__ - progress 15.578 , lr 1.4E-05 , loss 0.021 , qa loss 0.021 , lm loss 0.000 , avg batch size 4.0
2023-06-08 12:19:40,447 - 0:51:25 - 81.2s - INFO - __main__ - epoch 16/20 done , tot steps 27680 , lr 1.3E-05 , loss 0.02 , qa loss 0.02 , lm loss 0.00 , avg batch size 4.0
2023-06-08 12:21:30,116 - 0:53:15 - 109.7s - INFO - __main__ - progress 16.578 , lr 1.1E-05 , loss 0.017 , qa loss 0.017 , lm loss 0.000 , avg batch size 4.0
2023-06-08 12:22:51,174 - 0:54:36 - 81.1s - INFO - __main__ - epoch 17/20 done , tot steps 29410 , lr 9.4E-06 , loss 0.02 , qa loss 0.02 , lm loss 0.00 , avg batch size 4.0
2023-06-08 12:24:42,202 - 0:56:27 - 111.0s - INFO - __main__ - progress 17.578 , lr 7.6E-06 , loss 0.015 , qa loss 0.015 , lm loss 0.000 , avg batch size 4.0
2023-06-08 12:26:02,252 - 0:57:47 - 80.1s - INFO - __main__ - epoch 18/20 done , tot steps 31140 , lr 6.3E-06 , loss 0.01 , qa loss 0.01 , lm loss 0.00 , avg batch size 4.0
2023-06-08 12:27:52,238 - 0:59:37 - 110.0s - INFO - __main__ - progress 18.578 , lr 4.5E-06 , loss 0.008 , qa loss 0.008 , lm loss 0.000 , avg batch size 4.0
2023-06-08 12:29:11,745 - 1:00:56 - 79.5s - INFO - __main__ - epoch 19/20 done , tot steps 32870 , lr 3.1E-06 , loss 0.01 , qa loss 0.01 , lm loss 0.00 , avg batch size 4.0
2023-06-08 12:31:01,256 - 1:02:46 - 109.5s - INFO - __main__ - progress 19.578 , lr 1.3E-06 , loss 0.010 , qa loss 0.010 , lm loss 0.000 , avg batch size 4.0
2023-06-08 12:32:22,151 - 1:04:07 - 80.9s - INFO - __main__ - epoch 20/20 done , tot steps 34600 , lr 1.5E-08 , loss 0.01 , qa loss 0.01 , lm loss 0.00 , avg batch size 4.0
2023-06-08 12:32:30,401 - 1:04:15 - 8.3s - INFO - __main__ - start to train { task: ['srl'], seq train type: lll }
2023-06-08 12:32:34,412 - 1:04:19 - 4.0s - INFO - utils - writing extra data in ../../model_am2/gpt2/lll/sst_srl_zre_woz.en_0.0/sst/lm.csv ...
2023-06-08 12:32:34,413 - 1:04:19 - 0.0s - INFO - __main__ - extra training data size: 0
2023-06-08 12:32:34,613 - 1:04:19 - 0.2s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[sst]
The task with which model is saved sst
2023-06-08 12:32:40,416 - 1:04:25 - 5.8s - INFO - __main__ - len of train dataset: 6414 , max train batch size 4 , num of opt steps: 128280
2023-06-08 12:35:32,300 - 1:07:17 - 171.9s - INFO - __main__ - progress 0.624 , lr 6.1E-05 , loss 2.379 , qa loss 2.379 , lm loss 0.000 , avg batch size 4.0
2023-06-08 12:37:25,099 - 1:09:10 - 112.8s - INFO - __main__ - epoch 1/20 done , tot steps 1604 , lr 5.9E-05 , loss 1.80 , qa loss 1.80 , lm loss 0.00 , avg batch size 4.0
2023-06-08 12:40:16,017 - 1:12:01 - 170.9s - INFO - __main__ - progress 1.624 , lr 5.7E-05 , loss 0.744 , qa loss 0.744 , lm loss 0.000 , avg batch size 4.0
2023-06-08 12:42:01,437 - 1:13:46 - 105.4s - INFO - __main__ - epoch 2/20 done , tot steps 3208 , lr 5.6E-05 , loss 0.70 , qa loss 0.70 , lm loss 0.00 , avg batch size 4.0
2023-06-08 12:44:56,953 - 1:16:42 - 175.5s - INFO - __main__ - progress 2.624 , lr 5.4E-05 , loss 0.508 , qa loss 0.508 , lm loss 0.000 , avg batch size 4.0
2023-06-08 12:46:37,623 - 1:18:22 - 100.7s - INFO - __main__ - epoch 3/20 done , tot steps 4812 , lr 5.3E-05 , loss 0.51 , qa loss 0.51 , lm loss 0.00 , avg batch size 4.0
2023-06-08 12:49:30,484 - 1:21:15 - 172.9s - INFO - __main__ - progress 3.624 , lr 5.1E-05 , loss 0.421 , qa loss 0.421 , lm loss 0.000 , avg batch size 4.0
2023-06-08 12:51:13,660 - 1:22:58 - 103.2s - INFO - __main__ - epoch 4/20 done , tot steps 6416 , lr 5.0E-05 , loss 0.42 , qa loss 0.42 , lm loss 0.00 , avg batch size 4.0
2023-06-08 12:54:05,543 - 1:25:50 - 171.9s - INFO - __main__ - progress 4.624 , lr 4.8E-05 , loss 0.359 , qa loss 0.359 , lm loss 0.000 , avg batch size 4.0
2023-06-08 12:55:50,679 - 1:27:35 - 105.1s - INFO - __main__ - epoch 5/20 done , tot steps 8020 , lr 4.7E-05 , loss 0.35 , qa loss 0.35 , lm loss 0.00 , avg batch size 4.0
2023-06-08 12:58:41,855 - 1:30:27 - 171.2s - INFO - __main__ - progress 5.624 , lr 4.5E-05 , loss 0.294 , qa loss 0.294 , lm loss 0.000 , avg batch size 4.0
2023-06-08 13:00:23,109 - 1:32:08 - 101.3s - INFO - __main__ - epoch 6/20 done , tot steps 9624 , lr 4.4E-05 , loss 0.30 , qa loss 0.30 , lm loss 0.00 , avg batch size 4.0
2023-06-08 13:03:16,577 - 1:35:01 - 173.5s - INFO - __main__ - progress 6.624 , lr 4.2E-05 , loss 0.259 , qa loss 0.259 , lm loss 0.000 , avg batch size 4.0
2023-06-08 13:04:59,035 - 1:36:44 - 102.5s - INFO - __main__ - epoch 7/20 done , tot steps 11228 , lr 4.1E-05 , loss 0.27 , qa loss 0.27 , lm loss 0.00 , avg batch size 4.0
2023-06-08 13:07:51,163 - 1:39:36 - 172.1s - INFO - __main__ - progress 7.624 , lr 3.9E-05 , loss 0.219 , qa loss 0.219 , lm loss 0.000 , avg batch size 4.0
2023-06-08 13:09:35,583 - 1:41:20 - 104.4s - INFO - __main__ - epoch 8/20 done , tot steps 12832 , lr 3.8E-05 , loss 0.22 , qa loss 0.22 , lm loss 0.00 , avg batch size 4.0
2023-06-08 13:12:27,963 - 1:44:13 - 172.4s - INFO - __main__ - progress 8.624 , lr 3.6E-05 , loss 0.186 , qa loss 0.186 , lm loss 0.000 , avg batch size 4.0
2023-06-08 13:14:09,502 - 1:45:54 - 101.5s - INFO - __main__ - epoch 9/20 done , tot steps 14436 , lr 3.4E-05 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-06-08 13:16:58,885 - 1:48:44 - 169.4s - INFO - __main__ - progress 9.624 , lr 3.2E-05 , loss 0.171 , qa loss 0.171 , lm loss 0.000 , avg batch size 4.0
2023-06-08 13:18:43,685 - 1:50:28 - 104.8s - INFO - __main__ - epoch 10/20 done , tot steps 16040 , lr 3.1E-05 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
2023-06-08 13:21:38,371 - 1:53:23 - 174.7s - INFO - __main__ - progress 10.624 , lr 2.9E-05 , loss 0.150 , qa loss 0.150 , lm loss 0.000 , avg batch size 4.0
2023-06-08 13:23:21,607 - 1:55:06 - 103.2s - INFO - __main__ - epoch 11/20 done , tot steps 17644 , lr 2.8E-05 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-06-08 13:26:14,022 - 1:57:59 - 172.4s - INFO - __main__ - progress 11.624 , lr 2.6E-05 , loss 0.125 , qa loss 0.125 , lm loss 0.000 , avg batch size 4.0
2023-06-08 13:27:58,054 - 1:59:43 - 104.0s - INFO - __main__ - epoch 12/20 done , tot steps 19248 , lr 2.5E-05 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-06-08 13:30:49,760 - 2:02:34 - 171.7s - INFO - __main__ - progress 12.624 , lr 2.3E-05 , loss 0.113 , qa loss 0.113 , lm loss 0.000 , avg batch size 4.0
2023-06-08 13:32:33,524 - 2:04:18 - 103.8s - INFO - __main__ - epoch 13/20 done , tot steps 20852 , lr 2.2E-05 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-06-08 13:35:24,295 - 2:07:09 - 170.8s - INFO - __main__ - progress 13.624 , lr 2.0E-05 , loss 0.097 , qa loss 0.097 , lm loss 0.000 , avg batch size 4.0
2023-06-08 13:37:08,710 - 2:08:53 - 104.4s - INFO - __main__ - epoch 14/20 done , tot steps 22456 , lr 1.9E-05 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 4.0
2023-06-08 13:40:02,375 - 2:11:47 - 173.7s - INFO - __main__ - progress 14.624 , lr 1.7E-05 , loss 0.084 , qa loss 0.084 , lm loss 0.000 , avg batch size 4.0
2023-06-08 13:41:43,214 - 2:13:28 - 100.8s - INFO - __main__ - epoch 15/20 done , tot steps 24060 , lr 1.6E-05 , loss 0.08 , qa loss 0.08 , lm loss 0.00 , avg batch size 4.0
2023-06-08 13:44:40,002 - 2:16:25 - 176.8s - INFO - __main__ - progress 15.624 , lr 1.4E-05 , loss 0.073 , qa loss 0.073 , lm loss 0.000 , avg batch size 4.0
2023-06-08 13:46:20,638 - 2:18:05 - 100.6s - INFO - __main__ - epoch 16/20 done , tot steps 25664 , lr 1.3E-05 , loss 0.07 , qa loss 0.07 , lm loss 0.00 , avg batch size 4.0
2023-06-08 13:49:13,665 - 2:20:58 - 173.0s - INFO - __main__ - progress 16.624 , lr 1.1E-05 , loss 0.067 , qa loss 0.067 , lm loss 0.000 , avg batch size 4.0
2023-06-08 13:50:55,691 - 2:22:40 - 102.0s - INFO - __main__ - epoch 17/20 done , tot steps 27268 , lr 9.4E-06 , loss 0.07 , qa loss 0.07 , lm loss 0.00 , avg batch size 4.0
2023-06-08 13:53:46,277 - 2:25:31 - 170.6s - INFO - __main__ - progress 17.624 , lr 7.4E-06 , loss 0.065 , qa loss 0.065 , lm loss 0.000 , avg batch size 4.0
2023-06-08 13:55:30,541 - 2:27:15 - 104.3s - INFO - __main__ - epoch 18/20 done , tot steps 28872 , lr 6.3E-06 , loss 0.06 , qa loss 0.06 , lm loss 0.00 , avg batch size 4.0
2023-06-08 13:58:20,495 - 2:30:05 - 170.0s - INFO - __main__ - progress 18.624 , lr 4.3E-06 , loss 0.049 , qa loss 0.049 , lm loss 0.000 , avg batch size 4.0
2023-06-08 14:00:06,857 - 2:31:52 - 106.4s - INFO - __main__ - epoch 19/20 done , tot steps 30476 , lr 3.1E-06 , loss 0.05 , qa loss 0.05 , lm loss 0.00 , avg batch size 4.0
2023-06-08 14:03:02,156 - 2:34:47 - 175.3s - INFO - __main__ - progress 19.624 , lr 1.2E-06 , loss 0.046 , qa loss 0.046 , lm loss 0.000 , avg batch size 4.0
2023-06-08 14:04:43,373 - 2:36:28 - 101.2s - INFO - __main__ - epoch 20/20 done , tot steps 32080 , lr 1.6E-08 , loss 0.05 , qa loss 0.05 , lm loss 0.00 , avg batch size 4.0
2023-06-08 14:04:51,501 - 2:36:36 - 8.1s - INFO - __main__ - start to train { task: ['zre'], seq train type: lll }
2023-06-08 14:05:05,666 - 2:36:50 - 14.2s - INFO - utils - writing extra data in ../../model_am2/gpt2/lll/sst_srl_zre_woz.en_0.0/srl/lm.csv ...
2023-06-08 14:05:05,675 - 2:36:50 - 0.0s - INFO - __main__ - extra training data size: 0
2023-06-08 14:05:05,794 - 2:36:50 - 0.1s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[srl]
The task with which model is saved srl
2023-06-08 14:05:47,134 - 2:37:32 - 41.3s - INFO - __main__ - len of train dataset: 840000 , max train batch size 560 , num of opt steps: 16800000
2023-06-08 14:21:28,678 - 2:53:13 - 941.5s - INFO - __main__ - progress 0.189 , lr 6.2E-05 , loss 1.758 , qa loss 1.758 , lm loss 0.000 , avg batch size 159.1
2023-06-08 14:36:48,360 - 3:08:33 - 919.7s - INFO - __main__ - progress 0.380 , lr 6.1E-05 , loss 1.033 , qa loss 1.033 , lm loss 0.000 , avg batch size 159.7
2023-06-08 14:52:08,083 - 3:23:53 - 919.7s - INFO - __main__ - progress 0.571 , lr 6.1E-05 , loss 0.773 , qa loss 0.773 , lm loss 0.000 , avg batch size 159.8
2023-06-08 15:07:26,721 - 3:39:11 - 918.6s - INFO - __main__ - progress 0.760 , lr 6.0E-05 , loss 0.634 , qa loss 0.634 , lm loss 0.000 , avg batch size 159.7
2023-06-08 15:22:44,936 - 3:54:30 - 918.2s - INFO - __main__ - progress 0.952 , lr 6.0E-05 , loss 0.544 , qa loss 0.544 , lm loss 0.000 , avg batch size 160.0
2023-06-08 15:26:54,316 - 3:58:39 - 249.4s - INFO - __main__ - epoch 1/20 done , tot steps 5260 , lr 5.9E-05 , loss 0.53 , qa loss 0.53 , lm loss 0.00 , avg batch size 159.7
2023-06-08 15:42:21,785 - 4:14:06 - 927.5s - INFO - __main__ - progress 1.189 , lr 5.9E-05 , loss 0.161 , qa loss 0.161 , lm loss 0.000 , avg batch size 158.8
2023-06-08 15:57:43,741 - 4:29:28 - 922.0s - INFO - __main__ - progress 1.381 , lr 5.8E-05 , loss 0.155 , qa loss 0.155 , lm loss 0.000 , avg batch size 160.2
2023-06-08 16:13:06,729 - 4:44:51 - 923.0s - INFO - __main__ - progress 1.570 , lr 5.8E-05 , loss 0.148 , qa loss 0.148 , lm loss 0.000 , avg batch size 159.6
2023-06-08 16:28:26,355 - 5:00:11 - 919.6s - INFO - __main__ - progress 1.760 , lr 5.7E-05 , loss 0.144 , qa loss 0.144 , lm loss 0.000 , avg batch size 159.6
2023-06-08 16:43:47,151 - 5:15:32 - 920.8s - INFO - __main__ - progress 1.950 , lr 5.6E-05 , loss 0.139 , qa loss 0.139 , lm loss 0.000 , avg batch size 159.5
2023-06-08 16:47:50,430 - 5:19:35 - 243.3s - INFO - __main__ - epoch 2/20 done , tot steps 10519 , lr 5.6E-05 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 159.7
2023-06-08 17:03:25,970 - 5:35:11 - 935.5s - INFO - __main__ - progress 2.190 , lr 5.6E-05 , loss 0.107 , qa loss 0.107 , lm loss 0.000 , avg batch size 159.3
2023-06-08 17:18:46,799 - 5:50:31 - 920.8s - INFO - __main__ - progress 2.380 , lr 5.5E-05 , loss 0.106 , qa loss 0.106 , lm loss 0.000 , avg batch size 159.5
2023-06-08 17:34:07,805 - 6:05:53 - 921.0s - INFO - __main__ - progress 2.574 , lr 5.4E-05 , loss 0.103 , qa loss 0.103 , lm loss 0.000 , avg batch size 160.7
2023-06-08 17:49:27,938 - 6:21:13 - 920.1s - INFO - __main__ - progress 2.765 , lr 5.4E-05 , loss 0.101 , qa loss 0.101 , lm loss 0.000 , avg batch size 160.6
2023-06-08 18:04:48,342 - 6:36:33 - 920.4s - INFO - __main__ - progress 2.953 , lr 5.3E-05 , loss 0.099 , qa loss 0.099 , lm loss 0.000 , avg batch size 160.1
2023-06-08 18:08:39,442 - 6:40:24 - 231.1s - INFO - __main__ - epoch 3/20 done , tot steps 15766 , lr 5.3E-05 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 160.1
2023-06-08 18:24:12,371 - 6:55:57 - 932.9s - INFO - __main__ - progress 3.191 , lr 5.3E-05 , loss 0.083 , qa loss 0.083 , lm loss 0.000 , avg batch size 160.1
2023-06-08 18:39:35,571 - 7:11:20 - 923.2s - INFO - __main__ - progress 3.381 , lr 5.2E-05 , loss 0.081 , qa loss 0.081 , lm loss 0.000 , avg batch size 160.0
2023-06-08 18:54:51,953 - 7:26:37 - 916.4s - INFO - __main__ - progress 3.571 , lr 5.1E-05 , loss 0.079 , qa loss 0.079 , lm loss 0.000 , avg batch size 160.0
2023-06-08 19:10:09,582 - 7:41:54 - 917.6s - INFO - __main__ - progress 3.763 , lr 5.1E-05 , loss 0.078 , qa loss 0.078 , lm loss 0.000 , avg batch size 160.3
2023-06-08 19:25:30,406 - 7:57:15 - 920.8s - INFO - __main__ - progress 3.953 , lr 5.0E-05 , loss 0.077 , qa loss 0.077 , lm loss 0.000 , avg batch size 160.1
2023-06-08 19:29:24,839 - 8:01:10 - 234.4s - INFO - __main__ - epoch 4/20 done , tot steps 21017 , lr 5.0E-05 , loss 0.08 , qa loss 0.08 , lm loss 0.00 , avg batch size 160.0
2023-06-08 19:44:54,138 - 8:16:39 - 929.3s - INFO - __main__ - progress 4.193 , lr 4.9E-05 , loss 0.064 , qa loss 0.064 , lm loss 0.000 , avg batch size 161.7
2023-06-08 20:00:12,453 - 8:31:57 - 918.3s - INFO - __main__ - progress 4.383 , lr 4.9E-05 , loss 0.065 , qa loss 0.065 , lm loss 0.000 , avg batch size 160.9
2023-06-08 20:15:27,210 - 8:47:12 - 914.8s - INFO - __main__ - progress 4.573 , lr 4.8E-05 , loss 0.065 , qa loss 0.065 , lm loss 0.000 , avg batch size 160.4
2023-06-08 20:30:45,573 - 9:02:30 - 918.4s - INFO - __main__ - progress 4.763 , lr 4.8E-05 , loss 0.064 , qa loss 0.064 , lm loss 0.000 , avg batch size 160.2
2023-06-08 20:46:04,808 - 9:17:50 - 919.2s - INFO - __main__ - progress 4.952 , lr 4.7E-05 , loss 0.063 , qa loss 0.063 , lm loss 0.000 , avg batch size 159.9
2023-06-08 20:50:04,892 - 9:21:50 - 240.1s - INFO - __main__ - epoch 5/20 done , tot steps 26275 , lr 4.7E-05 , loss 0.06 , qa loss 0.06 , lm loss 0.00 , avg batch size 159.8
2023-06-08 21:05:35,954 - 9:37:21 - 931.1s - INFO - __main__ - progress 5.190 , lr 4.6E-05 , loss 0.055 , qa loss 0.055 , lm loss 0.000 , avg batch size 159.9
2023-06-08 21:20:55,588 - 9:52:40 - 919.6s - INFO - __main__ - progress 5.381 , lr 4.6E-05 , loss 0.054 , qa loss 0.054 , lm loss 0.000 , avg batch size 160.1
2023-06-08 21:36:15,896 - 10:08:01 - 920.3s - INFO - __main__ - progress 5.572 , lr 4.5E-05 , loss 0.053 , qa loss 0.053 , lm loss 0.000 , avg batch size 160.2
2023-06-08 21:51:35,646 - 10:23:20 - 919.7s - INFO - __main__ - progress 5.763 , lr 4.5E-05 , loss 0.053 , qa loss 0.053 , lm loss 0.000 , avg batch size 160.1
2023-06-08 22:06:52,429 - 10:38:37 - 916.8s - INFO - __main__ - progress 5.953 , lr 4.4E-05 , loss 0.053 , qa loss 0.053 , lm loss 0.000 , avg batch size 160.1
2023-06-08 22:10:45,475 - 10:42:30 - 233.0s - INFO - __main__ - epoch 6/20 done , tot steps 31525 , lr 4.4E-05 , loss 0.05 , qa loss 0.05 , lm loss 0.00 , avg batch size 160.0
2023-06-08 22:26:16,397 - 10:58:01 - 930.9s - INFO - __main__ - progress 6.190 , lr 4.3E-05 , loss 0.045 , qa loss 0.045 , lm loss 0.000 , avg batch size 159.6
2023-06-08 22:41:35,797 - 11:13:20 - 919.4s - INFO - __main__ - progress 6.380 , lr 4.3E-05 , loss 0.045 , qa loss 0.045 , lm loss 0.000 , avg batch size 159.6
2023-06-08 22:56:56,052 - 11:28:41 - 920.3s - INFO - __main__ - progress 6.572 , lr 4.2E-05 , loss 0.045 , qa loss 0.045 , lm loss 0.000 , avg batch size 160.1
2023-06-08 23:12:16,050 - 11:44:01 - 920.0s - INFO - __main__ - progress 6.763 , lr 4.1E-05 , loss 0.045 , qa loss 0.045 , lm loss 0.000 , avg batch size 160.2
2023-06-08 23:27:31,852 - 11:59:17 - 915.8s - INFO - __main__ - progress 6.953 , lr 4.1E-05 , loss 0.045 , qa loss 0.045 , lm loss 0.000 , avg batch size 160.1
2023-06-08 23:31:24,350 - 12:03:09 - 232.5s - INFO - __main__ - epoch 7/20 done , tot steps 36773 , lr 4.1E-05 , loss 0.05 , qa loss 0.05 , lm loss 0.00 , avg batch size 160.1
2023-06-08 23:46:53,437 - 12:18:38 - 929.1s - INFO - __main__ - progress 7.188 , lr 4.0E-05 , loss 0.039 , qa loss 0.039 , lm loss 0.000 , avg batch size 157.9
2023-06-09 00:02:12,283 - 12:33:57 - 918.8s - INFO - __main__ - progress 7.379 , lr 3.9E-05 , loss 0.039 , qa loss 0.039 , lm loss 0.000 , avg batch size 159.0
2023-06-09 00:17:30,684 - 12:49:15 - 918.4s - INFO - __main__ - progress 7.570 , lr 3.9E-05 , loss 0.040 , qa loss 0.040 , lm loss 0.000 , avg batch size 159.5
2023-06-09 00:32:50,272 - 13:04:35 - 919.6s - INFO - __main__ - progress 7.761 , lr 3.8E-05 , loss 0.040 , qa loss 0.040 , lm loss 0.000 , avg batch size 159.8
2023-06-09 00:48:08,106 - 13:19:53 - 917.8s - INFO - __main__ - progress 7.952 , lr 3.8E-05 , loss 0.039 , qa loss 0.039 , lm loss 0.000 , avg batch size 160.0
2023-06-09 00:52:07,291 - 13:23:52 - 239.2s - INFO - __main__ - epoch 8/20 done , tot steps 42030 , lr 3.8E-05 , loss 0.04 , qa loss 0.04 , lm loss 0.00 , avg batch size 159.8
2023-06-09 01:07:37,867 - 13:39:23 - 930.6s - INFO - __main__ - progress 8.191 , lr 3.7E-05 , loss 0.034 , qa loss 0.034 , lm loss 0.000 , avg batch size 160.1
2023-06-09 01:22:57,258 - 13:54:42 - 919.4s - INFO - __main__ - progress 8.383 , lr 3.6E-05 , loss 0.035 , qa loss 0.035 , lm loss 0.000 , avg batch size 160.9
2023-06-09 01:38:13,899 - 14:09:59 - 916.6s - INFO - __main__ - progress 8.573 , lr 3.6E-05 , loss 0.035 , qa loss 0.035 , lm loss 0.000 , avg batch size 160.6
2023-06-09 01:53:32,879 - 14:25:18 - 919.0s - INFO - __main__ - progress 8.761 , lr 3.5E-05 , loss 0.035 , qa loss 0.035 , lm loss 0.000 , avg batch size 159.9
2023-06-09 02:08:52,510 - 14:40:37 - 919.6s - INFO - __main__ - progress 8.951 , lr 3.5E-05 , loss 0.035 , qa loss 0.035 , lm loss 0.000 , avg batch size 159.8
2023-06-09 02:12:53,642 - 14:44:38 - 241.1s - INFO - __main__ - epoch 9/20 done , tot steps 47289 , lr 3.4E-05 , loss 0.03 , qa loss 0.03 , lm loss 0.00 , avg batch size 159.7
2023-06-09 02:28:24,517 - 15:00:09 - 930.9s - INFO - __main__ - progress 9.190 , lr 3.4E-05 , loss 0.031 , qa loss 0.031 , lm loss 0.000 , avg batch size 159.5
2023-06-09 02:43:41,937 - 15:15:27 - 917.4s - INFO - __main__ - progress 9.380 , lr 3.3E-05 , loss 0.031 , qa loss 0.031 , lm loss 0.000 , avg batch size 159.7
2023-06-09 02:59:00,579 - 15:30:45 - 918.6s - INFO - __main__ - progress 9.569 , lr 3.3E-05 , loss 0.031 , qa loss 0.031 , lm loss 0.000 , avg batch size 159.3
2023-06-09 03:14:19,742 - 15:46:04 - 919.2s - INFO - __main__ - progress 9.761 , lr 3.2E-05 , loss 0.031 , qa loss 0.031 , lm loss 0.000 , avg batch size 159.8
2023-06-09 03:29:38,988 - 16:01:24 - 919.2s - INFO - __main__ - progress 9.952 , lr 3.1E-05 , loss 0.031 , qa loss 0.031 , lm loss 0.000 , avg batch size 159.9
2023-06-09 03:33:36,145 - 16:05:21 - 237.2s - INFO - __main__ - epoch 10/20 done , tot steps 52544 , lr 3.1E-05 , loss 0.03 , qa loss 0.03 , lm loss 0.00 , avg batch size 159.8
2023-06-09 03:49:07,128 - 16:20:52 - 931.0s - INFO - __main__ - progress 10.192 , lr 3.1E-05 , loss 0.027 , qa loss 0.027 , lm loss 0.000 , avg batch size 161.2
2023-06-09 04:04:25,811 - 16:36:11 - 918.7s - INFO - __main__ - progress 10.383 , lr 3.0E-05 , loss 0.027 , qa loss 0.027 , lm loss 0.000 , avg batch size 160.7
2023-06-09 04:19:42,246 - 16:51:27 - 916.4s - INFO - __main__ - progress 10.574 , lr 2.9E-05 , loss 0.027 , qa loss 0.027 , lm loss 0.000 , avg batch size 160.9
2023-06-09 04:35:00,910 - 17:06:46 - 918.7s - INFO - __main__ - progress 10.763 , lr 2.9E-05 , loss 0.027 , qa loss 0.027 , lm loss 0.000 , avg batch size 160.2
2023-06-09 04:50:20,364 - 17:22:05 - 919.5s - INFO - __main__ - progress 10.951 , lr 2.8E-05 , loss 0.027 , qa loss 0.027 , lm loss 0.000 , avg batch size 159.8
2023-06-09 04:54:19,664 - 17:26:04 - 239.3s - INFO - __main__ - epoch 11/20 done , tot steps 57801 , lr 2.8E-05 , loss 0.03 , qa loss 0.03 , lm loss 0.00 , avg batch size 159.8
2023-06-09 05:09:50,080 - 17:41:35 - 930.4s - INFO - __main__ - progress 11.191 , lr 2.8E-05 , loss 0.024 , qa loss 0.024 , lm loss 0.000 , avg batch size 160.2
2023-06-09 05:25:08,550 - 17:56:53 - 918.5s - INFO - __main__ - progress 11.379 , lr 2.7E-05 , loss 0.025 , qa loss 0.025 , lm loss 0.000 , avg batch size 159.2
2023-06-09 05:40:26,457 - 18:12:11 - 917.9s - INFO - __main__ - progress 11.569 , lr 2.6E-05 , loss 0.025 , qa loss 0.025 , lm loss 0.000 , avg batch size 159.4
2023-06-09 05:55:43,922 - 18:27:29 - 917.5s - INFO - __main__ - progress 11.760 , lr 2.6E-05 , loss 0.025 , qa loss 0.025 , lm loss 0.000 , avg batch size 159.6
2023-06-09 06:11:03,294 - 18:42:48 - 919.4s - INFO - __main__ - progress 11.952 , lr 2.5E-05 , loss 0.025 , qa loss 0.025 , lm loss 0.000 , avg batch size 160.0
2023-06-09 06:15:00,111 - 18:46:45 - 236.8s - INFO - __main__ - epoch 12/20 done , tot steps 63055 , lr 2.5E-05 , loss 0.02 , qa loss 0.02 , lm loss 0.00 , avg batch size 159.9
2023-06-09 06:30:29,919 - 19:02:15 - 929.8s - INFO - __main__ - progress 12.189 , lr 2.4E-05 , loss 0.022 , qa loss 0.022 , lm loss 0.000 , avg batch size 159.0
2023-06-09 06:45:48,387 - 19:17:33 - 918.5s - INFO - __main__ - progress 12.379 , lr 2.4E-05 , loss 0.022 , qa loss 0.022 , lm loss 0.000 , avg batch size 159.1
2023-06-09 07:01:07,856 - 19:32:53 - 919.5s - INFO - __main__ - progress 12.569 , lr 2.3E-05 , loss 0.022 , qa loss 0.022 , lm loss 0.000 , avg batch size 159.4
2023-06-09 07:16:27,402 - 19:48:12 - 919.5s - INFO - __main__ - progress 12.760 , lr 2.3E-05 , loss 0.022 , qa loss 0.022 , lm loss 0.000 , avg batch size 159.5
2023-06-09 07:31:47,015 - 20:03:32 - 919.6s - INFO - __main__ - progress 12.950 , lr 2.2E-05 , loss 0.022 , qa loss 0.022 , lm loss 0.000 , avg batch size 159.6
2023-06-09 07:35:51,947 - 20:07:37 - 244.9s - INFO - __main__ - epoch 13/20 done , tot steps 68317 , lr 2.2E-05 , loss 0.02 , qa loss 0.02 , lm loss 0.00 , avg batch size 159.6
2023-06-09 07:51:20,473 - 20:23:05 - 928.5s - INFO - __main__ - progress 13.191 , lr 2.1E-05 , loss 0.020 , qa loss 0.020 , lm loss 0.000 , avg batch size 160.2
2023-06-09 08:06:39,860 - 20:38:25 - 919.4s - INFO - __main__ - progress 13.380 , lr 2.1E-05 , loss 0.020 , qa loss 0.020 , lm loss 0.000 , avg batch size 159.6
2023-06-09 08:21:59,148 - 20:53:44 - 919.3s - INFO - __main__ - progress 13.569 , lr 2.0E-05 , loss 0.020 , qa loss 0.020 , lm loss 0.000 , avg batch size 159.4
2023-06-09 08:37:18,569 - 21:09:03 - 919.4s - INFO - __main__ - progress 13.760 , lr 2.0E-05 , loss 0.020 , qa loss 0.020 , lm loss 0.000 , avg batch size 159.7
2023-06-09 08:52:34,537 - 21:24:19 - 916.0s - INFO - __main__ - progress 13.950 , lr 1.9E-05 , loss 0.020 , qa loss 0.020 , lm loss 0.000 , avg batch size 159.5
2023-06-09 08:56:38,744 - 21:28:23 - 244.2s - INFO - __main__ - epoch 14/20 done , tot steps 73579 , lr 1.9E-05 , loss 0.02 , qa loss 0.02 , lm loss 0.00 , avg batch size 159.6
2023-06-09 09:12:08,036 - 21:43:53 - 929.3s - INFO - __main__ - progress 14.190 , lr 1.8E-05 , loss 0.018 , qa loss 0.018 , lm loss 0.000 , avg batch size 159.9
2023-06-09 09:27:26,096 - 21:59:11 - 918.1s - INFO - __main__ - progress 14.381 , lr 1.8E-05 , loss 0.018 , qa loss 0.018 , lm loss 0.000 , avg batch size 160.1
2023-06-09 09:42:41,897 - 22:14:27 - 915.8s - INFO - __main__ - progress 14.571 , lr 1.7E-05 , loss 0.018 , qa loss 0.018 , lm loss 0.000 , avg batch size 159.8
2023-06-09 09:58:00,017 - 22:29:45 - 918.1s - INFO - __main__ - progress 14.760 , lr 1.6E-05 , loss 0.018 , qa loss 0.018 , lm loss 0.000 , avg batch size 159.6
2023-06-09 10:13:18,371 - 22:45:03 - 918.4s - INFO - __main__ - progress 14.951 , lr 1.6E-05 , loss 0.018 , qa loss 0.018 , lm loss 0.000 , avg batch size 159.8
2023-06-09 10:17:17,439 - 22:49:02 - 239.1s - INFO - __main__ - epoch 15/20 done , tot steps 78836 , lr 1.6E-05 , loss 0.02 , qa loss 0.02 , lm loss 0.00 , avg batch size 159.8
2023-06-09 10:32:48,330 - 23:04:33 - 930.9s - INFO - __main__ - progress 15.191 , lr 1.5E-05 , loss 0.016 , qa loss 0.016 , lm loss 0.000 , avg batch size 160.9
2023-06-09 10:48:05,964 - 23:19:51 - 917.6s - INFO - __main__ - progress 15.382 , lr 1.4E-05 , loss 0.016 , qa loss 0.016 , lm loss 0.000 , avg batch size 160.4
2023-06-09 11:03:23,919 - 23:35:09 - 918.0s - INFO - __main__ - progress 15.571 , lr 1.4E-05 , loss 0.016 , qa loss 0.016 , lm loss 0.000 , avg batch size 159.9
2023-06-09 11:18:40,828 - 23:50:26 - 916.9s - INFO - __main__ - progress 15.762 , lr 1.3E-05 , loss 0.016 , qa loss 0.016 , lm loss 0.000 , avg batch size 160.1
2023-06-09 11:33:59,946 - 1 day, 0:05:45 - 919.1s - INFO - __main__ - progress 15.953 , lr 1.3E-05 , loss 0.016 , qa loss 0.016 , lm loss 0.000 , avg batch size 160.2
2023-06-09 11:37:50,953 - 1 day, 0:09:36 - 231.0s - INFO - __main__ - epoch 16/20 done , tot steps 84084 , lr 1.3E-05 , loss 0.02 , qa loss 0.02 , lm loss 0.00 , avg batch size 160.1
2023-06-09 11:53:22,021 - 1 day, 0:25:07 - 931.1s - INFO - __main__ - progress 16.190 , lr 1.2E-05 , loss 0.015 , qa loss 0.015 , lm loss 0.000 , avg batch size 159.5
2023-06-09 12:08:41,825 - 1 day, 0:40:27 - 919.8s - INFO - __main__ - progress 16.381 , lr 1.1E-05 , loss 0.015 , qa loss 0.015 , lm loss 0.000 , avg batch size 160.1
2023-06-09 12:24:01,661 - 1 day, 0:55:46 - 919.8s - INFO - __main__ - progress 16.573 , lr 1.1E-05 , loss 0.015 , qa loss 0.015 , lm loss 0.000 , avg batch size 160.4
2023-06-09 12:39:22,045 - 1 day, 1:11:07 - 920.4s - INFO - __main__ - progress 16.764 , lr 1.0E-05 , loss 0.015 , qa loss 0.015 , lm loss 0.000 , avg batch size 160.4
2023-06-09 12:54:38,509 - 1 day, 1:26:23 - 916.5s - INFO - __main__ - progress 16.953 , lr 9.5E-06 , loss 0.015 , qa loss 0.015 , lm loss 0.000 , avg batch size 160.1
2023-06-09 12:58:35,272 - 1 day, 1:30:20 - 236.8s - INFO - __main__ - epoch 17/20 done , tot steps 89338 , lr 9.4E-06 , loss 0.02 , qa loss 0.02 , lm loss 0.00 , avg batch size 159.9
2023-06-09 13:14:06,257 - 1 day, 1:45:51 - 931.0s - INFO - __main__ - progress 17.191 , lr 8.8E-06 , loss 0.014 , qa loss 0.014 , lm loss 0.000 , avg batch size 160.5
2023-06-09 13:29:24,954 - 1 day, 2:01:10 - 918.7s - INFO - __main__ - progress 17.382 , lr 8.2E-06 , loss 0.014 , qa loss 0.014 , lm loss 0.000 , avg batch size 160.5
2023-06-09 13:44:43,872 - 1 day, 2:16:29 - 918.9s - INFO - __main__ - progress 17.570 , lr 7.6E-06 , loss 0.014 , qa loss 0.014 , lm loss 0.000 , avg batch size 159.6
2023-06-09 14:00:03,733 - 1 day, 2:31:48 - 919.9s - INFO - __main__ - progress 17.760 , lr 7.0E-06 , loss 0.014 , qa loss 0.014 , lm loss 0.000 , avg batch size 159.6
2023-06-09 14:15:23,359 - 1 day, 2:47:08 - 919.6s - INFO - __main__ - progress 17.951 , lr 6.4E-06 , loss 0.014 , qa loss 0.014 , lm loss 0.000 , avg batch size 159.8
2023-06-09 14:19:20,547 - 1 day, 2:51:05 - 237.2s - INFO - __main__ - epoch 18/20 done , tot steps 94592 , lr 6.3E-06 , loss 0.01 , qa loss 0.01 , lm loss 0.00 , avg batch size 159.9
2023-06-09 14:34:49,798 - 1 day, 3:06:34 - 929.3s - INFO - __main__ - progress 18.192 , lr 5.7E-06 , loss 0.013 , qa loss 0.013 , lm loss 0.000 , avg batch size 160.9
2023-06-09 14:50:08,021 - 1 day, 3:21:53 - 918.2s - INFO - __main__ - progress 18.382 , lr 5.1E-06 , loss 0.013 , qa loss 0.013 , lm loss 0.000 , avg batch size 160.5
2023-06-09 15:05:26,594 - 1 day, 3:37:11 - 918.6s - INFO - __main__ - progress 18.573 , lr 4.5E-06 , loss 0.013 , qa loss 0.013 , lm loss 0.000 , avg batch size 160.5
2023-06-09 15:20:44,389 - 1 day, 3:52:29 - 917.8s - INFO - __main__ - progress 18.761 , lr 3.9E-06 , loss 0.013 , qa loss 0.013 , lm loss 0.000 , avg batch size 159.8
2023-06-09 15:36:02,650 - 1 day, 4:07:47 - 918.3s - INFO - __main__ - progress 18.951 , lr 3.3E-06 , loss 0.013 , qa loss 0.013 , lm loss 0.000 , avg batch size 159.7
2023-06-09 15:40:01,225 - 1 day, 4:11:46 - 238.6s - INFO - __main__ - epoch 19/20 done , tot steps 99847 , lr 3.1E-06 , loss 0.01 , qa loss 0.01 , lm loss 0.00 , avg batch size 159.8
2023-06-09 15:55:32,008 - 1 day, 4:27:17 - 930.8s - INFO - __main__ - progress 19.189 , lr 2.6E-06 , loss 0.012 , qa loss 0.012 , lm loss 0.000 , avg batch size 158.8
2023-06-09 16:10:48,927 - 1 day, 4:42:34 - 916.9s - INFO - __main__ - progress 19.380 , lr 2.0E-06 , loss 0.012 , qa loss 0.012 , lm loss 0.000 , avg batch size 159.6
2023-06-09 16:26:07,542 - 1 day, 4:57:52 - 918.6s - INFO - __main__ - progress 19.571 , lr 1.4E-06 , loss 0.012 , qa loss 0.012 , lm loss 0.000 , avg batch size 159.9
2023-06-09 16:41:26,323 - 1 day, 5:13:11 - 918.8s - INFO - __main__ - progress 19.762 , lr 7.6E-07 , loss 0.012 , qa loss 0.012 , lm loss 0.000 , avg batch size 160.0
2023-06-09 16:56:44,651 - 1 day, 5:28:29 - 918.3s - INFO - __main__ - progress 19.951 , lr 1.7E-07 , loss 0.012 , qa loss 0.012 , lm loss 0.000 , avg batch size 159.8
2023-06-09 17:00:42,385 - 1 day, 5:32:27 - 237.7s - INFO - __main__ - epoch 20/20 done , tot steps 105102 , lr 1.6E-08 , loss 0.01 , qa loss 0.01 , lm loss 0.00 , avg batch size 159.8
2023-06-09 17:00:53,479 - 1 day, 5:32:38 - 11.1s - INFO - __main__ - start to train { task: ['woz.en'], seq train type: lll }
2023-06-09 17:00:56,385 - 1 day, 5:32:41 - 2.9s - INFO - utils - writing extra data in ../../model_am2/gpt2/lll/sst_srl_zre_woz.en_0.0/zre/lm.csv ...
2023-06-09 17:00:56,386 - 1 day, 5:32:41 - 0.0s - INFO - __main__ - extra training data size: 0
2023-06-09 17:00:56,503 - 1 day, 5:32:41 - 0.1s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[zre]
The task with which model is saved zre
2023-06-09 17:01:02,477 - 1 day, 5:32:47 - 6.0s - INFO - __main__ - len of train dataset: 2536 , max train batch size 4 , num of opt steps: 50720
2023-06-09 17:02:22,739 - 1 day, 5:34:07 - 80.3s - INFO - __main__ - epoch 1/20 done , tot steps 634 , lr 5.9E-05 , loss 1.95 , qa loss 1.95 , lm loss 0.00 , avg batch size 4.0
2023-06-09 17:03:37,077 - 1 day, 5:35:22 - 74.3s - INFO - __main__ - epoch 2/20 done , tot steps 1268 , lr 5.6E-05 , loss 0.36 , qa loss 0.36 , lm loss 0.00 , avg batch size 4.0
2023-06-09 17:04:51,618 - 1 day, 5:36:36 - 74.5s - INFO - __main__ - epoch 3/20 done , tot steps 1902 , lr 5.3E-05 , loss 0.25 , qa loss 0.25 , lm loss 0.00 , avg batch size 4.0
2023-06-09 17:06:08,689 - 1 day, 5:37:53 - 77.1s - INFO - __main__ - epoch 4/20 done , tot steps 2536 , lr 5.0E-05 , loss 0.21 , qa loss 0.21 , lm loss 0.00 , avg batch size 4.0
2023-06-09 17:07:23,422 - 1 day, 5:39:08 - 74.7s - INFO - __main__ - epoch 5/20 done , tot steps 3170 , lr 4.7E-05 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-06-09 17:08:38,492 - 1 day, 5:40:23 - 75.1s - INFO - __main__ - epoch 6/20 done , tot steps 3804 , lr 4.4E-05 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-06-09 17:09:53,496 - 1 day, 5:41:38 - 75.0s - INFO - __main__ - epoch 7/20 done , tot steps 4438 , lr 4.1E-05 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-06-09 17:11:08,757 - 1 day, 5:42:53 - 75.3s - INFO - __main__ - epoch 8/20 done , tot steps 5072 , lr 3.8E-05 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-06-09 17:12:23,612 - 1 day, 5:44:08 - 74.9s - INFO - __main__ - epoch 9/20 done , tot steps 5706 , lr 3.4E-05 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2023-06-09 17:13:38,024 - 1 day, 5:45:23 - 74.4s - INFO - __main__ - epoch 10/20 done , tot steps 6340 , lr 3.1E-05 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2023-06-09 17:14:52,843 - 1 day, 5:46:38 - 74.8s - INFO - __main__ - epoch 11/20 done , tot steps 6974 , lr 2.8E-05 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 4.0
2023-06-09 17:16:07,416 - 1 day, 5:47:52 - 74.6s - INFO - __main__ - epoch 12/20 done , tot steps 7608 , lr 2.5E-05 , loss 0.09 , qa loss 0.09 , lm loss 0.00 , avg batch size 4.0
2023-06-09 17:17:21,727 - 1 day, 5:49:06 - 74.3s - INFO - __main__ - epoch 13/20 done , tot steps 8242 , lr 2.2E-05 , loss 0.08 , qa loss 0.08 , lm loss 0.00 , avg batch size 4.0
2023-06-09 17:18:37,842 - 1 day, 5:50:23 - 76.1s - INFO - __main__ - epoch 14/20 done , tot steps 8876 , lr 1.9E-05 , loss 0.08 , qa loss 0.08 , lm loss 0.00 , avg batch size 4.0
2023-06-09 17:19:52,867 - 1 day, 5:51:38 - 75.0s - INFO - __main__ - epoch 15/20 done , tot steps 9510 , lr 1.6E-05 , loss 0.07 , qa loss 0.07 , lm loss 0.00 , avg batch size 4.0
2023-06-09 17:21:09,103 - 1 day, 5:52:54 - 76.2s - INFO - __main__ - epoch 16/20 done , tot steps 10144 , lr 1.3E-05 , loss 0.07 , qa loss 0.07 , lm loss 0.00 , avg batch size 4.0
2023-06-09 17:22:23,434 - 1 day, 5:54:08 - 74.3s - INFO - __main__ - epoch 17/20 done , tot steps 10778 , lr 9.4E-06 , loss 0.06 , qa loss 0.06 , lm loss 0.00 , avg batch size 4.0
2023-06-09 17:23:38,301 - 1 day, 5:55:23 - 74.9s - INFO - __main__ - epoch 18/20 done , tot steps 11412 , lr 6.3E-06 , loss 0.06 , qa loss 0.06 , lm loss 0.00 , avg batch size 4.0
2023-06-09 17:24:51,999 - 1 day, 5:56:37 - 73.7s - INFO - __main__ - epoch 19/20 done , tot steps 12046 , lr 3.1E-06 , loss 0.06 , qa loss 0.06 , lm loss 0.00 , avg batch size 4.0
2023-06-09 17:26:06,991 - 1 day, 5:57:52 - 75.0s - INFO - __main__ - epoch 20/20 done , tot steps 12680 , lr 1.5E-08 , loss 0.05 , qa loss 0.05 , lm loss 0.00 , avg batch size 4.0
The current active adapter is Stack[woz_en]
The task with which model is saved woz_en
Wall Execution time: 05:57:45
CPU Execution time: 16:34:24
