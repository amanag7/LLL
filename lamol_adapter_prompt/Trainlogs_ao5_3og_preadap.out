Not all gpus support fp16 training! Will use fp32 instead.
2023-07-03 18:28:06,712 - 0:00:04 - 0.0s - INFO - __main__ - args = Namespace(REG_TYPE_KEYS=['mas', 'ewc'], adam_epsilon=0.0001, add_task_tokens=False, data_dir='../../data', debug=False, decay_style='linear', device_ids=[2], dynamic_epochs=False, fp32=True, gen_lm_sample_percentage=0.0, learning_rate=6.25e-05, lm_lambda=0.0, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[40537.0], min_batch_size=4, min_n_steps=1500, model_dir_root='../../model_ao5/gpt2/lll/srl_sst_woz.en_0.0', model_name='gpt2', n_gpus=1, n_train_epochs={'srl': 8, 'sst': 8, 'woz.en': 8}, n_warmup_ratio=0.005, n_workers=75, qp_margin=0.5, real_sample=False, reg_lambda=1.0, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['srl', 'sst', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[14187], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[14187], unbound=0, use_sep=False, weight_decay=0.01)
2023-07-03 18:28:06,712 - 0:00:04 - 0.0s - INFO - __main__ - start to train { task: ['srl'], seq train type: lll }
2023-07-03 18:28:06,720 - 0:00:04 - 0.0s - INFO - __main__ - extra training data size: 0
2023-07-03 18:28:10,213 - 0:00:08 - 3.5s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
2023-07-03 18:28:12,132 - 0:00:10 - 1.9s - INFO - __main__ - len of train dataset: 6414 , max train batch size 4 , num of opt steps: 51312
/home/student/2021/cs21mtech11006/anaconda3/envs/lamol/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
2023-07-03 18:30:34,371 - 0:02:32 - 142.2s - INFO - __main__ - progress 0.624 , lr 5.8E-05 , loss 3.611 , qa loss 3.611 , lm loss 0.000 , avg batch size 4.0
2023-07-03 18:32:01,976 - 0:04:00 - 87.6s - INFO - __main__ - epoch 1/8 done , tot steps 1604 , lr 5.5E-05 , loss 2.60 , qa loss 2.60 , lm loss 0.00 , avg batch size 4.0
2023-07-03 18:34:21,409 - 0:06:19 - 139.4s - INFO - __main__ - progress 1.624 , lr 5.0E-05 , loss 0.782 , qa loss 0.782 , lm loss 0.000 , avg batch size 4.0
2023-07-03 18:35:54,917 - 0:07:53 - 93.5s - INFO - __main__ - epoch 2/8 done , tot steps 3208 , lr 4.7E-05 , loss 0.76 , qa loss 0.76 , lm loss 0.00 , avg batch size 4.0
2023-07-03 18:38:14,370 - 0:10:12 - 139.5s - INFO - __main__ - progress 2.624 , lr 4.2E-05 , loss 0.657 , qa loss 0.657 , lm loss 0.000 , avg batch size 4.0
2023-07-03 18:39:41,685 - 0:11:39 - 87.3s - INFO - __main__ - epoch 3/8 done , tot steps 4812 , lr 3.9E-05 , loss 0.65 , qa loss 0.65 , lm loss 0.00 , avg batch size 4.0
2023-07-03 18:42:03,393 - 0:14:01 - 141.7s - INFO - __main__ - progress 3.624 , lr 3.4E-05 , loss 0.579 , qa loss 0.579 , lm loss 0.000 , avg batch size 4.0
2023-07-03 18:43:29,854 - 0:15:28 - 86.5s - INFO - __main__ - epoch 4/8 done , tot steps 6416 , lr 3.1E-05 , loss 0.58 , qa loss 0.58 , lm loss 0.00 , avg batch size 4.0
2023-07-03 18:45:54,215 - 0:17:52 - 144.4s - INFO - __main__ - progress 4.624 , lr 2.6E-05 , loss 0.523 , qa loss 0.523 , lm loss 0.000 , avg batch size 4.0
2023-07-03 18:47:16,569 - 0:19:14 - 82.4s - INFO - __main__ - epoch 5/8 done , tot steps 8020 , lr 2.3E-05 , loss 0.53 , qa loss 0.53 , lm loss 0.00 , avg batch size 4.0
2023-07-03 18:49:37,935 - 0:21:36 - 141.4s - INFO - __main__ - progress 5.624 , lr 1.9E-05 , loss 0.492 , qa loss 0.492 , lm loss 0.000 , avg batch size 4.0
2023-07-03 18:51:02,089 - 0:23:00 - 84.2s - INFO - __main__ - epoch 6/8 done , tot steps 9624 , lr 1.6E-05 , loss 0.49 , qa loss 0.49 , lm loss 0.00 , avg batch size 4.0
2023-07-03 18:53:21,212 - 0:25:19 - 139.1s - INFO - __main__ - progress 6.624 , lr 1.1E-05 , loss 0.481 , qa loss 0.481 , lm loss 0.000 , avg batch size 4.0
2023-07-03 18:54:47,157 - 0:26:45 - 85.9s - INFO - __main__ - epoch 7/8 done , tot steps 11228 , lr 7.9E-06 , loss 0.48 , qa loss 0.48 , lm loss 0.00 , avg batch size 4.0
2023-07-03 18:57:04,347 - 0:29:02 - 137.2s - INFO - __main__ - progress 7.624 , lr 3.0E-06 , loss 0.452 , qa loss 0.452 , lm loss 0.000 , avg batch size 4.0
2023-07-03 18:58:39,004 - 0:30:37 - 94.7s - INFO - __main__ - epoch 8/8 done , tot steps 12832 , lr 3.9E-08 , loss 0.45 , qa loss 0.45 , lm loss 0.00 , avg batch size 4.0
2023-07-03 18:58:47,647 - 0:30:45 - 8.6s - INFO - __main__ - start to train { task: ['sst'], seq train type: lll }
2023-07-03 18:58:51,198 - 0:30:49 - 3.6s - INFO - utils - writing extra data in ../../model_ao5/gpt2/lll/srl_sst_woz.en_0.0/srl/lm.csv ...
2023-07-03 18:58:51,351 - 0:30:49 - 0.2s - INFO - __main__ - extra training data size: 0
2023-07-03 18:58:52,751 - 0:30:51 - 1.4s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[srl]
The task with which model is saved srl
2023-07-03 18:58:54,956 - 0:30:53 - 2.2s - INFO - __main__ - len of train dataset: 6920 , max train batch size 4 , num of opt steps: 55360
2023-07-03 19:00:12,380 - 0:32:10 - 77.4s - INFO - __main__ - progress 0.578 , lr 5.8E-05 , loss 2.821 , qa loss 2.821 , lm loss 0.000 , avg batch size 4.0
2023-07-03 19:01:09,609 - 0:33:07 - 57.2s - INFO - __main__ - epoch 1/8 done , tot steps 1730 , lr 5.5E-05 , loss 1.74 , qa loss 1.74 , lm loss 0.00 , avg batch size 4.0
2023-07-03 19:02:26,760 - 0:34:25 - 77.2s - INFO - __main__ - progress 1.578 , lr 5.0E-05 , loss 0.215 , qa loss 0.215 , lm loss 0.000 , avg batch size 4.0
2023-07-03 19:03:24,600 - 0:35:22 - 57.8s - INFO - __main__ - epoch 2/8 done , tot steps 3460 , lr 4.7E-05 , loss 0.21 , qa loss 0.21 , lm loss 0.00 , avg batch size 4.0
2023-07-03 19:04:42,044 - 0:36:40 - 77.4s - INFO - __main__ - progress 2.578 , lr 4.2E-05 , loss 0.203 , qa loss 0.203 , lm loss 0.000 , avg batch size 4.0
2023-07-03 19:05:39,663 - 0:37:37 - 57.6s - INFO - __main__ - epoch 3/8 done , tot steps 5190 , lr 3.9E-05 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-07-03 19:06:58,819 - 0:38:57 - 79.2s - INFO - __main__ - progress 3.578 , lr 3.5E-05 , loss 0.156 , qa loss 0.156 , lm loss 0.000 , avg batch size 4.0
2023-07-03 19:08:00,166 - 0:39:58 - 61.3s - INFO - __main__ - epoch 4/8 done , tot steps 6920 , lr 3.1E-05 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-07-03 19:09:18,777 - 0:41:17 - 78.6s - INFO - __main__ - progress 4.578 , lr 2.7E-05 , loss 0.149 , qa loss 0.149 , lm loss 0.000 , avg batch size 4.0
2023-07-03 19:10:15,855 - 0:42:14 - 57.1s - INFO - __main__ - epoch 5/8 done , tot steps 8650 , lr 2.3E-05 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
2023-07-03 19:11:33,979 - 0:43:32 - 78.1s - INFO - __main__ - progress 5.578 , lr 1.9E-05 , loss 0.131 , qa loss 0.131 , lm loss 0.000 , avg batch size 4.0
2023-07-03 19:12:31,082 - 0:44:29 - 57.1s - INFO - __main__ - epoch 6/8 done , tot steps 10380 , lr 1.6E-05 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
2023-07-03 19:13:49,572 - 0:45:47 - 78.5s - INFO - __main__ - progress 6.578 , lr 1.1E-05 , loss 0.134 , qa loss 0.134 , lm loss 0.000 , avg batch size 4.0
2023-07-03 19:14:46,632 - 0:46:44 - 57.1s - INFO - __main__ - epoch 7/8 done , tot steps 12110 , lr 7.9E-06 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-07-03 19:16:04,611 - 0:48:02 - 78.0s - INFO - __main__ - progress 7.578 , lr 3.3E-06 , loss 0.119 , qa loss 0.119 , lm loss 0.000 , avg batch size 4.0
2023-07-03 19:17:07,433 - 0:49:05 - 62.8s - INFO - __main__ - epoch 8/8 done , tot steps 13840 , lr 3.8E-08 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-07-03 19:17:16,369 - 0:49:14 - 8.9s - INFO - __main__ - start to train { task: ['woz.en'], seq train type: lll }
2023-07-03 19:17:20,574 - 0:49:18 - 4.2s - INFO - utils - writing extra data in ../../model_ao5/gpt2/lll/srl_sst_woz.en_0.0/sst/lm.csv ...
2023-07-03 19:17:20,602 - 0:49:18 - 0.0s - INFO - __main__ - extra training data size: 0
2023-07-03 19:17:20,943 - 0:49:19 - 0.3s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[sst]
The task with which model is saved sst
2023-07-03 19:17:23,665 - 0:49:21 - 2.7s - INFO - __main__ - len of train dataset: 2536 , max train batch size 4 , num of opt steps: 20288
2023-07-03 19:18:13,691 - 0:50:11 - 50.0s - INFO - __main__ - epoch 1/8 done , tot steps 634 , lr 5.5E-05 , loss 4.80 , qa loss 4.80 , lm loss 0.00 , avg batch size 4.0
2023-07-03 19:19:02,873 - 0:51:01 - 49.2s - INFO - __main__ - epoch 2/8 done , tot steps 1268 , lr 4.7E-05 , loss 0.49 , qa loss 0.49 , lm loss 0.00 , avg batch size 4.0
2023-07-03 19:19:52,088 - 0:51:50 - 49.2s - INFO - __main__ - epoch 3/8 done , tot steps 1902 , lr 3.9E-05 , loss 0.35 , qa loss 0.35 , lm loss 0.00 , avg batch size 4.0
2023-07-03 19:20:41,606 - 0:52:39 - 49.5s - INFO - __main__ - epoch 4/8 done , tot steps 2536 , lr 3.1E-05 , loss 0.29 , qa loss 0.29 , lm loss 0.00 , avg batch size 4.0
2023-07-03 19:21:31,035 - 0:53:29 - 49.4s - INFO - __main__ - epoch 5/8 done , tot steps 3170 , lr 2.3E-05 , loss 0.24 , qa loss 0.24 , lm loss 0.00 , avg batch size 4.0
2023-07-03 19:22:21,016 - 0:54:19 - 50.0s - INFO - __main__ - epoch 6/8 done , tot steps 3804 , lr 1.6E-05 , loss 0.22 , qa loss 0.22 , lm loss 0.00 , avg batch size 4.0
2023-07-03 19:23:10,727 - 0:55:08 - 49.7s - INFO - __main__ - epoch 7/8 done , tot steps 4438 , lr 7.8E-06 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-07-03 19:24:11,942 - 0:56:10 - 61.2s - INFO - __main__ - epoch 8/8 done , tot steps 5072 , lr 3.7E-08 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
The current active adapter is Stack[woz_en]
The task with which model is saved woz_en
Wall Execution time: 00:56:14
CPU Execution time: 00:57:53
