Not all gpus support fp16 training! Will use fp32 instead.
2023-06-29 23:40:36,329 - 0:00:06 - 0.0s - INFO - __main__ - args = Namespace(REG_TYPE_KEYS=['mas', 'ewc'], adam_epsilon=0.0001, add_task_tokens=False, data_dir='../../data', debug=False, decay_style='linear', device_ids=[0, 1], dynamic_epochs=False, fp32=True, gen_lm_sample_percentage=0.0, learning_rate=6.25e-05, lm_lambda=0.0, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[38915.52, 42158.48], min_batch_size=4, min_n_steps=1500, model_dir_root='../../model_aamm2/gpt2/lll/wikisql_ag_amazon_sst_srl_woz.en_0.0', model_name='gpt2', n_gpus=2, n_train_epochs={'wikisql': 20, 'ag': 20, 'amazon': 20, 'sst': 20, 'srl': 20, 'woz.en': 20}, n_warmup_ratio=0.005, n_workers=75, qp_margin=0.5, real_sample=False, reg_lambda=1.0, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['wikisql', 'ag', 'amazon', 'sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[13620, 14755], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[13620, 14755], unbound=0, use_sep=False, weight_decay=0.01)
2023-06-29 23:40:36,329 - 0:00:06 - 0.0s - INFO - __main__ - start to train { task: ['wikisql'], seq train type: lll }
2023-06-29 23:40:36,337 - 0:00:06 - 0.0s - INFO - __main__ - extra training data size: 0
2023-06-29 23:40:39,631 - 0:00:09 - 3.3s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
2023-06-29 23:43:55,639 - 0:03:25 - 196.0s - INFO - __main__ - len of train dataset: 56355 , max train batch size 37 , num of opt steps: 1127100
/home/student/2021/cs21mtech11006/anaconda3/envs/lamol/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/home/student/2021/cs21mtech11006/anaconda3/envs/lamol/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
2023-06-29 23:56:38,089 - 0:16:08 - 762.4s - INFO - __main__ - progress 0.657 , lr 6.0E-05 , loss 2.337 , qa loss 2.337 , lm loss 0.000 , avg batch size 37.0
2023-06-30 00:03:14,563 - 0:22:44 - 396.5s - INFO - __main__ - epoch 1/20 done , tot steps 1524 , lr 5.9E-05 , loss 1.66 , qa loss 1.66 , lm loss 0.00 , avg batch size 37.0
2023-06-30 00:15:52,252 - 0:35:22 - 757.7s - INFO - __main__ - progress 1.657 , lr 5.7E-05 , loss 0.290 , qa loss 0.290 , lm loss 0.000 , avg batch size 37.0
2023-06-30 00:22:27,254 - 0:41:57 - 395.0s - INFO - __main__ - epoch 2/20 done , tot steps 3048 , lr 5.6E-05 , loss 0.27 , qa loss 0.27 , lm loss 0.00 , avg batch size 37.0
2023-06-30 00:35:06,009 - 0:54:36 - 758.8s - INFO - __main__ - progress 2.656 , lr 5.4E-05 , loss 0.214 , qa loss 0.214 , lm loss 0.000 , avg batch size 37.0
2023-06-30 00:41:40,459 - 1:01:10 - 394.4s - INFO - __main__ - epoch 3/20 done , tot steps 4572 , lr 5.3E-05 , loss 0.21 , qa loss 0.21 , lm loss 0.00 , avg batch size 37.0
2023-06-30 00:54:17,416 - 1:13:47 - 757.0s - INFO - __main__ - progress 3.656 , lr 5.1E-05 , loss 0.187 , qa loss 0.187 , lm loss 0.000 , avg batch size 37.0
2023-06-30 01:00:51,729 - 1:20:21 - 394.3s - INFO - __main__ - epoch 4/20 done , tot steps 6096 , lr 5.0E-05 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 37.0
2023-06-30 01:13:28,906 - 1:32:59 - 757.2s - INFO - __main__ - progress 4.657 , lr 4.8E-05 , loss 0.166 , qa loss 0.166 , lm loss 0.000 , avg batch size 37.0
2023-06-30 01:20:04,507 - 1:39:34 - 395.6s - INFO - __main__ - epoch 5/20 done , tot steps 7620 , lr 4.7E-05 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 37.0
2023-06-30 01:32:41,253 - 1:52:11 - 756.7s - INFO - __main__ - progress 5.656 , lr 4.5E-05 , loss 0.150 , qa loss 0.150 , lm loss 0.000 , avg batch size 37.0
2023-06-30 01:39:17,551 - 1:58:47 - 396.3s - INFO - __main__ - epoch 6/20 done , tot steps 9144 , lr 4.4E-05 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 37.0
2023-06-30 01:51:57,757 - 2:11:27 - 760.2s - INFO - __main__ - progress 6.657 , lr 4.2E-05 , loss 0.144 , qa loss 0.144 , lm loss 0.000 , avg batch size 37.0
2023-06-30 01:58:32,227 - 2:18:02 - 394.5s - INFO - __main__ - epoch 7/20 done , tot steps 10668 , lr 4.1E-05 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 37.0
2023-06-30 02:11:10,766 - 2:30:40 - 758.5s - INFO - __main__ - progress 7.657 , lr 3.9E-05 , loss 0.134 , qa loss 0.134 , lm loss 0.000 , avg batch size 37.0
2023-06-30 02:17:45,087 - 2:37:15 - 394.3s - INFO - __main__ - epoch 8/20 done , tot steps 12192 , lr 3.8E-05 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 37.0
2023-06-30 02:30:22,621 - 2:49:52 - 757.5s - INFO - __main__ - progress 8.657 , lr 3.5E-05 , loss 0.126 , qa loss 0.126 , lm loss 0.000 , avg batch size 37.0
2023-06-30 02:36:57,100 - 2:56:27 - 394.5s - INFO - __main__ - epoch 9/20 done , tot steps 13716 , lr 3.4E-05 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 37.0
2023-06-30 02:49:35,655 - 3:09:05 - 758.6s - INFO - __main__ - progress 9.657 , lr 3.2E-05 , loss 0.120 , qa loss 0.120 , lm loss 0.000 , avg batch size 37.0
2023-06-30 02:56:09,859 - 3:15:39 - 394.2s - INFO - __main__ - epoch 10/20 done , tot steps 15240 , lr 3.1E-05 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 37.0
2023-06-30 03:08:47,518 - 3:28:17 - 757.7s - INFO - __main__ - progress 10.656 , lr 2.9E-05 , loss 0.120 , qa loss 0.120 , lm loss 0.000 , avg batch size 37.0
2023-06-30 03:15:20,970 - 3:34:51 - 393.5s - INFO - __main__ - epoch 11/20 done , tot steps 16764 , lr 2.8E-05 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 37.0
2023-06-30 03:27:59,734 - 3:47:29 - 758.8s - INFO - __main__ - progress 11.657 , lr 2.6E-05 , loss 0.113 , qa loss 0.113 , lm loss 0.000 , avg batch size 37.0
2023-06-30 03:34:34,642 - 3:54:04 - 394.9s - INFO - __main__ - epoch 12/20 done , tot steps 18288 , lr 2.5E-05 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 37.0
2023-06-30 03:47:14,527 - 4:06:44 - 759.9s - INFO - __main__ - progress 12.656 , lr 2.3E-05 , loss 0.111 , qa loss 0.111 , lm loss 0.000 , avg batch size 37.0
2023-06-30 03:53:48,674 - 4:13:18 - 394.1s - INFO - __main__ - epoch 13/20 done , tot steps 19812 , lr 2.2E-05 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 37.0
2023-06-30 04:06:28,186 - 4:25:58 - 759.5s - INFO - __main__ - progress 13.656 , lr 2.0E-05 , loss 0.106 , qa loss 0.106 , lm loss 0.000 , avg batch size 37.0
2023-06-30 04:13:02,649 - 4:32:32 - 394.5s - INFO - __main__ - epoch 14/20 done , tot steps 21336 , lr 1.9E-05 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 37.0
2023-06-30 04:25:40,764 - 4:45:10 - 758.1s - INFO - __main__ - progress 14.656 , lr 1.7E-05 , loss 0.106 , qa loss 0.106 , lm loss 0.000 , avg batch size 37.0
2023-06-30 04:32:14,578 - 4:51:44 - 393.8s - INFO - __main__ - epoch 15/20 done , tot steps 22860 , lr 1.6E-05 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 37.0
2023-06-30 04:44:52,912 - 5:04:23 - 758.3s - INFO - __main__ - progress 15.656 , lr 1.4E-05 , loss 0.101 , qa loss 0.101 , lm loss 0.000 , avg batch size 37.0
2023-06-30 04:51:27,431 - 5:10:57 - 394.5s - INFO - __main__ - epoch 16/20 done , tot steps 24384 , lr 1.3E-05 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 37.0
2023-06-30 05:04:04,586 - 5:23:34 - 757.2s - INFO - __main__ - progress 16.657 , lr 1.0E-05 , loss 0.101 , qa loss 0.101 , lm loss 0.000 , avg batch size 37.0
2023-06-30 05:10:40,088 - 5:30:10 - 395.5s - INFO - __main__ - epoch 17/20 done , tot steps 25908 , lr 9.4E-06 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 37.0
2023-06-30 05:23:17,911 - 5:42:48 - 757.8s - INFO - __main__ - progress 17.657 , lr 7.3E-06 , loss 0.100 , qa loss 0.100 , lm loss 0.000 , avg batch size 37.0
2023-06-30 05:29:52,595 - 5:49:22 - 394.7s - INFO - __main__ - epoch 18/20 done , tot steps 27432 , lr 6.3E-06 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 37.0
2023-06-30 05:42:32,793 - 6:02:02 - 760.2s - INFO - __main__ - progress 18.656 , lr 4.2E-06 , loss 0.099 , qa loss 0.099 , lm loss 0.000 , avg batch size 37.0
2023-06-30 05:49:08,261 - 6:08:38 - 395.5s - INFO - __main__ - epoch 19/20 done , tot steps 28956 , lr 3.1E-06 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 37.0
2023-06-30 06:01:47,434 - 6:21:17 - 759.2s - INFO - __main__ - progress 19.656 , lr 1.1E-06 , loss 0.098 , qa loss 0.098 , lm loss 0.000 , avg batch size 37.0
2023-06-30 06:08:28,261 - 6:27:58 - 400.8s - INFO - __main__ - epoch 20/20 done , tot steps 30480 , lr 1.6E-08 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 37.0
2023-06-30 06:08:37,728 - 6:28:07 - 9.5s - INFO - __main__ - start to train { task: ['ag'], seq train type: lll }
2023-06-30 06:08:41,085 - 6:28:11 - 3.4s - INFO - utils - writing extra data in ../../model_aamm2/gpt2/lll/wikisql_ag_amazon_sst_srl_woz.en_0.0/wikisql/lm.csv ...
2023-06-30 06:08:41,121 - 6:28:11 - 0.0s - INFO - __main__ - extra training data size: 0
2023-06-30 06:08:41,366 - 6:28:11 - 0.2s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[wikisql]
The task with which model is saved wikisql
2023-06-30 06:08:54,625 - 6:28:24 - 13.3s - INFO - __main__ - len of train dataset: 115000 , max train batch size 76 , num of opt steps: 2300000
2023-06-30 06:22:13,470 - 6:41:43 - 798.8s - INFO - __main__ - progress 0.650 , lr 6.0E-05 , loss 1.347 , qa loss 1.347 , lm loss 0.000 , avg batch size 74.7
2023-06-30 06:29:22,536 - 6:48:52 - 429.1s - INFO - __main__ - epoch 1/20 done , tot steps 1541 , lr 5.9E-05 , loss 0.92 , qa loss 0.92 , lm loss 0.00 , avg batch size 74.6
2023-06-30 06:42:41,619 - 7:02:11 - 799.1s - INFO - __main__ - progress 1.648 , lr 5.7E-05 , loss 0.102 , qa loss 0.102 , lm loss 0.000 , avg batch size 74.6
2023-06-30 06:49:51,697 - 7:09:21 - 430.1s - INFO - __main__ - epoch 2/20 done , tot steps 3082 , lr 5.6E-05 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 74.6
2023-06-30 07:03:12,265 - 7:22:42 - 800.6s - INFO - __main__ - progress 2.649 , lr 5.4E-05 , loss 0.084 , qa loss 0.084 , lm loss 0.000 , avg batch size 74.6
2023-06-30 07:10:24,512 - 7:29:54 - 432.2s - INFO - __main__ - epoch 3/20 done , tot steps 4626 , lr 5.3E-05 , loss 0.08 , qa loss 0.08 , lm loss 0.00 , avg batch size 74.5
2023-06-30 07:23:47,027 - 7:43:17 - 802.5s - INFO - __main__ - progress 3.650 , lr 5.1E-05 , loss 0.077 , qa loss 0.077 , lm loss 0.000 , avg batch size 74.7
2023-06-30 07:30:57,220 - 7:50:27 - 430.2s - INFO - __main__ - epoch 4/20 done , tot steps 6166 , lr 5.0E-05 , loss 0.08 , qa loss 0.08 , lm loss 0.00 , avg batch size 74.7
2023-06-30 07:44:17,486 - 8:03:47 - 800.3s - INFO - __main__ - progress 4.648 , lr 4.8E-05 , loss 0.075 , qa loss 0.075 , lm loss 0.000 , avg batch size 74.5
2023-06-30 07:51:29,338 - 8:10:59 - 431.9s - INFO - __main__ - epoch 5/20 done , tot steps 7708 , lr 4.7E-05 , loss 0.07 , qa loss 0.07 , lm loss 0.00 , avg batch size 74.6
2023-06-30 08:04:50,200 - 8:24:20 - 800.9s - INFO - __main__ - progress 5.650 , lr 4.5E-05 , loss 0.068 , qa loss 0.068 , lm loss 0.000 , avg batch size 74.7
2023-06-30 08:12:01,549 - 8:31:31 - 431.3s - INFO - __main__ - epoch 6/20 done , tot steps 9249 , lr 4.4E-05 , loss 0.07 , qa loss 0.07 , lm loss 0.00 , avg batch size 74.6
2023-06-30 08:25:21,281 - 8:44:51 - 799.7s - INFO - __main__ - progress 6.648 , lr 4.2E-05 , loss 0.067 , qa loss 0.067 , lm loss 0.000 , avg batch size 74.6
2023-06-30 08:32:32,707 - 8:52:02 - 431.4s - INFO - __main__ - epoch 7/20 done , tot steps 10792 , lr 4.1E-05 , loss 0.07 , qa loss 0.07 , lm loss 0.00 , avg batch size 74.5
2023-06-30 08:45:53,346 - 9:05:23 - 800.6s - INFO - __main__ - progress 7.649 , lr 3.9E-05 , loss 0.064 , qa loss 0.064 , lm loss 0.000 , avg batch size 74.6
2023-06-30 08:53:01,804 - 9:12:31 - 428.5s - INFO - __main__ - epoch 8/20 done , tot steps 12331 , lr 3.8E-05 , loss 0.06 , qa loss 0.06 , lm loss 0.00 , avg batch size 74.7
2023-06-30 09:06:21,552 - 9:25:51 - 799.7s - INFO - __main__ - progress 8.649 , lr 3.5E-05 , loss 0.061 , qa loss 0.061 , lm loss 0.000 , avg batch size 74.6
2023-06-30 09:13:33,286 - 9:33:03 - 431.7s - INFO - __main__ - epoch 9/20 done , tot steps 13873 , lr 3.4E-05 , loss 0.06 , qa loss 0.06 , lm loss 0.00 , avg batch size 74.6
2023-06-30 09:26:53,462 - 9:46:23 - 800.2s - INFO - __main__ - progress 9.648 , lr 3.2E-05 , loss 0.059 , qa loss 0.059 , lm loss 0.000 , avg batch size 74.5
2023-06-30 09:34:04,841 - 9:53:34 - 431.4s - INFO - __main__ - epoch 10/20 done , tot steps 15415 , lr 3.1E-05 , loss 0.06 , qa loss 0.06 , lm loss 0.00 , avg batch size 74.6
2023-06-30 09:47:23,198 - 10:06:53 - 798.4s - INFO - __main__ - progress 10.648 , lr 2.9E-05 , loss 0.058 , qa loss 0.058 , lm loss 0.000 , avg batch size 74.5
2023-06-30 09:54:34,932 - 10:14:05 - 431.7s - INFO - __main__ - epoch 11/20 done , tot steps 16959 , lr 2.8E-05 , loss 0.06 , qa loss 0.06 , lm loss 0.00 , avg batch size 74.5
2023-06-30 10:07:53,456 - 10:27:23 - 798.5s - INFO - __main__ - progress 11.651 , lr 2.6E-05 , loss 0.056 , qa loss 0.056 , lm loss 0.000 , avg batch size 74.8
2023-06-30 10:15:04,617 - 10:34:34 - 431.2s - INFO - __main__ - epoch 12/20 done , tot steps 18500 , lr 2.5E-05 , loss 0.06 , qa loss 0.06 , lm loss 0.00 , avg batch size 74.6
2023-06-30 10:28:23,435 - 10:47:53 - 798.8s - INFO - __main__ - progress 12.650 , lr 2.3E-05 , loss 0.055 , qa loss 0.055 , lm loss 0.000 , avg batch size 74.7
2023-06-30 10:35:33,243 - 10:55:03 - 429.8s - INFO - __main__ - epoch 13/20 done , tot steps 20041 , lr 2.2E-05 , loss 0.05 , qa loss 0.05 , lm loss 0.00 , avg batch size 74.6
2023-06-30 10:48:53,061 - 11:08:23 - 799.8s - INFO - __main__ - progress 13.648 , lr 2.0E-05 , loss 0.054 , qa loss 0.054 , lm loss 0.000 , avg batch size 74.5
2023-06-30 10:56:03,369 - 11:15:33 - 430.3s - INFO - __main__ - epoch 14/20 done , tot steps 21583 , lr 1.9E-05 , loss 0.05 , qa loss 0.05 , lm loss 0.00 , avg batch size 74.6
2023-06-30 11:09:22,451 - 11:28:52 - 799.1s - INFO - __main__ - progress 14.647 , lr 1.7E-05 , loss 0.052 , qa loss 0.052 , lm loss 0.000 , avg batch size 74.4
2023-06-30 11:16:35,023 - 11:36:05 - 432.6s - INFO - __main__ - epoch 15/20 done , tot steps 23127 , lr 1.6E-05 , loss 0.05 , qa loss 0.05 , lm loss 0.00 , avg batch size 74.5
2023-06-30 11:29:54,779 - 11:49:24 - 799.8s - INFO - __main__ - progress 15.648 , lr 1.4E-05 , loss 0.051 , qa loss 0.051 , lm loss 0.000 , avg batch size 74.5
2023-06-30 11:37:05,601 - 11:56:35 - 430.8s - INFO - __main__ - epoch 16/20 done , tot steps 24670 , lr 1.3E-05 , loss 0.05 , qa loss 0.05 , lm loss 0.00 , avg batch size 74.5
2023-06-30 11:50:24,335 - 12:09:54 - 798.7s - INFO - __main__ - progress 16.648 , lr 1.0E-05 , loss 0.050 , qa loss 0.050 , lm loss 0.000 , avg batch size 74.5
2023-06-30 11:57:34,031 - 12:17:04 - 429.7s - INFO - __main__ - epoch 17/20 done , tot steps 26213 , lr 9.4E-06 , loss 0.05 , qa loss 0.05 , lm loss 0.00 , avg batch size 74.5
2023-06-30 12:10:52,290 - 12:30:22 - 798.3s - INFO - __main__ - progress 17.648 , lr 7.4E-06 , loss 0.050 , qa loss 0.050 , lm loss 0.000 , avg batch size 74.6
2023-06-30 12:18:03,667 - 12:37:33 - 431.4s - INFO - __main__ - epoch 18/20 done , tot steps 27756 , lr 6.3E-06 , loss 0.05 , qa loss 0.05 , lm loss 0.00 , avg batch size 74.5
2023-06-30 12:31:22,877 - 12:50:53 - 799.2s - INFO - __main__ - progress 18.649 , lr 4.2E-06 , loss 0.048 , qa loss 0.048 , lm loss 0.000 , avg batch size 74.7
2023-06-30 12:38:33,807 - 12:58:03 - 430.9s - INFO - __main__ - epoch 19/20 done , tot steps 29300 , lr 3.1E-06 , loss 0.05 , qa loss 0.05 , lm loss 0.00 , avg batch size 74.5
2023-06-30 12:51:53,957 - 13:11:24 - 800.2s - INFO - __main__ - progress 19.648 , lr 1.1E-06 , loss 0.048 , qa loss 0.048 , lm loss 0.000 , avg batch size 74.5
2023-06-30 12:59:14,154 - 13:18:44 - 440.2s - INFO - __main__ - epoch 20/20 done , tot steps 30844 , lr 1.6E-08 , loss 0.05 , qa loss 0.05 , lm loss 0.00 , avg batch size 74.5
2023-06-30 12:59:23,135 - 13:18:53 - 9.0s - INFO - __main__ - start to train { task: ['amazon'], seq train type: lll }
2023-06-30 12:59:26,824 - 13:18:56 - 3.7s - INFO - utils - writing extra data in ../../model_aamm2/gpt2/lll/wikisql_ag_amazon_sst_srl_woz.en_0.0/ag/lm.csv ...
2023-06-30 12:59:26,857 - 13:18:56 - 0.0s - INFO - __main__ - extra training data size: 0
2023-06-30 12:59:27,266 - 13:18:57 - 0.4s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[ag]
The task with which model is saved ag
2023-06-30 12:59:42,349 - 13:19:12 - 15.1s - INFO - __main__ - len of train dataset: 115000 , max train batch size 76 , num of opt steps: 2300000
2023-06-30 13:13:32,071 - 13:33:02 - 829.7s - INFO - __main__ - progress 0.429 , lr 6.1E-05 , loss 1.918 , qa loss 1.918 , lm loss 0.000 , avg batch size 49.3
2023-06-30 13:27:09,479 - 13:46:39 - 817.4s - INFO - __main__ - progress 0.856 , lr 6.0E-05 , loss 1.184 , qa loss 1.184 , lm loss 0.000 , avg batch size 49.2
2023-06-30 13:31:46,454 - 13:51:16 - 277.0s - INFO - __main__ - epoch 1/20 done , tot steps 2336 , lr 5.9E-05 , loss 1.08 , qa loss 1.08 , lm loss 0.00 , avg batch size 49.2
2023-06-30 13:45:36,427 - 14:05:06 - 830.0s - INFO - __main__ - progress 1.428 , lr 5.8E-05 , loss 0.425 , qa loss 0.425 , lm loss 0.000 , avg batch size 49.3
2023-06-30 13:59:14,916 - 14:18:45 - 818.5s - INFO - __main__ - progress 1.858 , lr 5.7E-05 , loss 0.420 , qa loss 0.420 , lm loss 0.000 , avg batch size 49.4
2023-06-30 14:03:48,864 - 14:23:19 - 273.9s - INFO - __main__ - epoch 2/20 done , tot steps 4668 , lr 5.6E-05 , loss 0.42 , qa loss 0.42 , lm loss 0.00 , avg batch size 49.3
2023-06-30 14:17:37,296 - 14:37:07 - 828.4s - INFO - __main__ - progress 2.428 , lr 5.5E-05 , loss 0.404 , qa loss 0.404 , lm loss 0.000 , avg batch size 49.3
2023-06-30 14:31:14,802 - 14:50:44 - 817.5s - INFO - __main__ - progress 2.858 , lr 5.4E-05 , loss 0.404 , qa loss 0.404 , lm loss 0.000 , avg batch size 49.3
2023-06-30 14:35:49,018 - 14:55:19 - 274.2s - INFO - __main__ - epoch 3/20 done , tot steps 7000 , lr 5.3E-05 , loss 0.40 , qa loss 0.40 , lm loss 0.00 , avg batch size 49.3
2023-06-30 14:49:39,634 - 15:09:09 - 830.6s - INFO - __main__ - progress 3.428 , lr 5.2E-05 , loss 0.395 , qa loss 0.395 , lm loss 0.000 , avg batch size 49.2
2023-06-30 15:03:17,173 - 15:22:47 - 817.5s - INFO - __main__ - progress 3.857 , lr 5.0E-05 , loss 0.394 , qa loss 0.394 , lm loss 0.000 , avg batch size 49.3
2023-06-30 15:07:54,891 - 15:27:25 - 277.7s - INFO - __main__ - epoch 4/20 done , tot steps 9336 , lr 5.0E-05 , loss 0.39 , qa loss 0.39 , lm loss 0.00 , avg batch size 49.2
2023-06-30 15:21:45,639 - 15:41:15 - 830.7s - INFO - __main__ - progress 4.427 , lr 4.9E-05 , loss 0.385 , qa loss 0.385 , lm loss 0.000 , avg batch size 49.1
2023-06-30 15:35:23,785 - 15:54:53 - 818.1s - INFO - __main__ - progress 4.856 , lr 4.7E-05 , loss 0.385 , qa loss 0.385 , lm loss 0.000 , avg batch size 49.2
2023-06-30 15:40:00,894 - 15:59:31 - 277.1s - INFO - __main__ - epoch 5/20 done , tot steps 11672 , lr 4.7E-05 , loss 0.38 , qa loss 0.38 , lm loss 0.00 , avg batch size 49.2
2023-06-30 15:53:50,301 - 16:13:20 - 829.4s - INFO - __main__ - progress 5.429 , lr 4.6E-05 , loss 0.379 , qa loss 0.379 , lm loss 0.000 , avg batch size 49.4
2023-06-30 16:07:26,696 - 16:26:56 - 816.4s - INFO - __main__ - progress 5.858 , lr 4.4E-05 , loss 0.379 , qa loss 0.379 , lm loss 0.000 , avg batch size 49.3
2023-06-30 16:12:01,866 - 16:31:32 - 275.2s - INFO - __main__ - epoch 6/20 done , tot steps 14005 , lr 4.4E-05 , loss 0.38 , qa loss 0.38 , lm loss 0.00 , avg batch size 49.3
2023-06-30 16:25:49,631 - 16:45:19 - 827.8s - INFO - __main__ - progress 6.430 , lr 4.2E-05 , loss 0.373 , qa loss 0.373 , lm loss 0.000 , avg batch size 49.5
2023-06-30 16:39:26,952 - 16:58:57 - 817.3s - INFO - __main__ - progress 6.858 , lr 4.1E-05 , loss 0.374 , qa loss 0.374 , lm loss 0.000 , avg batch size 49.3
2023-06-30 16:44:01,891 - 17:03:32 - 274.9s - INFO - __main__ - epoch 7/20 done , tot steps 16338 , lr 4.1E-05 , loss 0.37 , qa loss 0.37 , lm loss 0.00 , avg batch size 49.3
2023-06-30 16:57:50,183 - 17:17:20 - 828.3s - INFO - __main__ - progress 7.429 , lr 3.9E-05 , loss 0.366 , qa loss 0.366 , lm loss 0.000 , avg batch size 49.3
2023-06-30 17:11:26,711 - 17:30:56 - 816.5s - INFO - __main__ - progress 7.856 , lr 3.8E-05 , loss 0.368 , qa loss 0.368 , lm loss 0.000 , avg batch size 49.2
2023-06-30 17:16:02,348 - 17:35:32 - 275.6s - INFO - __main__ - epoch 8/20 done , tot steps 18671 , lr 3.8E-05 , loss 0.37 , qa loss 0.37 , lm loss 0.00 , avg batch size 49.3
2023-06-30 17:29:50,613 - 17:49:20 - 828.3s - INFO - __main__ - progress 8.429 , lr 3.6E-05 , loss 0.365 , qa loss 0.365 , lm loss 0.000 , avg batch size 49.3
2023-06-30 17:43:27,425 - 18:02:57 - 816.8s - INFO - __main__ - progress 8.856 , lr 3.5E-05 , loss 0.363 , qa loss 0.363 , lm loss 0.000 , avg batch size 49.2
2023-06-30 17:48:01,004 - 18:07:31 - 273.6s - INFO - __main__ - epoch 9/20 done , tot steps 21004 , lr 3.4E-05 , loss 0.36 , qa loss 0.36 , lm loss 0.00 , avg batch size 49.3
2023-06-30 18:01:48,700 - 18:21:18 - 827.7s - INFO - __main__ - progress 9.429 , lr 3.3E-05 , loss 0.358 , qa loss 0.358 , lm loss 0.000 , avg batch size 49.3
2023-06-30 18:15:25,365 - 18:34:55 - 816.7s - INFO - __main__ - progress 9.858 , lr 3.2E-05 , loss 0.358 , qa loss 0.358 , lm loss 0.000 , avg batch size 49.3
2023-06-30 18:19:59,185 - 18:39:29 - 273.8s - INFO - __main__ - epoch 10/20 done , tot steps 23337 , lr 3.1E-05 , loss 0.36 , qa loss 0.36 , lm loss 0.00 , avg batch size 49.3
2023-06-30 18:33:47,522 - 18:53:17 - 828.3s - INFO - __main__ - progress 10.427 , lr 3.0E-05 , loss 0.356 , qa loss 0.356 , lm loss 0.000 , avg batch size 49.1
2023-06-30 18:47:25,699 - 19:06:55 - 818.2s - INFO - __main__ - progress 10.856 , lr 2.9E-05 , loss 0.356 , qa loss 0.356 , lm loss 0.000 , avg batch size 49.2
2023-06-30 18:52:01,954 - 19:11:32 - 276.3s - INFO - __main__ - epoch 11/20 done , tot steps 25672 , lr 2.8E-05 , loss 0.36 , qa loss 0.36 , lm loss 0.00 , avg batch size 49.3
2023-06-30 19:05:52,184 - 19:25:22 - 830.2s - INFO - __main__ - progress 11.429 , lr 2.7E-05 , loss 0.352 , qa loss 0.352 , lm loss 0.000 , avg batch size 49.3
2023-06-30 19:19:29,134 - 19:38:59 - 816.9s - INFO - __main__ - progress 11.858 , lr 2.5E-05 , loss 0.353 , qa loss 0.353 , lm loss 0.000 , avg batch size 49.3
2023-06-30 19:24:03,082 - 19:43:33 - 273.9s - INFO - __main__ - epoch 12/20 done , tot steps 28004 , lr 2.5E-05 , loss 0.35 , qa loss 0.35 , lm loss 0.00 , avg batch size 49.3
2023-06-30 19:37:52,691 - 19:57:22 - 829.6s - INFO - __main__ - progress 12.429 , lr 2.4E-05 , loss 0.350 , qa loss 0.350 , lm loss 0.000 , avg batch size 49.4
2023-06-30 19:51:31,547 - 20:11:01 - 818.9s - INFO - __main__ - progress 12.858 , lr 2.2E-05 , loss 0.350 , qa loss 0.350 , lm loss 0.000 , avg batch size 49.3
2023-06-30 19:56:06,631 - 20:15:36 - 275.1s - INFO - __main__ - epoch 13/20 done , tot steps 30336 , lr 2.2E-05 , loss 0.35 , qa loss 0.35 , lm loss 0.00 , avg batch size 49.3
2023-06-30 20:09:56,433 - 20:29:26 - 829.8s - INFO - __main__ - progress 13.427 , lr 2.1E-05 , loss 0.347 , qa loss 0.347 , lm loss 0.000 , avg batch size 49.1
2023-06-30 20:23:35,204 - 20:43:05 - 818.8s - INFO - __main__ - progress 13.858 , lr 1.9E-05 , loss 0.347 , qa loss 0.347 , lm loss 0.000 , avg batch size 49.3
2023-06-30 20:28:10,743 - 20:47:40 - 275.5s - INFO - __main__ - epoch 14/20 done , tot steps 32670 , lr 1.9E-05 , loss 0.35 , qa loss 0.35 , lm loss 0.00 , avg batch size 49.3
2023-06-30 20:42:00,970 - 21:01:31 - 830.2s - INFO - __main__ - progress 14.430 , lr 1.7E-05 , loss 0.343 , qa loss 0.343 , lm loss 0.000 , avg batch size 49.4
2023-06-30 20:55:38,109 - 21:15:08 - 817.1s - INFO - __main__ - progress 14.858 , lr 1.6E-05 , loss 0.345 , qa loss 0.345 , lm loss 0.000 , avg batch size 49.3
2023-06-30 21:00:11,828 - 21:19:41 - 273.7s - INFO - __main__ - epoch 15/20 done , tot steps 35003 , lr 1.6E-05 , loss 0.34 , qa loss 0.34 , lm loss 0.00 , avg batch size 49.3
2023-06-30 21:14:02,069 - 21:33:32 - 830.2s - INFO - __main__ - progress 15.429 , lr 1.4E-05 , loss 0.342 , qa loss 0.342 , lm loss 0.000 , avg batch size 49.3
2023-06-30 21:27:41,048 - 21:47:11 - 819.0s - INFO - __main__ - progress 15.856 , lr 1.3E-05 , loss 0.343 , qa loss 0.343 , lm loss 0.000 , avg batch size 49.2
2023-06-30 21:32:15,615 - 21:51:45 - 274.6s - INFO - __main__ - epoch 16/20 done , tot steps 37336 , lr 1.3E-05 , loss 0.34 , qa loss 0.34 , lm loss 0.00 , avg batch size 49.3
2023-06-30 21:46:06,864 - 22:05:37 - 831.2s - INFO - __main__ - progress 16.430 , lr 1.1E-05 , loss 0.341 , qa loss 0.341 , lm loss 0.000 , avg batch size 49.4
2023-06-30 21:59:45,709 - 22:19:15 - 818.8s - INFO - __main__ - progress 16.858 , lr 9.8E-06 , loss 0.341 , qa loss 0.341 , lm loss 0.000 , avg batch size 49.3
2023-06-30 22:04:20,978 - 22:23:51 - 275.3s - INFO - __main__ - epoch 17/20 done , tot steps 39670 , lr 9.4E-06 , loss 0.34 , qa loss 0.34 , lm loss 0.00 , avg batch size 49.3
2023-06-30 22:18:11,557 - 22:37:41 - 830.6s - INFO - __main__ - progress 17.430 , lr 8.0E-06 , loss 0.337 , qa loss 0.337 , lm loss 0.000 , avg batch size 49.5
2023-06-30 22:31:48,456 - 22:51:18 - 816.9s - INFO - __main__ - progress 17.858 , lr 6.7E-06 , loss 0.339 , qa loss 0.339 , lm loss 0.000 , avg batch size 49.3
2023-06-30 22:36:22,834 - 22:55:52 - 274.4s - INFO - __main__ - epoch 18/20 done , tot steps 42002 , lr 6.3E-06 , loss 0.34 , qa loss 0.34 , lm loss 0.00 , avg batch size 49.3
2023-06-30 22:50:12,758 - 23:09:42 - 829.9s - INFO - __main__ - progress 18.428 , lr 4.9E-06 , loss 0.338 , qa loss 0.338 , lm loss 0.000 , avg batch size 49.2
2023-06-30 23:03:51,667 - 23:23:21 - 818.9s - INFO - __main__ - progress 18.857 , lr 3.6E-06 , loss 0.338 , qa loss 0.338 , lm loss 0.000 , avg batch size 49.3
2023-06-30 23:08:26,495 - 23:27:56 - 274.8s - INFO - __main__ - epoch 19/20 done , tot steps 44334 , lr 3.1E-06 , loss 0.34 , qa loss 0.34 , lm loss 0.00 , avg batch size 49.3
2023-06-30 23:22:16,118 - 23:41:46 - 829.6s - INFO - __main__ - progress 19.430 , lr 1.8E-06 , loss 0.336 , qa loss 0.336 , lm loss 0.000 , avg batch size 49.4
2023-06-30 23:35:53,806 - 23:55:23 - 817.7s - INFO - __main__ - progress 19.857 , lr 4.6E-07 , loss 0.337 , qa loss 0.337 , lm loss 0.000 , avg batch size 49.3
2023-06-30 23:40:35,169 - 1 day, 0:00:05 - 281.4s - INFO - __main__ - epoch 20/20 done , tot steps 46667 , lr 1.6E-08 , loss 0.34 , qa loss 0.34 , lm loss 0.00 , avg batch size 49.3
2023-06-30 23:40:44,521 - 1 day, 0:00:14 - 9.4s - INFO - __main__ - start to train { task: ['sst'], seq train type: lll }
2023-06-30 23:40:47,643 - 1 day, 0:00:17 - 3.1s - INFO - utils - writing extra data in ../../model_aamm2/gpt2/lll/wikisql_ag_amazon_sst_srl_woz.en_0.0/amazon/lm.csv ...
2023-06-30 23:40:47,710 - 1 day, 0:00:17 - 0.1s - INFO - __main__ - extra training data size: 0
2023-06-30 23:40:47,985 - 1 day, 0:00:18 - 0.3s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[amazon]
The task with which model is saved amazon
2023-06-30 23:40:55,247 - 1 day, 0:00:25 - 7.3s - INFO - __main__ - len of train dataset: 6920 , max train batch size 4 , num of opt steps: 138400
2023-06-30 23:42:52,953 - 1 day, 0:02:23 - 117.7s - INFO - __main__ - progress 0.578 , lr 6.1E-05 , loss 3.285 , qa loss 3.285 , lm loss 0.000 , avg batch size 4.0
2023-06-30 23:44:14,689 - 1 day, 0:03:44 - 81.7s - INFO - __main__ - epoch 1/20 done , tot steps 1730 , lr 5.9E-05 , loss 2.01 , qa loss 2.01 , lm loss 0.00 , avg batch size 4.0
2023-06-30 23:46:12,091 - 1 day, 0:05:42 - 117.4s - INFO - __main__ - progress 1.578 , lr 5.8E-05 , loss 0.231 , qa loss 0.231 , lm loss 0.000 , avg batch size 4.0
2023-06-30 23:47:34,417 - 1 day, 0:07:04 - 82.3s - INFO - __main__ - epoch 2/20 done , tot steps 3460 , lr 5.6E-05 , loss 0.21 , qa loss 0.21 , lm loss 0.00 , avg batch size 4.0
2023-06-30 23:49:31,594 - 1 day, 0:09:01 - 117.2s - INFO - __main__ - progress 2.578 , lr 5.4E-05 , loss 0.178 , qa loss 0.178 , lm loss 0.000 , avg batch size 4.0
2023-06-30 23:50:52,974 - 1 day, 0:10:23 - 81.4s - INFO - __main__ - epoch 3/20 done , tot steps 5190 , lr 5.3E-05 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-06-30 23:52:51,976 - 1 day, 0:12:22 - 119.0s - INFO - __main__ - progress 3.578 , lr 5.1E-05 , loss 0.160 , qa loss 0.160 , lm loss 0.000 , avg batch size 4.0
2023-06-30 23:54:13,463 - 1 day, 0:13:43 - 81.5s - INFO - __main__ - epoch 4/20 done , tot steps 6920 , lr 5.0E-05 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-06-30 23:56:10,963 - 1 day, 0:15:41 - 117.5s - INFO - __main__ - progress 4.578 , lr 4.8E-05 , loss 0.145 , qa loss 0.145 , lm loss 0.000 , avg batch size 4.0
2023-06-30 23:57:33,286 - 1 day, 0:17:03 - 82.3s - INFO - __main__ - epoch 5/20 done , tot steps 8650 , lr 4.7E-05 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
2023-06-30 23:59:31,496 - 1 day, 0:19:01 - 118.2s - INFO - __main__ - progress 5.578 , lr 4.5E-05 , loss 0.136 , qa loss 0.136 , lm loss 0.000 , avg batch size 4.0
2023-07-01 00:00:53,684 - 1 day, 0:20:23 - 82.2s - INFO - __main__ - epoch 6/20 done , tot steps 10380 , lr 4.4E-05 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
2023-07-01 00:02:51,178 - 1 day, 0:22:21 - 117.5s - INFO - __main__ - progress 6.578 , lr 4.2E-05 , loss 0.124 , qa loss 0.124 , lm loss 0.000 , avg batch size 4.0
2023-07-01 00:04:14,645 - 1 day, 0:23:44 - 83.5s - INFO - __main__ - epoch 7/20 done , tot steps 12110 , lr 4.1E-05 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2023-07-01 00:06:14,249 - 1 day, 0:25:44 - 119.6s - INFO - __main__ - progress 7.578 , lr 3.9E-05 , loss 0.103 , qa loss 0.103 , lm loss 0.000 , avg batch size 4.0
2023-07-01 00:07:38,138 - 1 day, 0:27:08 - 83.9s - INFO - __main__ - epoch 8/20 done , tot steps 13840 , lr 3.8E-05 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-07-01 00:09:35,017 - 1 day, 0:29:05 - 116.9s - INFO - __main__ - progress 8.578 , lr 3.6E-05 , loss 0.096 , qa loss 0.096 , lm loss 0.000 , avg batch size 4.0
2023-07-01 00:10:57,390 - 1 day, 0:30:27 - 82.4s - INFO - __main__ - epoch 9/20 done , tot steps 15570 , lr 3.4E-05 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 4.0
2023-07-01 00:12:56,116 - 1 day, 0:32:26 - 118.7s - INFO - __main__ - progress 9.578 , lr 3.3E-05 , loss 0.100 , qa loss 0.100 , lm loss 0.000 , avg batch size 4.0
2023-07-01 00:14:18,296 - 1 day, 0:33:48 - 82.2s - INFO - __main__ - epoch 10/20 done , tot steps 17300 , lr 3.1E-05 , loss 0.09 , qa loss 0.09 , lm loss 0.00 , avg batch size 4.0
2023-07-01 00:16:16,764 - 1 day, 0:35:46 - 118.5s - INFO - __main__ - progress 10.578 , lr 2.9E-05 , loss 0.081 , qa loss 0.081 , lm loss 0.000 , avg batch size 4.0
2023-07-01 00:17:38,594 - 1 day, 0:37:08 - 81.8s - INFO - __main__ - epoch 11/20 done , tot steps 19030 , lr 2.8E-05 , loss 0.09 , qa loss 0.09 , lm loss 0.00 , avg batch size 4.0
2023-07-01 00:19:35,390 - 1 day, 0:39:05 - 116.8s - INFO - __main__ - progress 11.578 , lr 2.6E-05 , loss 0.067 , qa loss 0.067 , lm loss 0.000 , avg batch size 4.0
2023-07-01 00:20:58,124 - 1 day, 0:40:28 - 82.7s - INFO - __main__ - epoch 12/20 done , tot steps 20760 , lr 2.5E-05 , loss 0.07 , qa loss 0.07 , lm loss 0.00 , avg batch size 4.0
2023-07-01 00:22:55,200 - 1 day, 0:42:25 - 117.1s - INFO - __main__ - progress 12.578 , lr 2.3E-05 , loss 0.075 , qa loss 0.075 , lm loss 0.000 , avg batch size 4.0
2023-07-01 00:24:18,391 - 1 day, 0:43:48 - 83.2s - INFO - __main__ - epoch 13/20 done , tot steps 22490 , lr 2.2E-05 , loss 0.07 , qa loss 0.07 , lm loss 0.00 , avg batch size 4.0
2023-07-01 00:26:16,906 - 1 day, 0:45:47 - 118.5s - INFO - __main__ - progress 13.578 , lr 2.0E-05 , loss 0.058 , qa loss 0.058 , lm loss 0.000 , avg batch size 4.0
2023-07-01 00:27:37,663 - 1 day, 0:47:07 - 80.8s - INFO - __main__ - epoch 14/20 done , tot steps 24220 , lr 1.9E-05 , loss 0.06 , qa loss 0.06 , lm loss 0.00 , avg batch size 4.0
2023-07-01 00:29:36,193 - 1 day, 0:49:06 - 118.5s - INFO - __main__ - progress 14.578 , lr 1.7E-05 , loss 0.052 , qa loss 0.052 , lm loss 0.000 , avg batch size 4.0
2023-07-01 00:31:00,616 - 1 day, 0:50:30 - 84.4s - INFO - __main__ - epoch 15/20 done , tot steps 25950 , lr 1.6E-05 , loss 0.06 , qa loss 0.06 , lm loss 0.00 , avg batch size 4.0
2023-07-01 00:32:58,162 - 1 day, 0:52:28 - 117.5s - INFO - __main__ - progress 15.578 , lr 1.4E-05 , loss 0.056 , qa loss 0.056 , lm loss 0.000 , avg batch size 4.0
2023-07-01 00:34:21,420 - 1 day, 0:53:51 - 83.3s - INFO - __main__ - epoch 16/20 done , tot steps 27680 , lr 1.3E-05 , loss 0.06 , qa loss 0.06 , lm loss 0.00 , avg batch size 4.0
2023-07-01 00:36:19,961 - 1 day, 0:55:50 - 118.5s - INFO - __main__ - progress 16.578 , lr 1.1E-05 , loss 0.049 , qa loss 0.049 , lm loss 0.000 , avg batch size 4.0
2023-07-01 00:37:43,348 - 1 day, 0:57:13 - 83.4s - INFO - __main__ - epoch 17/20 done , tot steps 29410 , lr 9.4E-06 , loss 0.06 , qa loss 0.06 , lm loss 0.00 , avg batch size 4.0
2023-07-01 00:39:38,793 - 1 day, 0:59:08 - 115.4s - INFO - __main__ - progress 17.578 , lr 7.6E-06 , loss 0.047 , qa loss 0.047 , lm loss 0.000 , avg batch size 4.0
2023-07-01 00:41:01,933 - 1 day, 1:00:32 - 83.1s - INFO - __main__ - epoch 18/20 done , tot steps 31140 , lr 6.3E-06 , loss 0.05 , qa loss 0.05 , lm loss 0.00 , avg batch size 4.0
2023-07-01 00:43:00,072 - 1 day, 1:02:30 - 118.1s - INFO - __main__ - progress 18.578 , lr 4.5E-06 , loss 0.043 , qa loss 0.043 , lm loss 0.000 , avg batch size 4.0
2023-07-01 00:44:24,232 - 1 day, 1:03:54 - 84.2s - INFO - __main__ - epoch 19/20 done , tot steps 32870 , lr 3.1E-06 , loss 0.05 , qa loss 0.05 , lm loss 0.00 , avg batch size 4.0
2023-07-01 00:46:21,159 - 1 day, 1:05:51 - 116.9s - INFO - __main__ - progress 19.578 , lr 1.3E-06 , loss 0.043 , qa loss 0.043 , lm loss 0.000 , avg batch size 4.0
2023-07-01 00:47:49,535 - 1 day, 1:07:19 - 88.4s - INFO - __main__ - epoch 20/20 done , tot steps 34600 , lr 1.5E-08 , loss 0.04 , qa loss 0.04 , lm loss 0.00 , avg batch size 4.0
2023-07-01 00:47:57,718 - 1 day, 1:07:27 - 8.2s - INFO - __main__ - start to train { task: ['srl'], seq train type: lll }
2023-07-01 00:48:02,598 - 1 day, 1:07:32 - 4.9s - INFO - utils - writing extra data in ../../model_aamm2/gpt2/lll/wikisql_ag_amazon_sst_srl_woz.en_0.0/sst/lm.csv ...
2023-07-01 00:48:02,715 - 1 day, 1:07:32 - 0.1s - INFO - __main__ - extra training data size: 0
2023-07-01 00:48:03,216 - 1 day, 1:07:33 - 0.5s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[sst]
The task with which model is saved sst
2023-07-01 00:48:11,273 - 1 day, 1:07:41 - 8.1s - INFO - __main__ - len of train dataset: 6414 , max train batch size 4 , num of opt steps: 128280
2023-07-01 00:51:11,003 - 1 day, 1:10:41 - 179.7s - INFO - __main__ - progress 0.624 , lr 6.1E-05 , loss 3.936 , qa loss 3.936 , lm loss 0.000 , avg batch size 4.0
2023-07-01 00:52:54,170 - 1 day, 1:12:24 - 103.2s - INFO - __main__ - epoch 1/20 done , tot steps 1604 , lr 5.9E-05 , loss 2.79 , qa loss 2.79 , lm loss 0.00 , avg batch size 4.0
2023-07-01 00:55:55,466 - 1 day, 1:15:25 - 181.3s - INFO - __main__ - progress 1.624 , lr 5.7E-05 , loss 0.779 , qa loss 0.779 , lm loss 0.000 , avg batch size 4.0
2023-07-01 00:57:37,692 - 1 day, 1:17:07 - 102.2s - INFO - __main__ - epoch 2/20 done , tot steps 3208 , lr 5.6E-05 , loss 0.75 , qa loss 0.75 , lm loss 0.00 , avg batch size 4.0
2023-07-01 01:00:36,100 - 1 day, 1:20:06 - 178.4s - INFO - __main__ - progress 2.624 , lr 5.4E-05 , loss 0.639 , qa loss 0.639 , lm loss 0.000 , avg batch size 4.0
2023-07-01 01:02:20,498 - 1 day, 1:21:50 - 104.4s - INFO - __main__ - epoch 3/20 done , tot steps 4812 , lr 5.3E-05 , loss 0.63 , qa loss 0.63 , lm loss 0.00 , avg batch size 4.0
2023-07-01 01:05:20,599 - 1 day, 1:24:50 - 180.1s - INFO - __main__ - progress 3.624 , lr 5.1E-05 , loss 0.558 , qa loss 0.558 , lm loss 0.000 , avg batch size 4.0
2023-07-01 01:07:03,468 - 1 day, 1:26:33 - 102.9s - INFO - __main__ - epoch 4/20 done , tot steps 6416 , lr 5.0E-05 , loss 0.57 , qa loss 0.57 , lm loss 0.00 , avg batch size 4.0
2023-07-01 01:10:02,861 - 1 day, 1:29:32 - 179.4s - INFO - __main__ - progress 4.624 , lr 4.8E-05 , loss 0.505 , qa loss 0.505 , lm loss 0.000 , avg batch size 4.0
2023-07-01 01:11:47,156 - 1 day, 1:31:17 - 104.3s - INFO - __main__ - epoch 5/20 done , tot steps 8020 , lr 4.7E-05 , loss 0.51 , qa loss 0.51 , lm loss 0.00 , avg batch size 4.0
2023-07-01 01:14:45,907 - 1 day, 1:34:16 - 178.8s - INFO - __main__ - progress 5.624 , lr 4.5E-05 , loss 0.469 , qa loss 0.469 , lm loss 0.000 , avg batch size 4.0
2023-07-01 01:16:30,674 - 1 day, 1:36:00 - 104.8s - INFO - __main__ - epoch 6/20 done , tot steps 9624 , lr 4.4E-05 , loss 0.47 , qa loss 0.47 , lm loss 0.00 , avg batch size 4.0
2023-07-01 01:19:26,028 - 1 day, 1:38:56 - 175.4s - INFO - __main__ - progress 6.624 , lr 4.2E-05 , loss 0.437 , qa loss 0.437 , lm loss 0.000 , avg batch size 4.0
2023-07-01 01:21:12,794 - 1 day, 1:40:42 - 106.8s - INFO - __main__ - epoch 7/20 done , tot steps 11228 , lr 4.1E-05 , loss 0.43 , qa loss 0.43 , lm loss 0.00 , avg batch size 4.0
2023-07-01 01:24:06,569 - 1 day, 1:43:36 - 173.8s - INFO - __main__ - progress 7.624 , lr 3.9E-05 , loss 0.398 , qa loss 0.398 , lm loss 0.000 , avg batch size 4.0
2023-07-01 01:25:55,231 - 1 day, 1:45:25 - 108.7s - INFO - __main__ - epoch 8/20 done , tot steps 12832 , lr 3.8E-05 , loss 0.40 , qa loss 0.40 , lm loss 0.00 , avg batch size 4.0
2023-07-01 01:28:52,403 - 1 day, 1:48:22 - 177.2s - INFO - __main__ - progress 8.624 , lr 3.6E-05 , loss 0.376 , qa loss 0.376 , lm loss 0.000 , avg batch size 4.0
2023-07-01 01:30:37,307 - 1 day, 1:50:07 - 104.9s - INFO - __main__ - epoch 9/20 done , tot steps 14436 , lr 3.4E-05 , loss 0.38 , qa loss 0.38 , lm loss 0.00 , avg batch size 4.0
2023-07-01 01:33:37,498 - 1 day, 1:53:07 - 180.2s - INFO - __main__ - progress 9.624 , lr 3.2E-05 , loss 0.353 , qa loss 0.353 , lm loss 0.000 , avg batch size 4.0
2023-07-01 01:35:17,349 - 1 day, 1:54:47 - 99.9s - INFO - __main__ - epoch 10/20 done , tot steps 16040 , lr 3.1E-05 , loss 0.35 , qa loss 0.35 , lm loss 0.00 , avg batch size 4.0
2023-07-01 01:38:14,745 - 1 day, 1:57:44 - 177.4s - INFO - __main__ - progress 10.624 , lr 2.9E-05 , loss 0.330 , qa loss 0.330 , lm loss 0.000 , avg batch size 4.0
2023-07-01 01:39:58,499 - 1 day, 1:59:28 - 103.8s - INFO - __main__ - epoch 11/20 done , tot steps 17644 , lr 2.8E-05 , loss 0.33 , qa loss 0.33 , lm loss 0.00 , avg batch size 4.0
2023-07-01 01:42:58,673 - 1 day, 2:02:28 - 180.2s - INFO - __main__ - progress 11.624 , lr 2.6E-05 , loss 0.313 , qa loss 0.313 , lm loss 0.000 , avg batch size 4.0
2023-07-01 01:44:41,731 - 1 day, 2:04:11 - 103.1s - INFO - __main__ - epoch 12/20 done , tot steps 19248 , lr 2.5E-05 , loss 0.31 , qa loss 0.31 , lm loss 0.00 , avg batch size 4.0
2023-07-01 01:47:37,623 - 1 day, 2:07:07 - 175.9s - INFO - __main__ - progress 12.624 , lr 2.3E-05 , loss 0.305 , qa loss 0.305 , lm loss 0.000 , avg batch size 4.0
2023-07-01 01:49:22,681 - 1 day, 2:08:52 - 105.1s - INFO - __main__ - epoch 13/20 done , tot steps 20852 , lr 2.2E-05 , loss 0.30 , qa loss 0.30 , lm loss 0.00 , avg batch size 4.0
2023-07-01 01:52:22,953 - 1 day, 2:11:53 - 180.3s - INFO - __main__ - progress 13.624 , lr 2.0E-05 , loss 0.286 , qa loss 0.286 , lm loss 0.000 , avg batch size 4.0
2023-07-01 01:54:07,133 - 1 day, 2:13:37 - 104.2s - INFO - __main__ - epoch 14/20 done , tot steps 22456 , lr 1.9E-05 , loss 0.29 , qa loss 0.29 , lm loss 0.00 , avg batch size 4.0
2023-07-01 01:57:05,118 - 1 day, 2:16:35 - 178.0s - INFO - __main__ - progress 14.624 , lr 1.7E-05 , loss 0.268 , qa loss 0.268 , lm loss 0.000 , avg batch size 4.0
2023-07-01 01:58:51,395 - 1 day, 2:18:21 - 106.3s - INFO - __main__ - epoch 15/20 done , tot steps 24060 , lr 1.6E-05 , loss 0.27 , qa loss 0.27 , lm loss 0.00 , avg batch size 4.0
2023-07-01 02:01:49,305 - 1 day, 2:21:19 - 177.9s - INFO - __main__ - progress 15.624 , lr 1.4E-05 , loss 0.263 , qa loss 0.263 , lm loss 0.000 , avg batch size 4.0
2023-07-01 02:03:36,124 - 1 day, 2:23:06 - 106.8s - INFO - __main__ - epoch 16/20 done , tot steps 25664 , lr 1.3E-05 , loss 0.27 , qa loss 0.27 , lm loss 0.00 , avg batch size 4.0
2023-07-01 02:06:34,192 - 1 day, 2:26:04 - 178.1s - INFO - __main__ - progress 16.624 , lr 1.1E-05 , loss 0.260 , qa loss 0.260 , lm loss 0.000 , avg batch size 4.0
2023-07-01 02:08:18,423 - 1 day, 2:27:48 - 104.2s - INFO - __main__ - epoch 17/20 done , tot steps 27268 , lr 9.4E-06 , loss 0.25 , qa loss 0.25 , lm loss 0.00 , avg batch size 4.0
2023-07-01 02:11:15,534 - 1 day, 2:30:45 - 177.1s - INFO - __main__ - progress 17.624 , lr 7.4E-06 , loss 0.255 , qa loss 0.255 , lm loss 0.000 , avg batch size 4.0
2023-07-01 02:13:01,691 - 1 day, 2:32:31 - 106.2s - INFO - __main__ - epoch 18/20 done , tot steps 28872 , lr 6.3E-06 , loss 0.25 , qa loss 0.25 , lm loss 0.00 , avg batch size 4.0
2023-07-01 02:16:01,394 - 1 day, 2:35:31 - 179.7s - INFO - __main__ - progress 18.624 , lr 4.3E-06 , loss 0.239 , qa loss 0.239 , lm loss 0.000 , avg batch size 4.0
2023-07-01 02:17:46,026 - 1 day, 2:37:16 - 104.6s - INFO - __main__ - epoch 19/20 done , tot steps 30476 , lr 3.1E-06 , loss 0.24 , qa loss 0.24 , lm loss 0.00 , avg batch size 4.0
2023-07-01 02:20:41,719 - 1 day, 2:40:11 - 175.7s - INFO - __main__ - progress 19.624 , lr 1.2E-06 , loss 0.236 , qa loss 0.236 , lm loss 0.000 , avg batch size 4.0
2023-07-01 02:22:32,706 - 1 day, 2:42:02 - 111.0s - INFO - __main__ - epoch 20/20 done , tot steps 32080 , lr 1.6E-08 , loss 0.23 , qa loss 0.23 , lm loss 0.00 , avg batch size 4.0
2023-07-01 02:22:41,753 - 1 day, 2:42:11 - 9.0s - INFO - __main__ - start to train { task: ['woz.en'], seq train type: lll }
2023-07-01 02:22:44,471 - 1 day, 2:42:14 - 2.7s - INFO - utils - writing extra data in ../../model_aamm2/gpt2/lll/wikisql_ag_amazon_sst_srl_woz.en_0.0/srl/lm.csv ...
2023-07-01 02:22:44,490 - 1 day, 2:42:14 - 0.0s - INFO - __main__ - extra training data size: 0
2023-07-01 02:22:44,822 - 1 day, 2:42:14 - 0.3s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[srl]
The task with which model is saved srl
2023-07-01 02:22:52,125 - 1 day, 2:42:22 - 7.3s - INFO - __main__ - len of train dataset: 2536 , max train batch size 4 , num of opt steps: 50720
2023-07-01 02:24:10,992 - 1 day, 2:43:41 - 78.9s - INFO - __main__ - epoch 1/20 done , tot steps 634 , lr 5.9E-05 , loss 4.42 , qa loss 4.42 , lm loss 0.00 , avg batch size 4.0
2023-07-01 02:25:27,613 - 1 day, 2:44:57 - 76.6s - INFO - __main__ - epoch 2/20 done , tot steps 1268 , lr 5.6E-05 , loss 0.49 , qa loss 0.49 , lm loss 0.00 , avg batch size 4.0
2023-07-01 02:26:46,026 - 1 day, 2:46:16 - 78.4s - INFO - __main__ - epoch 3/20 done , tot steps 1902 , lr 5.3E-05 , loss 0.33 , qa loss 0.33 , lm loss 0.00 , avg batch size 4.0
2023-07-01 02:28:04,384 - 1 day, 2:47:34 - 78.4s - INFO - __main__ - epoch 4/20 done , tot steps 2536 , lr 5.0E-05 , loss 0.26 , qa loss 0.26 , lm loss 0.00 , avg batch size 4.0
2023-07-01 02:29:23,375 - 1 day, 2:48:53 - 79.0s - INFO - __main__ - epoch 5/20 done , tot steps 3170 , lr 4.7E-05 , loss 0.23 , qa loss 0.23 , lm loss 0.00 , avg batch size 4.0
2023-07-01 02:30:41,807 - 1 day, 2:50:11 - 78.4s - INFO - __main__ - epoch 6/20 done , tot steps 3804 , lr 4.4E-05 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-07-01 02:31:59,924 - 1 day, 2:51:30 - 78.1s - INFO - __main__ - epoch 7/20 done , tot steps 4438 , lr 4.1E-05 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-07-01 02:33:19,139 - 1 day, 2:52:49 - 79.2s - INFO - __main__ - epoch 8/20 done , tot steps 5072 , lr 3.8E-05 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-07-01 02:34:37,059 - 1 day, 2:54:07 - 77.9s - INFO - __main__ - epoch 9/20 done , tot steps 5706 , lr 3.4E-05 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-07-01 02:35:54,598 - 1 day, 2:55:24 - 77.5s - INFO - __main__ - epoch 10/20 done , tot steps 6340 , lr 3.1E-05 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-07-01 02:37:13,096 - 1 day, 2:56:43 - 78.5s - INFO - __main__ - epoch 11/20 done , tot steps 6974 , lr 2.8E-05 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
2023-07-01 02:38:30,220 - 1 day, 2:58:00 - 77.1s - INFO - __main__ - epoch 12/20 done , tot steps 7608 , lr 2.5E-05 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
2023-07-01 02:39:48,867 - 1 day, 2:59:19 - 78.6s - INFO - __main__ - epoch 13/20 done , tot steps 8242 , lr 2.2E-05 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2023-07-01 02:41:07,549 - 1 day, 3:00:37 - 78.7s - INFO - __main__ - epoch 14/20 done , tot steps 8876 , lr 1.9E-05 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2023-07-01 02:42:23,288 - 1 day, 3:01:53 - 75.7s - INFO - __main__ - epoch 15/20 done , tot steps 9510 , lr 1.6E-05 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2023-07-01 02:43:40,277 - 1 day, 3:03:10 - 77.0s - INFO - __main__ - epoch 16/20 done , tot steps 10144 , lr 1.3E-05 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-07-01 02:44:57,033 - 1 day, 3:04:27 - 76.8s - INFO - __main__ - epoch 17/20 done , tot steps 10778 , lr 9.4E-06 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-07-01 02:46:12,460 - 1 day, 3:05:42 - 75.4s - INFO - __main__ - epoch 18/20 done , tot steps 11412 , lr 6.3E-06 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 4.0
2023-07-01 02:47:27,607 - 1 day, 3:06:57 - 75.1s - INFO - __main__ - epoch 19/20 done , tot steps 12046 , lr 3.1E-06 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 4.0
2023-07-01 02:48:49,873 - 1 day, 3:08:20 - 82.3s - INFO - __main__ - epoch 20/20 done , tot steps 12680 , lr 1.5E-08 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 4.0
The current active adapter is Stack[woz_en]
The task with which model is saved woz_en
Wall Execution time: 03:08:22
CPU Execution time: 12:55:20
