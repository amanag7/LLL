Not all gpus support fp16 training! Will use fp32 instead.
wandb: Currently logged in as: sprshag (cl-nlp). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.15.4
wandb: Run data is saved locally in /u/student/2021/cs21mtech11006/LLL/lamol_adapter_prompt/wandb/run-20230629_163427-n6kwc4m1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run test run
wandb: ‚≠êÔ∏è View project at https://wandb.ai/cl-nlp/cl-nlp
wandb: üöÄ View run at https://wandb.ai/cl-nlp/cl-nlp/runs/n6kwc4m1
2023-06-29 16:34:31,570 - 0:00:10 - 0.0s - INFO - __main__ - args = Namespace(REG_TYPE_KEYS=['mas', 'ewc'], adam_epsilon=0.0001, add_task_tokens=False, data_dir='../../data', debug=False, decay_style='linear', device_ids=[0], dynamic_epochs=False, fp32=True, gen_lm_sample_percentage=0.0, learning_rate=6.25e-05, lm_lambda=0.0, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[40537.0], min_batch_size=4, min_n_steps=1500, model_dir_root='../../model_cm20/gpt2/lll/sst_srl_woz.en_0.0', model_name='gpt2', n_gpus=1, n_train_epochs={'sst': 20, 'srl': 20, 'woz.en': 20}, n_warmup_ratio=0.005, n_workers=75, qp_margin=0.5, real_sample=False, reg_lambda=1.0, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[14187], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[14187], unbound=0, use_sep=False, weight_decay=0.01)
2023-06-29 16:34:31,570 - 0:00:10 - 0.0s - INFO - __main__ - start to train { task: ['sst'], seq train type: lll }
2023-06-29 16:34:31,577 - 0:00:10 - 0.0s - INFO - __main__ - extra training data size: 0
2023-06-29 16:34:34,527 - 0:00:13 - 2.9s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
2023-06-29 16:34:39,483 - 0:00:18 - 5.0s - INFO - __main__ - len of train dataset: 6920 , max train batch size 4 , num of opt steps: 138400
/home/student/2021/cs21mtech11006/anaconda3/envs/lamol/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
2023-06-29 16:36:47,983 - 0:02:26 - 128.5s - INFO - __main__ - progress 0.578 , lr 6.1E-05 , loss 1.897 , qa loss 1.897 , lm loss 0.000 , avg batch size 4.0
2023-06-29 16:38:13,589 - 0:03:52 - 85.6s - INFO - __main__ - epoch 1/20 done , tot steps 1730 , lr 5.9E-05 , loss 1.22 , qa loss 1.22 , lm loss 0.00 , avg batch size 4.0
2023-06-29 16:40:17,932 - 0:05:56 - 124.3s - INFO - __main__ - progress 1.578 , lr 5.8E-05 , loss 0.288 , qa loss 0.288 , lm loss 0.000 , avg batch size 4.0
2023-06-29 16:41:42,394 - 0:07:21 - 84.5s - INFO - __main__ - epoch 2/20 done , tot steps 3460 , lr 5.6E-05 , loss 0.27 , qa loss 0.27 , lm loss 0.00 , avg batch size 4.0
2023-06-29 16:43:46,112 - 0:09:25 - 123.7s - INFO - __main__ - progress 2.578 , lr 5.4E-05 , loss 0.215 , qa loss 0.215 , lm loss 0.000 , avg batch size 4.0
2023-06-29 16:45:11,152 - 0:10:50 - 85.0s - INFO - __main__ - epoch 3/20 done , tot steps 5190 , lr 5.3E-05 , loss 0.21 , qa loss 0.21 , lm loss 0.00 , avg batch size 4.0
2023-06-29 16:46:26,988 - 0:12:05 - 75.8s - INFO - __main__ - progress 3.578 , lr 5.1E-05 , loss 0.187 , qa loss 0.187 , lm loss 0.000 , avg batch size 4.0
2023-06-29 16:47:56,185 - 0:13:35 - 89.2s - INFO - __main__ - epoch 4/20 done , tot steps 6920 , lr 5.0E-05 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-06-29 16:49:57,610 - 0:15:36 - 121.4s - INFO - __main__ - progress 4.578 , lr 4.8E-05 , loss 0.168 , qa loss 0.168 , lm loss 0.000 , avg batch size 4.0
2023-06-29 16:51:26,509 - 0:17:05 - 88.9s - INFO - __main__ - epoch 5/20 done , tot steps 8650 , lr 4.7E-05 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
2023-06-29 16:53:27,532 - 0:19:06 - 121.0s - INFO - __main__ - progress 5.578 , lr 4.5E-05 , loss 0.157 , qa loss 0.157 , lm loss 0.000 , avg batch size 4.0
2023-06-29 16:54:57,743 - 0:20:36 - 90.2s - INFO - __main__ - epoch 6/20 done , tot steps 10380 , lr 4.4E-05 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-06-29 16:56:56,814 - 0:22:35 - 119.1s - INFO - __main__ - progress 6.578 , lr 4.2E-05 , loss 0.144 , qa loss 0.144 , lm loss 0.000 , avg batch size 4.0
2023-06-29 16:58:28,544 - 0:24:07 - 91.7s - INFO - __main__ - epoch 7/20 done , tot steps 12110 , lr 4.1E-05 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-06-29 17:00:27,620 - 0:26:06 - 119.1s - INFO - __main__ - progress 7.578 , lr 3.9E-05 , loss 0.133 , qa loss 0.133 , lm loss 0.000 , avg batch size 4.0
2023-06-29 17:01:59,159 - 0:27:38 - 91.5s - INFO - __main__ - epoch 8/20 done , tot steps 13840 , lr 3.8E-05 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-06-29 17:03:58,489 - 0:29:37 - 119.3s - INFO - __main__ - progress 8.578 , lr 3.6E-05 , loss 0.123 , qa loss 0.123 , lm loss 0.000 , avg batch size 4.0
2023-06-29 17:05:30,580 - 0:31:09 - 92.1s - INFO - __main__ - epoch 9/20 done , tot steps 15570 , lr 3.4E-05 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-06-29 17:07:30,583 - 0:33:09 - 120.0s - INFO - __main__ - progress 9.578 , lr 3.3E-05 , loss 0.117 , qa loss 0.117 , lm loss 0.000 , avg batch size 4.0
2023-06-29 17:09:00,918 - 0:34:39 - 90.3s - INFO - __main__ - epoch 10/20 done , tot steps 17300 , lr 3.1E-05 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2023-06-29 17:11:01,389 - 0:36:40 - 120.5s - INFO - __main__ - progress 10.578 , lr 2.9E-05 , loss 0.111 , qa loss 0.111 , lm loss 0.000 , avg batch size 4.0
2023-06-29 17:12:32,356 - 0:38:11 - 91.0s - INFO - __main__ - epoch 11/20 done , tot steps 19030 , lr 2.8E-05 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-06-29 17:14:33,891 - 0:40:12 - 121.5s - INFO - __main__ - progress 11.578 , lr 2.6E-05 , loss 0.108 , qa loss 0.108 , lm loss 0.000 , avg batch size 4.0
2023-06-29 17:16:03,272 - 0:41:42 - 89.4s - INFO - __main__ - epoch 12/20 done , tot steps 20760 , lr 2.5E-05 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-06-29 17:18:03,511 - 0:43:42 - 120.2s - INFO - __main__ - progress 12.578 , lr 2.3E-05 , loss 0.104 , qa loss 0.104 , lm loss 0.000 , avg batch size 4.0
2023-06-29 17:19:34,329 - 0:45:13 - 90.8s - INFO - __main__ - epoch 13/20 done , tot steps 22490 , lr 2.2E-05 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 4.0
2023-06-29 17:21:36,386 - 0:47:15 - 122.1s - INFO - __main__ - progress 13.578 , lr 2.0E-05 , loss 0.088 , qa loss 0.088 , lm loss 0.000 , avg batch size 4.0
2023-06-29 17:23:05,720 - 0:48:44 - 89.3s - INFO - __main__ - epoch 14/20 done , tot steps 24220 , lr 1.9E-05 , loss 0.09 , qa loss 0.09 , lm loss 0.00 , avg batch size 4.0
2023-06-29 17:25:06,413 - 0:50:45 - 120.7s - INFO - __main__ - progress 14.578 , lr 1.7E-05 , loss 0.087 , qa loss 0.087 , lm loss 0.000 , avg batch size 4.0
2023-06-29 17:26:37,410 - 0:52:16 - 91.0s - INFO - __main__ - epoch 15/20 done , tot steps 25950 , lr 1.6E-05 , loss 0.09 , qa loss 0.09 , lm loss 0.00 , avg batch size 4.0
2023-06-29 17:28:38,489 - 0:54:17 - 121.1s - INFO - __main__ - progress 15.578 , lr 1.4E-05 , loss 0.087 , qa loss 0.087 , lm loss 0.000 , avg batch size 4.0
2023-06-29 17:30:08,120 - 0:55:47 - 89.6s - INFO - __main__ - epoch 16/20 done , tot steps 27680 , lr 1.3E-05 , loss 0.08 , qa loss 0.08 , lm loss 0.00 , avg batch size 4.0
2023-06-29 17:32:08,578 - 0:57:47 - 120.5s - INFO - __main__ - progress 16.578 , lr 1.1E-05 , loss 0.083 , qa loss 0.083 , lm loss 0.000 , avg batch size 4.0
2023-06-29 17:33:38,692 - 0:59:17 - 90.1s - INFO - __main__ - epoch 17/20 done , tot steps 29410 , lr 9.4E-06 , loss 0.08 , qa loss 0.08 , lm loss 0.00 , avg batch size 4.0
2023-06-29 17:35:38,716 - 1:01:17 - 120.0s - INFO - __main__ - progress 17.578 , lr 7.6E-06 , loss 0.081 , qa loss 0.081 , lm loss 0.000 , avg batch size 4.0
2023-06-29 17:37:10,565 - 1:02:49 - 91.8s - INFO - __main__ - epoch 18/20 done , tot steps 31140 , lr 6.3E-06 , loss 0.08 , qa loss 0.08 , lm loss 0.00 , avg batch size 4.0
2023-06-29 17:39:12,108 - 1:04:51 - 121.5s - INFO - __main__ - progress 18.578 , lr 4.5E-06 , loss 0.067 , qa loss 0.067 , lm loss 0.000 , avg batch size 4.0
2023-06-29 17:40:43,889 - 1:06:22 - 91.8s - INFO - __main__ - epoch 19/20 done , tot steps 32870 , lr 3.1E-06 , loss 0.07 , qa loss 0.07 , lm loss 0.00 , avg batch size 4.0
2023-06-29 17:42:44,624 - 1:08:23 - 120.7s - INFO - __main__ - progress 19.578 , lr 1.3E-06 , loss 0.075 , qa loss 0.075 , lm loss 0.000 , avg batch size 4.0
2023-06-29 17:44:21,609 - 1:10:00 - 97.0s - INFO - __main__ - epoch 20/20 done , tot steps 34600 , lr 1.5E-08 , loss 0.07 , qa loss 0.07 , lm loss 0.00 , avg batch size 4.0
2023-06-29 17:44:29,982 - 1:10:08 - 8.4s - INFO - __main__ - start to train { task: ['srl'], seq train type: lll }
2023-06-29 17:44:33,204 - 1:10:12 - 3.2s - INFO - utils - writing extra data in ../../model_cm20/gpt2/lll/sst_srl_woz.en_0.0/sst/lm.csv ...
2023-06-29 17:44:33,287 - 1:10:12 - 0.1s - INFO - __main__ - extra training data size: 0
2023-06-29 17:44:33,737 - 1:10:12 - 0.5s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[sst]
The task with which model is saved sst
2023-06-29 17:44:38,136 - 1:10:17 - 4.4s - INFO - __main__ - len of train dataset: 6414 , max train batch size 4 , num of opt steps: 128280
2023-06-29 17:48:22,606 - 1:14:01 - 224.5s - INFO - __main__ - progress 0.624 , lr 6.1E-05 , loss 3.168 , qa loss 3.168 , lm loss 0.000 , avg batch size 4.0
2023-06-29 17:50:40,093 - 1:16:19 - 137.5s - INFO - __main__ - epoch 1/20 done , tot steps 1604 , lr 5.9E-05 , loss 2.36 , qa loss 2.36 , lm loss 0.00 , avg batch size 4.0
2023-06-29 17:54:29,274 - 1:20:08 - 229.2s - INFO - __main__ - progress 1.624 , lr 5.7E-05 , loss 0.881 , qa loss 0.881 , lm loss 0.000 , avg batch size 4.0
2023-06-29 17:56:41,982 - 1:22:20 - 132.7s - INFO - __main__ - epoch 2/20 done , tot steps 3208 , lr 5.6E-05 , loss 0.85 , qa loss 0.85 , lm loss 0.00 , avg batch size 4.0
2023-06-29 18:00:38,127 - 1:26:17 - 236.1s - INFO - __main__ - progress 2.624 , lr 5.4E-05 , loss 0.723 , qa loss 0.723 , lm loss 0.000 , avg batch size 4.0
2023-06-29 18:03:16,562 - 1:28:55 - 158.4s - INFO - __main__ - epoch 3/20 done , tot steps 4812 , lr 5.3E-05 , loss 0.71 , qa loss 0.71 , lm loss 0.00 , avg batch size 4.0
2023-06-29 18:07:35,627 - 1:33:14 - 259.1s - INFO - __main__ - progress 3.624 , lr 5.1E-05 , loss 0.644 , qa loss 0.644 , lm loss 0.000 , avg batch size 4.0
2023-06-29 18:10:11,157 - 1:35:50 - 155.5s - INFO - __main__ - epoch 4/20 done , tot steps 6416 , lr 5.0E-05 , loss 0.63 , qa loss 0.63 , lm loss 0.00 , avg batch size 4.0
2023-06-29 18:14:27,731 - 1:40:06 - 256.6s - INFO - __main__ - progress 4.624 , lr 4.8E-05 , loss 0.607 , qa loss 0.607 , lm loss 0.000 , avg batch size 4.0
2023-06-29 18:17:01,657 - 1:42:40 - 153.9s - INFO - __main__ - epoch 5/20 done , tot steps 8020 , lr 4.7E-05 , loss 0.59 , qa loss 0.59 , lm loss 0.00 , avg batch size 4.0
2023-06-29 18:21:22,257 - 1:47:01 - 260.6s - INFO - __main__ - progress 5.624 , lr 4.5E-05 , loss 0.550 , qa loss 0.550 , lm loss 0.000 , avg batch size 4.0
2023-06-29 18:24:00,776 - 1:49:39 - 158.5s - INFO - __main__ - epoch 6/20 done , tot steps 9624 , lr 4.4E-05 , loss 0.55 , qa loss 0.55 , lm loss 0.00 , avg batch size 4.0
2023-06-29 18:28:21,222 - 1:54:00 - 260.4s - INFO - __main__ - progress 6.624 , lr 4.2E-05 , loss 0.511 , qa loss 0.511 , lm loss 0.000 , avg batch size 4.0
2023-06-29 18:30:59,470 - 1:56:38 - 158.2s - INFO - __main__ - epoch 7/20 done , tot steps 11228 , lr 4.1E-05 , loss 0.51 , qa loss 0.51 , lm loss 0.00 , avg batch size 4.0
2023-06-29 18:35:22,928 - 2:01:01 - 263.5s - INFO - __main__ - progress 7.624 , lr 3.9E-05 , loss 0.476 , qa loss 0.476 , lm loss 0.000 , avg batch size 4.0
2023-06-29 18:37:59,626 - 2:03:38 - 156.7s - INFO - __main__ - epoch 8/20 done , tot steps 12832 , lr 3.8E-05 , loss 0.48 , qa loss 0.48 , lm loss 0.00 , avg batch size 4.0
2023-06-29 18:42:13,765 - 2:07:52 - 254.1s - INFO - __main__ - progress 8.624 , lr 3.6E-05 , loss 0.467 , qa loss 0.467 , lm loss 0.000 , avg batch size 4.0
2023-06-29 18:44:50,722 - 2:10:29 - 157.0s - INFO - __main__ - epoch 9/20 done , tot steps 14436 , lr 3.4E-05 , loss 0.45 , qa loss 0.45 , lm loss 0.00 , avg batch size 4.0
2023-06-29 18:49:07,843 - 2:14:46 - 257.1s - INFO - __main__ - progress 9.624 , lr 3.2E-05 , loss 0.428 , qa loss 0.428 , lm loss 0.000 , avg batch size 4.0
2023-06-29 18:51:47,249 - 2:17:26 - 159.4s - INFO - __main__ - epoch 10/20 done , tot steps 16040 , lr 3.1E-05 , loss 0.43 , qa loss 0.43 , lm loss 0.00 , avg batch size 4.0
2023-06-29 18:56:08,798 - 2:21:47 - 261.5s - INFO - __main__ - progress 10.624 , lr 2.9E-05 , loss 0.427 , qa loss 0.427 , lm loss 0.000 , avg batch size 4.0
2023-06-29 18:58:44,990 - 2:24:23 - 156.2s - INFO - __main__ - epoch 11/20 done , tot steps 17644 , lr 2.8E-05 , loss 0.42 , qa loss 0.42 , lm loss 0.00 , avg batch size 4.0
2023-06-29 19:02:57,017 - 2:28:36 - 252.0s - INFO - __main__ - progress 11.624 , lr 2.6E-05 , loss 0.412 , qa loss 0.412 , lm loss 0.000 , avg batch size 4.0
2023-06-29 19:05:37,827 - 2:31:16 - 160.8s - INFO - __main__ - epoch 12/20 done , tot steps 19248 , lr 2.5E-05 , loss 0.40 , qa loss 0.40 , lm loss 0.00 , avg batch size 4.0
2023-06-29 19:09:59,484 - 2:35:38 - 261.7s - INFO - __main__ - progress 12.624 , lr 2.3E-05 , loss 0.373 , qa loss 0.373 , lm loss 0.000 , avg batch size 4.0
2023-06-29 19:12:31,707 - 2:38:10 - 152.2s - INFO - __main__ - epoch 13/20 done , tot steps 20852 , lr 2.2E-05 , loss 0.38 , qa loss 0.38 , lm loss 0.00 , avg batch size 4.0
2023-06-29 19:16:47,884 - 2:42:26 - 256.2s - INFO - __main__ - progress 13.624 , lr 2.0E-05 , loss 0.366 , qa loss 0.366 , lm loss 0.000 , avg batch size 4.0
2023-06-29 19:19:26,975 - 2:45:05 - 159.1s - INFO - __main__ - epoch 14/20 done , tot steps 22456 , lr 1.9E-05 , loss 0.37 , qa loss 0.37 , lm loss 0.00 , avg batch size 4.0
2023-06-29 19:23:44,633 - 2:49:23 - 257.7s - INFO - __main__ - progress 14.624 , lr 1.7E-05 , loss 0.358 , qa loss 0.358 , lm loss 0.000 , avg batch size 4.0
2023-06-29 19:26:23,847 - 2:52:02 - 159.2s - INFO - __main__ - epoch 15/20 done , tot steps 24060 , lr 1.6E-05 , loss 0.36 , qa loss 0.36 , lm loss 0.00 , avg batch size 4.0
2023-06-29 19:30:41,464 - 2:56:20 - 257.6s - INFO - __main__ - progress 15.624 , lr 1.4E-05 , loss 0.359 , qa loss 0.359 , lm loss 0.000 , avg batch size 4.0
2023-06-29 19:33:17,938 - 2:58:56 - 156.5s - INFO - __main__ - epoch 16/20 done , tot steps 25664 , lr 1.3E-05 , loss 0.36 , qa loss 0.36 , lm loss 0.00 , avg batch size 4.0
2023-06-29 19:37:37,913 - 3:03:16 - 260.0s - INFO - __main__ - progress 16.624 , lr 1.1E-05 , loss 0.342 , qa loss 0.342 , lm loss 0.000 , avg batch size 4.0
2023-06-29 19:40:15,981 - 3:05:54 - 158.1s - INFO - __main__ - epoch 17/20 done , tot steps 27268 , lr 9.4E-06 , loss 0.34 , qa loss 0.34 , lm loss 0.00 , avg batch size 4.0
2023-06-29 19:44:36,113 - 3:10:15 - 260.1s - INFO - __main__ - progress 17.624 , lr 7.4E-06 , loss 0.334 , qa loss 0.334 , lm loss 0.000 , avg batch size 4.0
2023-06-29 19:47:11,567 - 3:12:50 - 155.5s - INFO - __main__ - epoch 18/20 done , tot steps 28872 , lr 6.3E-06 , loss 0.33 , qa loss 0.33 , lm loss 0.00 , avg batch size 4.0
2023-06-29 19:51:30,396 - 3:17:09 - 258.8s - INFO - __main__ - progress 18.624 , lr 4.3E-06 , loss 0.320 , qa loss 0.320 , lm loss 0.000 , avg batch size 4.0
2023-06-29 19:54:15,577 - 3:19:54 - 165.2s - INFO - __main__ - epoch 19/20 done , tot steps 30476 , lr 3.1E-06 , loss 0.33 , qa loss 0.33 , lm loss 0.00 , avg batch size 4.0
2023-06-29 19:58:34,673 - 3:24:13 - 259.1s - INFO - __main__ - progress 19.624 , lr 1.2E-06 , loss 0.325 , qa loss 0.325 , lm loss 0.000 , avg batch size 4.0
2023-06-29 20:01:20,972 - 3:26:59 - 166.3s - INFO - __main__ - epoch 20/20 done , tot steps 32080 , lr 1.6E-08 , loss 0.32 , qa loss 0.32 , lm loss 0.00 , avg batch size 4.0
2023-06-29 20:01:31,143 - 3:27:10 - 10.2s - INFO - __main__ - start to train { task: ['woz.en'], seq train type: lll }
2023-06-29 20:01:34,505 - 3:27:13 - 3.4s - INFO - utils - writing extra data in ../../model_cm20/gpt2/lll/sst_srl_woz.en_0.0/srl/lm.csv ...
2023-06-29 20:01:34,555 - 3:27:13 - 0.0s - INFO - __main__ - extra training data size: 0
2023-06-29 20:01:35,158 - 3:27:14 - 0.6s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[srl]
The task with which model is saved srl
2023-06-29 20:01:39,777 - 3:27:18 - 4.6s - INFO - __main__ - len of train dataset: 2536 , max train batch size 4 , num of opt steps: 50720
2023-06-29 20:03:07,201 - 3:28:46 - 87.4s - INFO - __main__ - epoch 1/20 done , tot steps 634 , lr 5.9E-05 , loss 2.84 , qa loss 2.84 , lm loss 0.00 , avg batch size 4.0
2023-06-29 20:04:34,166 - 3:30:13 - 87.0s - INFO - __main__ - epoch 2/20 done , tot steps 1268 , lr 5.6E-05 , loss 0.58 , qa loss 0.58 , lm loss 0.00 , avg batch size 4.0
2023-06-29 20:05:59,913 - 3:31:38 - 85.7s - INFO - __main__ - epoch 3/20 done , tot steps 1902 , lr 5.3E-05 , loss 0.43 , qa loss 0.43 , lm loss 0.00 , avg batch size 4.0
2023-06-29 20:07:20,347 - 3:32:59 - 80.4s - INFO - __main__ - epoch 4/20 done , tot steps 2536 , lr 5.0E-05 , loss 0.35 , qa loss 0.35 , lm loss 0.00 , avg batch size 4.0
2023-06-29 20:08:47,339 - 3:34:26 - 87.0s - INFO - __main__ - epoch 5/20 done , tot steps 3170 , lr 4.7E-05 , loss 0.30 , qa loss 0.30 , lm loss 0.00 , avg batch size 4.0
2023-06-29 20:10:13,802 - 3:35:52 - 86.5s - INFO - __main__ - epoch 6/20 done , tot steps 3804 , lr 4.4E-05 , loss 0.25 , qa loss 0.25 , lm loss 0.00 , avg batch size 4.0
2023-06-29 20:11:40,977 - 3:37:19 - 87.2s - INFO - __main__ - epoch 7/20 done , tot steps 4438 , lr 4.1E-05 , loss 0.23 , qa loss 0.23 , lm loss 0.00 , avg batch size 4.0
2023-06-29 20:12:37,035 - 3:38:16 - 56.1s - INFO - __main__ - epoch 8/20 done , tot steps 5072 , lr 3.8E-05 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-06-29 20:13:45,512 - 3:39:24 - 68.5s - INFO - __main__ - epoch 9/20 done , tot steps 5706 , lr 3.4E-05 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-06-29 20:14:57,587 - 3:40:36 - 72.1s - INFO - __main__ - epoch 10/20 done , tot steps 6340 , lr 3.1E-05 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-06-29 20:16:06,127 - 3:41:45 - 68.5s - INFO - __main__ - epoch 11/20 done , tot steps 6974 , lr 2.8E-05 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
2023-06-29 20:17:16,867 - 3:42:55 - 70.7s - INFO - __main__ - epoch 12/20 done , tot steps 7608 , lr 2.5E-05 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-06-29 20:18:25,499 - 3:44:04 - 68.6s - INFO - __main__ - epoch 13/20 done , tot steps 8242 , lr 2.2E-05 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-06-29 20:19:33,214 - 3:45:12 - 67.7s - INFO - __main__ - epoch 14/20 done , tot steps 8876 , lr 1.9E-05 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-06-29 20:20:41,979 - 3:46:20 - 68.8s - INFO - __main__ - epoch 15/20 done , tot steps 9510 , lr 1.6E-05 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
2023-06-29 20:21:48,428 - 3:47:27 - 66.4s - INFO - __main__ - epoch 16/20 done , tot steps 10144 , lr 1.3E-05 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
2023-06-29 20:22:56,180 - 3:48:35 - 67.8s - INFO - __main__ - epoch 17/20 done , tot steps 10778 , lr 9.4E-06 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-06-29 20:24:04,472 - 3:49:43 - 68.3s - INFO - __main__ - epoch 18/20 done , tot steps 11412 , lr 6.3E-06 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-06-29 20:25:13,521 - 3:50:52 - 69.0s - INFO - __main__ - epoch 19/20 done , tot steps 12046 , lr 3.1E-06 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-06-29 20:26:28,792 - 3:52:07 - 75.3s - INFO - __main__ - epoch 20/20 done , tot steps 12680 , lr 1.5E-08 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
The current active adapter is Stack[woz_en]
The task with which model is saved woz_en
Wall Execution time: 03:52:07
CPU Execution time: 03:50:26
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:      Epoch ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà
wandb: Train Loss ‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñà‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb:      Epoch 20
wandb: Train Loss 0.12349
wandb: 
wandb: üöÄ View run test run at: https://wandb.ai/cl-nlp/cl-nlp/runs/n6kwc4m1
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230629_163427-n6kwc4m1/logs
