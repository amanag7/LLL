Not all gpus support fp16 training! Will use fp32 instead.
2023-06-14 20:15:29,667 - 0:00:05 - 0.0s - INFO - __main__ - args = Namespace(REG_TYPE_KEYS=['mas', 'ewc'], adam_epsilon=0.0001, add_task_tokens=False, data_dir='../../data', debug=False, decay_style='linear', device_ids=[2, 3], dynamic_epochs=False, fp32=True, gen_lm_sample_percentage=0.0, learning_rate=6.25e-05, lm_lambda=0.0, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[38915.52, 42158.48], min_batch_size=4, min_n_steps=1500, model_dir_root='../../model_c10/gpt2/lll/sst_srl_woz.en_0.0', model_name='gpt2', n_gpus=2, n_train_epochs={'sst': 20, 'srl': 20, 'woz.en': 20}, n_warmup_ratio=0.005, n_workers=75, qp_margin=0.5, real_sample=False, reg_lambda=1.0, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[13620, 14755], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[13620, 14755], unbound=0, use_sep=False, weight_decay=0.01)
2023-06-14 20:15:29,667 - 0:00:05 - 0.0s - INFO - __main__ - start to train { task: ['sst'], seq train type: lll }
2023-06-14 20:15:29,675 - 0:00:05 - 0.0s - INFO - __main__ - extra training data size: 0
2023-06-14 20:15:42,177 - 0:00:17 - 12.5s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
2023-06-14 20:15:43,998 - 0:00:19 - 1.8s - INFO - __main__ - len of train dataset: 6920 , max train batch size 4 , num of opt steps: 138400
/home/student/2021/cs21mtech11006/anaconda3/envs/lamol/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/home/student/2021/cs21mtech11006/anaconda3/envs/lamol/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
2023-06-14 20:17:51,246 - 0:02:26 - 127.2s - INFO - __main__ - progress 0.578 , lr 6.1E-05 , loss 1.487 , qa loss 1.487 , lm loss 0.000 , avg batch size 4.0
2023-06-14 20:19:31,070 - 0:04:06 - 99.8s - INFO - __main__ - epoch 1/20 done , tot steps 1730 , lr 5.9E-05 , loss 0.98 , qa loss 0.98 , lm loss 0.00 , avg batch size 4.0
2023-06-14 20:21:33,448 - 0:06:09 - 122.4s - INFO - __main__ - progress 1.578 , lr 5.8E-05 , loss 0.258 , qa loss 0.258 , lm loss 0.000 , avg batch size 4.0
2023-06-14 20:23:05,084 - 0:07:40 - 91.6s - INFO - __main__ - epoch 2/20 done , tot steps 3460 , lr 5.6E-05 , loss 0.25 , qa loss 0.25 , lm loss 0.00 , avg batch size 4.0
2023-06-14 20:25:08,677 - 0:09:44 - 123.6s - INFO - __main__ - progress 2.578 , lr 5.4E-05 , loss 0.205 , qa loss 0.205 , lm loss 0.000 , avg batch size 4.0
2023-06-14 20:26:40,488 - 0:11:16 - 91.8s - INFO - __main__ - epoch 3/20 done , tot steps 5190 , lr 5.3E-05 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-06-14 20:28:43,647 - 0:13:19 - 123.2s - INFO - __main__ - progress 3.578 , lr 5.1E-05 , loss 0.171 , qa loss 0.171 , lm loss 0.000 , avg batch size 4.0
2023-06-14 20:30:14,989 - 0:14:50 - 91.3s - INFO - __main__ - epoch 4/20 done , tot steps 6920 , lr 5.0E-05 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
2023-06-14 20:32:18,337 - 0:16:53 - 123.3s - INFO - __main__ - progress 4.578 , lr 4.8E-05 , loss 0.151 , qa loss 0.151 , lm loss 0.000 , avg batch size 4.0
2023-06-14 20:33:50,415 - 0:18:26 - 92.1s - INFO - __main__ - epoch 5/20 done , tot steps 8650 , lr 4.7E-05 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-06-14 20:35:53,150 - 0:20:28 - 122.7s - INFO - __main__ - progress 5.578 , lr 4.5E-05 , loss 0.137 , qa loss 0.137 , lm loss 0.000 , avg batch size 4.0
2023-06-14 20:37:24,161 - 0:21:59 - 91.0s - INFO - __main__ - epoch 6/20 done , tot steps 10380 , lr 4.4E-05 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-06-14 20:39:25,294 - 0:24:00 - 121.1s - INFO - __main__ - progress 6.578 , lr 4.2E-05 , loss 0.115 , qa loss 0.115 , lm loss 0.000 , avg batch size 4.0
2023-06-14 20:40:55,499 - 0:25:31 - 90.2s - INFO - __main__ - epoch 7/20 done , tot steps 12110 , lr 4.1E-05 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2023-06-14 20:42:56,923 - 0:27:32 - 121.4s - INFO - __main__ - progress 7.578 , lr 3.9E-05 , loss 0.099 , qa loss 0.099 , lm loss 0.000 , avg batch size 4.0
2023-06-14 20:44:29,810 - 0:29:05 - 92.9s - INFO - __main__ - epoch 8/20 done , tot steps 13840 , lr 3.8E-05 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 4.0
2023-06-14 20:46:31,819 - 0:31:07 - 122.0s - INFO - __main__ - progress 8.578 , lr 3.6E-05 , loss 0.094 , qa loss 0.094 , lm loss 0.000 , avg batch size 4.0
2023-06-14 20:48:04,013 - 0:32:39 - 92.2s - INFO - __main__ - epoch 9/20 done , tot steps 15570 , lr 3.4E-05 , loss 0.09 , qa loss 0.09 , lm loss 0.00 , avg batch size 4.0
2023-06-14 20:50:06,375 - 0:34:41 - 122.4s - INFO - __main__ - progress 9.578 , lr 3.3E-05 , loss 0.088 , qa loss 0.088 , lm loss 0.000 , avg batch size 4.0
2023-06-14 20:51:37,662 - 0:36:13 - 91.3s - INFO - __main__ - epoch 10/20 done , tot steps 17300 , lr 3.1E-05 , loss 0.08 , qa loss 0.08 , lm loss 0.00 , avg batch size 4.0
2023-06-14 20:53:41,743 - 0:38:17 - 124.1s - INFO - __main__ - progress 10.578 , lr 2.9E-05 , loss 0.081 , qa loss 0.081 , lm loss 0.000 , avg batch size 4.0
2023-06-14 20:55:13,722 - 0:39:49 - 92.0s - INFO - __main__ - epoch 11/20 done , tot steps 19030 , lr 2.8E-05 , loss 0.08 , qa loss 0.08 , lm loss 0.00 , avg batch size 4.0
2023-06-14 20:57:16,062 - 0:41:51 - 122.3s - INFO - __main__ - progress 11.578 , lr 2.6E-05 , loss 0.067 , qa loss 0.067 , lm loss 0.000 , avg batch size 4.0
2023-06-14 20:58:47,834 - 0:43:23 - 91.8s - INFO - __main__ - epoch 12/20 done , tot steps 20760 , lr 2.5E-05 , loss 0.06 , qa loss 0.06 , lm loss 0.00 , avg batch size 4.0
2023-06-14 21:00:50,032 - 0:45:25 - 122.2s - INFO - __main__ - progress 12.578 , lr 2.3E-05 , loss 0.053 , qa loss 0.053 , lm loss 0.000 , avg batch size 4.0
2023-06-14 21:02:20,787 - 0:46:56 - 90.8s - INFO - __main__ - epoch 13/20 done , tot steps 22490 , lr 2.2E-05 , loss 0.05 , qa loss 0.05 , lm loss 0.00 , avg batch size 4.0
2023-06-14 21:04:21,510 - 0:48:57 - 120.7s - INFO - __main__ - progress 13.578 , lr 2.0E-05 , loss 0.047 , qa loss 0.047 , lm loss 0.000 , avg batch size 4.0
2023-06-14 21:05:51,230 - 0:50:26 - 89.7s - INFO - __main__ - epoch 14/20 done , tot steps 24220 , lr 1.9E-05 , loss 0.05 , qa loss 0.05 , lm loss 0.00 , avg batch size 4.0
2023-06-14 21:07:52,977 - 0:52:28 - 121.7s - INFO - __main__ - progress 14.578 , lr 1.7E-05 , loss 0.041 , qa loss 0.041 , lm loss 0.000 , avg batch size 4.0
2023-06-14 21:09:23,004 - 0:53:58 - 90.0s - INFO - __main__ - epoch 15/20 done , tot steps 25950 , lr 1.6E-05 , loss 0.04 , qa loss 0.04 , lm loss 0.00 , avg batch size 4.0
2023-06-14 21:11:24,149 - 0:55:59 - 121.1s - INFO - __main__ - progress 15.578 , lr 1.4E-05 , loss 0.041 , qa loss 0.041 , lm loss 0.000 , avg batch size 4.0
2023-06-14 21:12:55,828 - 0:57:31 - 91.7s - INFO - __main__ - epoch 16/20 done , tot steps 27680 , lr 1.3E-05 , loss 0.04 , qa loss 0.04 , lm loss 0.00 , avg batch size 4.0
2023-06-14 21:14:57,479 - 0:59:33 - 121.7s - INFO - __main__ - progress 16.578 , lr 1.1E-05 , loss 0.030 , qa loss 0.030 , lm loss 0.000 , avg batch size 4.0
2023-06-14 21:16:29,906 - 1:01:05 - 92.4s - INFO - __main__ - epoch 17/20 done , tot steps 29410 , lr 9.4E-06 , loss 0.04 , qa loss 0.04 , lm loss 0.00 , avg batch size 4.0
2023-06-14 21:18:31,988 - 1:03:07 - 122.1s - INFO - __main__ - progress 17.578 , lr 7.6E-06 , loss 0.031 , qa loss 0.031 , lm loss 0.000 , avg batch size 4.0
2023-06-14 21:20:04,830 - 1:04:40 - 92.8s - INFO - __main__ - epoch 18/20 done , tot steps 31140 , lr 6.3E-06 , loss 0.03 , qa loss 0.03 , lm loss 0.00 , avg batch size 4.0
2023-06-14 21:22:07,756 - 1:06:43 - 122.9s - INFO - __main__ - progress 18.578 , lr 4.5E-06 , loss 0.029 , qa loss 0.029 , lm loss 0.000 , avg batch size 4.0
2023-06-14 21:23:37,812 - 1:08:13 - 90.1s - INFO - __main__ - epoch 19/20 done , tot steps 32870 , lr 3.1E-06 , loss 0.03 , qa loss 0.03 , lm loss 0.00 , avg batch size 4.0
2023-06-14 21:25:37,701 - 1:10:13 - 119.9s - INFO - __main__ - progress 19.578 , lr 1.3E-06 , loss 0.024 , qa loss 0.024 , lm loss 0.000 , avg batch size 4.0
2023-06-14 21:27:09,347 - 1:11:44 - 91.6s - INFO - __main__ - epoch 20/20 done , tot steps 34600 , lr 1.5E-08 , loss 0.03 , qa loss 0.03 , lm loss 0.00 , avg batch size 4.0
2023-06-14 21:27:20,999 - 1:11:56 - 11.7s - INFO - __main__ - start to train { task: ['srl'], seq train type: lll }
2023-06-14 21:27:25,216 - 1:12:00 - 4.2s - INFO - utils - writing extra data in ../../model_c10/gpt2/lll/sst_srl_woz.en_0.0/sst/lm.csv ...
2023-06-14 21:27:25,274 - 1:12:00 - 0.1s - INFO - __main__ - extra training data size: 0
2023-06-14 21:27:25,583 - 1:12:01 - 0.3s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[sst]
The task with which model is saved sst
2023-06-14 21:27:27,996 - 1:12:03 - 2.4s - INFO - __main__ - len of train dataset: 6414 , max train batch size 4 , num of opt steps: 128280
2023-06-14 21:30:28,045 - 1:15:03 - 180.0s - INFO - __main__ - progress 0.624 , lr 6.1E-05 , loss 2.835 , qa loss 2.835 , lm loss 0.000 , avg batch size 4.0
2023-06-14 21:32:35,168 - 1:17:10 - 127.1s - INFO - __main__ - epoch 1/20 done , tot steps 1604 , lr 5.9E-05 , loss 2.10 , qa loss 2.10 , lm loss 0.00 , avg batch size 4.0
2023-06-14 21:35:36,445 - 1:20:12 - 181.3s - INFO - __main__ - progress 1.624 , lr 5.7E-05 , loss 0.742 , qa loss 0.742 , lm loss 0.000 , avg batch size 4.0
2023-06-14 21:37:32,866 - 1:22:08 - 116.4s - INFO - __main__ - epoch 2/20 done , tot steps 3208 , lr 5.6E-05 , loss 0.74 , qa loss 0.74 , lm loss 0.00 , avg batch size 4.0
2023-06-14 21:40:32,884 - 1:25:08 - 180.0s - INFO - __main__ - progress 2.624 , lr 5.4E-05 , loss 0.626 , qa loss 0.626 , lm loss 0.000 , avg batch size 4.0
2023-06-14 21:42:28,442 - 1:27:04 - 115.6s - INFO - __main__ - epoch 3/20 done , tot steps 4812 , lr 5.3E-05 , loss 0.61 , qa loss 0.61 , lm loss 0.00 , avg batch size 4.0
2023-06-14 21:45:30,413 - 1:30:06 - 182.0s - INFO - __main__ - progress 3.624 , lr 5.1E-05 , loss 0.522 , qa loss 0.522 , lm loss 0.000 , avg batch size 4.0
2023-06-14 21:47:22,340 - 1:31:57 - 111.9s - INFO - __main__ - epoch 4/20 done , tot steps 6416 , lr 5.0E-05 , loss 0.51 , qa loss 0.51 , lm loss 0.00 , avg batch size 4.0
2023-06-14 21:50:24,077 - 1:34:59 - 181.7s - INFO - __main__ - progress 4.624 , lr 4.8E-05 , loss 0.451 , qa loss 0.451 , lm loss 0.000 , avg batch size 4.0
2023-06-14 21:52:15,511 - 1:36:51 - 111.4s - INFO - __main__ - epoch 5/20 done , tot steps 8020 , lr 4.7E-05 , loss 0.45 , qa loss 0.45 , lm loss 0.00 , avg batch size 4.0
2023-06-14 21:55:20,195 - 1:39:55 - 184.7s - INFO - __main__ - progress 5.624 , lr 4.5E-05 , loss 0.408 , qa loss 0.408 , lm loss 0.000 , avg batch size 4.0
2023-06-14 21:57:09,597 - 1:41:45 - 109.4s - INFO - __main__ - epoch 6/20 done , tot steps 9624 , lr 4.4E-05 , loss 0.41 , qa loss 0.41 , lm loss 0.00 , avg batch size 4.0
2023-06-14 22:00:11,785 - 1:44:47 - 182.2s - INFO - __main__ - progress 6.624 , lr 4.2E-05 , loss 0.374 , qa loss 0.374 , lm loss 0.000 , avg batch size 4.0
2023-06-14 22:02:03,353 - 1:46:38 - 111.6s - INFO - __main__ - epoch 7/20 done , tot steps 11228 , lr 4.1E-05 , loss 0.38 , qa loss 0.38 , lm loss 0.00 , avg batch size 4.0
2023-06-14 22:05:05,488 - 1:49:41 - 182.1s - INFO - __main__ - progress 7.624 , lr 3.9E-05 , loss 0.344 , qa loss 0.344 , lm loss 0.000 , avg batch size 4.0
2023-06-14 22:06:56,473 - 1:51:32 - 111.0s - INFO - __main__ - epoch 8/20 done , tot steps 12832 , lr 3.8E-05 , loss 0.34 , qa loss 0.34 , lm loss 0.00 , avg batch size 4.0
2023-06-14 22:10:01,826 - 1:54:37 - 185.4s - INFO - __main__ - progress 8.624 , lr 3.6E-05 , loss 0.322 , qa loss 0.322 , lm loss 0.000 , avg batch size 4.0
2023-06-14 22:11:53,878 - 1:56:29 - 112.1s - INFO - __main__ - epoch 9/20 done , tot steps 14436 , lr 3.4E-05 , loss 0.32 , qa loss 0.32 , lm loss 0.00 , avg batch size 4.0
2023-06-14 22:14:52,983 - 1:59:28 - 179.1s - INFO - __main__ - progress 9.624 , lr 3.2E-05 , loss 0.288 , qa loss 0.288 , lm loss 0.000 , avg batch size 4.0
2023-06-14 22:16:47,251 - 2:01:22 - 114.3s - INFO - __main__ - epoch 10/20 done , tot steps 16040 , lr 3.1E-05 , loss 0.29 , qa loss 0.29 , lm loss 0.00 , avg batch size 4.0
2023-06-14 22:19:51,160 - 2:04:26 - 183.9s - INFO - __main__ - progress 10.624 , lr 2.9E-05 , loss 0.264 , qa loss 0.264 , lm loss 0.000 , avg batch size 4.0
2023-06-14 22:21:42,361 - 2:06:17 - 111.2s - INFO - __main__ - epoch 11/20 done , tot steps 17644 , lr 2.8E-05 , loss 0.27 , qa loss 0.27 , lm loss 0.00 , avg batch size 4.0
2023-06-14 22:24:41,700 - 2:09:17 - 179.3s - INFO - __main__ - progress 11.624 , lr 2.6E-05 , loss 0.258 , qa loss 0.258 , lm loss 0.000 , avg batch size 4.0
2023-06-14 22:26:35,270 - 2:11:10 - 113.6s - INFO - __main__ - epoch 12/20 done , tot steps 19248 , lr 2.5E-05 , loss 0.26 , qa loss 0.26 , lm loss 0.00 , avg batch size 4.0
2023-06-14 22:29:35,995 - 2:14:11 - 180.7s - INFO - __main__ - progress 12.624 , lr 2.3E-05 , loss 0.228 , qa loss 0.228 , lm loss 0.000 , avg batch size 4.0
2023-06-14 22:31:30,229 - 2:16:05 - 114.2s - INFO - __main__ - epoch 13/20 done , tot steps 20852 , lr 2.2E-05 , loss 0.23 , qa loss 0.23 , lm loss 0.00 , avg batch size 4.0
2023-06-14 22:34:34,331 - 2:19:09 - 184.1s - INFO - __main__ - progress 13.624 , lr 2.0E-05 , loss 0.211 , qa loss 0.211 , lm loss 0.000 , avg batch size 4.0
2023-06-14 22:36:25,548 - 2:21:01 - 111.2s - INFO - __main__ - epoch 14/20 done , tot steps 22456 , lr 1.9E-05 , loss 0.22 , qa loss 0.22 , lm loss 0.00 , avg batch size 4.0
2023-06-14 22:39:29,871 - 2:24:05 - 184.3s - INFO - __main__ - progress 14.624 , lr 1.7E-05 , loss 0.205 , qa loss 0.205 , lm loss 0.000 , avg batch size 4.0
2023-06-14 22:41:20,693 - 2:25:56 - 110.8s - INFO - __main__ - epoch 15/20 done , tot steps 24060 , lr 1.6E-05 , loss 0.21 , qa loss 0.21 , lm loss 0.00 , avg batch size 4.0
2023-06-14 22:44:23,804 - 2:28:59 - 183.1s - INFO - __main__ - progress 15.624 , lr 1.4E-05 , loss 0.193 , qa loss 0.193 , lm loss 0.000 , avg batch size 4.0
2023-06-14 22:46:15,296 - 2:30:50 - 111.5s - INFO - __main__ - epoch 16/20 done , tot steps 25664 , lr 1.3E-05 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-06-14 22:49:18,200 - 2:33:53 - 182.9s - INFO - __main__ - progress 16.624 , lr 1.1E-05 , loss 0.177 , qa loss 0.177 , lm loss 0.000 , avg batch size 4.0
2023-06-14 22:51:10,564 - 2:35:46 - 112.4s - INFO - __main__ - epoch 17/20 done , tot steps 27268 , lr 9.4E-06 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-06-14 22:54:12,700 - 2:38:48 - 182.1s - INFO - __main__ - progress 17.624 , lr 7.4E-06 , loss 0.181 , qa loss 0.181 , lm loss 0.000 , avg batch size 4.0
2023-06-14 22:56:06,791 - 2:40:42 - 114.1s - INFO - __main__ - epoch 18/20 done , tot steps 28872 , lr 6.3E-06 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
2023-06-14 22:59:09,276 - 2:43:44 - 182.5s - INFO - __main__ - progress 18.624 , lr 4.3E-06 , loss 0.161 , qa loss 0.161 , lm loss 0.000 , avg batch size 4.0
2023-06-14 23:01:02,794 - 2:45:38 - 113.5s - INFO - __main__ - epoch 19/20 done , tot steps 30476 , lr 3.1E-06 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
2023-06-14 23:04:03,871 - 2:48:39 - 181.1s - INFO - __main__ - progress 19.624 , lr 1.2E-06 , loss 0.157 , qa loss 0.157 , lm loss 0.000 , avg batch size 4.0
2023-06-14 23:05:58,990 - 2:50:34 - 115.1s - INFO - __main__ - epoch 20/20 done , tot steps 32080 , lr 1.6E-08 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-06-14 23:06:10,334 - 2:50:45 - 11.3s - INFO - __main__ - start to train { task: ['woz.en'], seq train type: lll }
2023-06-14 23:06:14,492 - 2:50:50 - 4.2s - INFO - utils - writing extra data in ../../model_c10/gpt2/lll/sst_srl_woz.en_0.0/srl/lm.csv ...
2023-06-14 23:06:14,525 - 2:50:50 - 0.0s - INFO - __main__ - extra training data size: 0
2023-06-14 23:06:14,941 - 2:50:50 - 0.4s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[srl]
The task with which model is saved srl
2023-06-14 23:06:18,110 - 2:50:53 - 3.2s - INFO - __main__ - len of train dataset: 2536 , max train batch size 4 , num of opt steps: 50720
2023-06-14 23:07:48,837 - 2:52:24 - 90.7s - INFO - __main__ - epoch 1/20 done , tot steps 634 , lr 5.9E-05 , loss 2.11 , qa loss 2.11 , lm loss 0.00 , avg batch size 4.0
2023-06-14 23:09:12,392 - 2:53:47 - 83.6s - INFO - __main__ - epoch 2/20 done , tot steps 1268 , lr 5.6E-05 , loss 0.44 , qa loss 0.44 , lm loss 0.00 , avg batch size 4.0
2023-06-14 23:10:34,950 - 2:55:10 - 82.6s - INFO - __main__ - epoch 3/20 done , tot steps 1902 , lr 5.3E-05 , loss 0.31 , qa loss 0.31 , lm loss 0.00 , avg batch size 4.0
2023-06-14 23:11:57,823 - 2:56:33 - 82.9s - INFO - __main__ - epoch 4/20 done , tot steps 2536 , lr 5.0E-05 , loss 0.25 , qa loss 0.25 , lm loss 0.00 , avg batch size 4.0
2023-06-14 23:13:20,701 - 2:57:56 - 82.9s - INFO - __main__ - epoch 5/20 done , tot steps 3170 , lr 4.7E-05 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-06-14 23:14:44,031 - 2:59:19 - 83.3s - INFO - __main__ - epoch 6/20 done , tot steps 3804 , lr 4.4E-05 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-06-14 23:16:05,609 - 3:00:41 - 81.6s - INFO - __main__ - epoch 7/20 done , tot steps 4438 , lr 4.1E-05 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
2023-06-14 23:17:27,863 - 3:02:03 - 82.3s - INFO - __main__ - epoch 8/20 done , tot steps 5072 , lr 3.8E-05 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-06-14 23:18:51,318 - 3:03:26 - 83.5s - INFO - __main__ - epoch 9/20 done , tot steps 5706 , lr 3.4E-05 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
2023-06-14 23:20:13,462 - 3:04:49 - 82.1s - INFO - __main__ - epoch 10/20 done , tot steps 6340 , lr 3.1E-05 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-06-14 23:21:36,161 - 3:06:11 - 82.7s - INFO - __main__ - epoch 11/20 done , tot steps 6974 , lr 2.8E-05 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2023-06-14 23:22:57,540 - 3:07:33 - 81.4s - INFO - __main__ - epoch 12/20 done , tot steps 7608 , lr 2.5E-05 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-06-14 23:24:19,855 - 3:08:55 - 82.3s - INFO - __main__ - epoch 13/20 done , tot steps 8242 , lr 2.2E-05 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-06-14 23:25:41,844 - 3:10:17 - 82.0s - INFO - __main__ - epoch 14/20 done , tot steps 8876 , lr 1.9E-05 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-06-14 23:27:04,434 - 3:11:40 - 82.6s - INFO - __main__ - epoch 15/20 done , tot steps 9510 , lr 1.6E-05 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 4.0
2023-06-14 23:28:28,031 - 3:13:03 - 83.6s - INFO - __main__ - epoch 16/20 done , tot steps 10144 , lr 1.3E-05 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 4.0
2023-06-14 23:29:49,867 - 3:14:25 - 81.8s - INFO - __main__ - epoch 17/20 done , tot steps 10778 , lr 9.4E-06 , loss 0.09 , qa loss 0.09 , lm loss 0.00 , avg batch size 4.0
2023-06-14 23:31:11,722 - 3:15:47 - 81.9s - INFO - __main__ - epoch 18/20 done , tot steps 11412 , lr 6.3E-06 , loss 0.09 , qa loss 0.09 , lm loss 0.00 , avg batch size 4.0
2023-06-14 23:32:33,087 - 3:17:08 - 81.4s - INFO - __main__ - epoch 19/20 done , tot steps 12046 , lr 3.1E-06 , loss 0.08 , qa loss 0.08 , lm loss 0.00 , avg batch size 4.0
2023-06-14 23:33:56,183 - 3:18:31 - 83.1s - INFO - __main__ - epoch 20/20 done , tot steps 12680 , lr 1.5E-08 , loss 0.08 , qa loss 0.08 , lm loss 0.00 , avg batch size 4.0
The current active adapter is Stack[woz_en]
The task with which model is saved woz_en
Wall Execution time: 03:18:40
CPU Execution time: 03:19:30
