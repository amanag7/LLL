Not all gpus support fp16 training! Will use fp32 instead.
wandb: Currently logged in as: sprshag (cl-nlp). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.15.4
wandb: Run data is saved locally in /u/student/2021/cs21mtech11006/LLL/lamol_adapter_prompt/wandb/run-20230629_105524-tqx3p2yj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run test run
wandb: ‚≠êÔ∏è View project at https://wandb.ai/cl-nlp/cl-nlp
wandb: üöÄ View run at https://wandb.ai/cl-nlp/cl-nlp/runs/tqx3p2yj
2023-06-29 10:55:34,915 - 0:00:41 - 0.0s - INFO - __main__ - args = Namespace(REG_TYPE_KEYS=['mas', 'ewc'], adam_epsilon=0.0001, add_task_tokens=False, data_dir='../../data', debug=False, decay_style='linear', device_ids=[0, 1], dynamic_epochs=False, fp32=True, gen_lm_sample_percentage=0.0, learning_rate=6.25e-05, lm_lambda=0.0, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[38915.52, 42158.48], min_batch_size=4, min_n_steps=1500, model_dir_root='../../model_ccmm1/gpt2/lll/sst_srl_woz.en_0.0', model_name='gpt2', n_gpus=2, n_train_epochs={'sst': 20, 'srl': 20, 'woz.en': 20}, n_warmup_ratio=0.005, n_workers=75, qp_margin=0.5, real_sample=False, reg_lambda=1.0, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[13620, 14755], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[13620, 14755], unbound=0, use_sep=False, weight_decay=0.01)
2023-06-29 10:55:34,916 - 0:00:41 - 0.0s - INFO - __main__ - start to train { task: ['sst'], seq train type: lll }
2023-06-29 10:55:34,923 - 0:00:41 - 0.0s - INFO - __main__ - extra training data size: 0
2023-06-29 10:55:44,119 - 0:00:50 - 9.2s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
2023-06-29 10:55:47,262 - 0:00:53 - 3.1s - INFO - __main__ - len of train dataset: 6920 , max train batch size 4 , num of opt steps: 138400
/home/student/2021/cs21mtech11006/anaconda3/envs/lamol/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/home/student/2021/cs21mtech11006/anaconda3/envs/lamol/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
2023-06-29 10:57:31,596 - 0:02:38 - 104.3s - INFO - __main__ - progress 0.578 , lr 6.1E-05 , loss 2.075 , qa loss 2.075 , lm loss 0.000 , avg batch size 4.0
2023-06-29 10:58:42,229 - 0:03:48 - 70.6s - INFO - __main__ - epoch 1/20 done , tot steps 1730 , lr 5.9E-05 , loss 1.33 , qa loss 1.33 , lm loss 0.00 , avg batch size 4.0
2023-06-29 11:00:18,053 - 0:05:24 - 95.8s - INFO - __main__ - progress 1.578 , lr 5.8E-05 , loss 0.267 , qa loss 0.267 , lm loss 0.000 , avg batch size 4.0
2023-06-29 11:01:28,455 - 0:06:34 - 70.4s - INFO - __main__ - epoch 2/20 done , tot steps 3460 , lr 5.6E-05 , loss 0.27 , qa loss 0.27 , lm loss 0.00 , avg batch size 4.0
2023-06-29 11:03:04,313 - 0:08:10 - 95.9s - INFO - __main__ - progress 2.578 , lr 5.4E-05 , loss 0.216 , qa loss 0.216 , lm loss 0.000 , avg batch size 4.0
2023-06-29 11:04:14,719 - 0:09:21 - 70.4s - INFO - __main__ - epoch 3/20 done , tot steps 5190 , lr 5.3E-05 , loss 0.21 , qa loss 0.21 , lm loss 0.00 , avg batch size 4.0
2023-06-29 11:05:51,442 - 0:10:57 - 96.7s - INFO - __main__ - progress 3.578 , lr 5.1E-05 , loss 0.181 , qa loss 0.181 , lm loss 0.000 , avg batch size 4.0
2023-06-29 11:07:02,028 - 0:12:08 - 70.6s - INFO - __main__ - epoch 4/20 done , tot steps 6920 , lr 5.0E-05 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-06-29 11:08:38,582 - 0:13:45 - 96.6s - INFO - __main__ - progress 4.578 , lr 4.8E-05 , loss 0.162 , qa loss 0.162 , lm loss 0.000 , avg batch size 4.0
2023-06-29 11:09:48,556 - 0:14:55 - 70.0s - INFO - __main__ - epoch 5/20 done , tot steps 8650 , lr 4.7E-05 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
2023-06-29 11:11:25,793 - 0:16:32 - 97.2s - INFO - __main__ - progress 5.578 , lr 4.5E-05 , loss 0.163 , qa loss 0.163 , lm loss 0.000 , avg batch size 4.0
2023-06-29 11:12:36,699 - 0:17:43 - 70.9s - INFO - __main__ - epoch 6/20 done , tot steps 10380 , lr 4.4E-05 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-06-29 11:14:13,444 - 0:19:19 - 96.7s - INFO - __main__ - progress 6.578 , lr 4.2E-05 , loss 0.149 , qa loss 0.149 , lm loss 0.000 , avg batch size 4.0
2023-06-29 11:15:23,376 - 0:20:29 - 69.9s - INFO - __main__ - epoch 7/20 done , tot steps 12110 , lr 4.1E-05 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-06-29 11:16:59,991 - 0:22:06 - 96.6s - INFO - __main__ - progress 7.578 , lr 3.9E-05 , loss 0.136 , qa loss 0.136 , lm loss 0.000 , avg batch size 4.0
2023-06-29 11:18:10,259 - 0:23:16 - 70.3s - INFO - __main__ - epoch 8/20 done , tot steps 13840 , lr 3.8E-05 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
2023-06-29 11:19:47,878 - 0:24:54 - 97.6s - INFO - __main__ - progress 8.578 , lr 3.6E-05 , loss 0.128 , qa loss 0.128 , lm loss 0.000 , avg batch size 4.0
2023-06-29 11:20:57,768 - 0:26:04 - 69.9s - INFO - __main__ - epoch 9/20 done , tot steps 15570 , lr 3.4E-05 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-06-29 11:22:34,038 - 0:27:40 - 96.3s - INFO - __main__ - progress 9.578 , lr 3.3E-05 , loss 0.119 , qa loss 0.119 , lm loss 0.000 , avg batch size 4.0
2023-06-29 11:23:44,662 - 0:28:51 - 70.6s - INFO - __main__ - epoch 10/20 done , tot steps 17300 , lr 3.1E-05 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2023-06-29 11:25:21,407 - 0:30:27 - 96.7s - INFO - __main__ - progress 10.578 , lr 2.9E-05 , loss 0.113 , qa loss 0.113 , lm loss 0.000 , avg batch size 4.0
2023-06-29 11:26:31,336 - 0:31:37 - 69.9s - INFO - __main__ - epoch 11/20 done , tot steps 19030 , lr 2.8E-05 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-06-29 11:28:08,405 - 0:33:14 - 97.1s - INFO - __main__ - progress 11.578 , lr 2.6E-05 , loss 0.105 , qa loss 0.105 , lm loss 0.000 , avg batch size 4.0
2023-06-29 11:29:18,508 - 0:34:25 - 70.1s - INFO - __main__ - epoch 12/20 done , tot steps 20760 , lr 2.5E-05 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-06-29 11:30:55,647 - 0:36:02 - 97.1s - INFO - __main__ - progress 12.578 , lr 2.3E-05 , loss 0.111 , qa loss 0.111 , lm loss 0.000 , avg batch size 4.0
2023-06-29 11:32:06,543 - 0:37:13 - 70.9s - INFO - __main__ - epoch 13/20 done , tot steps 22490 , lr 2.2E-05 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 4.0
2023-06-29 11:33:43,686 - 0:38:50 - 97.1s - INFO - __main__ - progress 13.578 , lr 2.0E-05 , loss 0.098 , qa loss 0.098 , lm loss 0.000 , avg batch size 4.0
2023-06-29 11:34:53,957 - 0:40:00 - 70.3s - INFO - __main__ - epoch 14/20 done , tot steps 24220 , lr 1.9E-05 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 4.0
2023-06-29 11:36:29,349 - 0:41:35 - 95.4s - INFO - __main__ - progress 14.578 , lr 1.7E-05 , loss 0.087 , qa loss 0.087 , lm loss 0.000 , avg batch size 4.0
2023-06-29 11:37:39,733 - 0:42:46 - 70.4s - INFO - __main__ - epoch 15/20 done , tot steps 25950 , lr 1.6E-05 , loss 0.09 , qa loss 0.09 , lm loss 0.00 , avg batch size 4.0
2023-06-29 11:39:15,832 - 0:44:22 - 96.1s - INFO - __main__ - progress 15.578 , lr 1.4E-05 , loss 0.093 , qa loss 0.093 , lm loss 0.000 , avg batch size 4.0
2023-06-29 11:40:26,050 - 0:45:32 - 70.2s - INFO - __main__ - epoch 16/20 done , tot steps 27680 , lr 1.3E-05 , loss 0.09 , qa loss 0.09 , lm loss 0.00 , avg batch size 4.0
2023-06-29 11:42:03,212 - 0:47:09 - 97.2s - INFO - __main__ - progress 16.578 , lr 1.1E-05 , loss 0.088 , qa loss 0.088 , lm loss 0.000 , avg batch size 4.0
2023-06-29 11:43:12,777 - 0:48:19 - 69.6s - INFO - __main__ - epoch 17/20 done , tot steps 29410 , lr 9.4E-06 , loss 0.08 , qa loss 0.08 , lm loss 0.00 , avg batch size 4.0
2023-06-29 11:44:48,840 - 0:49:55 - 96.1s - INFO - __main__ - progress 17.578 , lr 7.6E-06 , loss 0.081 , qa loss 0.081 , lm loss 0.000 , avg batch size 4.0
2023-06-29 11:45:59,430 - 0:51:05 - 70.6s - INFO - __main__ - epoch 18/20 done , tot steps 31140 , lr 6.3E-06 , loss 0.08 , qa loss 0.08 , lm loss 0.00 , avg batch size 4.0
2023-06-29 11:47:36,226 - 0:52:42 - 96.8s - INFO - __main__ - progress 18.578 , lr 4.5E-06 , loss 0.078 , qa loss 0.078 , lm loss 0.000 , avg batch size 4.0
2023-06-29 11:48:46,820 - 0:53:53 - 70.6s - INFO - __main__ - epoch 19/20 done , tot steps 32870 , lr 3.1E-06 , loss 0.08 , qa loss 0.08 , lm loss 0.00 , avg batch size 4.0
2023-06-29 11:50:24,029 - 0:55:30 - 97.2s - INFO - __main__ - progress 19.578 , lr 1.3E-06 , loss 0.075 , qa loss 0.075 , lm loss 0.000 , avg batch size 4.0
2023-06-29 11:51:41,549 - 0:56:48 - 77.5s - INFO - __main__ - epoch 20/20 done , tot steps 34600 , lr 1.5E-08 , loss 0.07 , qa loss 0.07 , lm loss 0.00 , avg batch size 4.0
2023-06-29 11:51:50,536 - 0:56:57 - 9.0s - INFO - __main__ - start to train { task: ['srl'], seq train type: lll }
2023-06-29 11:51:54,722 - 0:57:01 - 4.2s - INFO - utils - writing extra data in ../../model_ccmm1/gpt2/lll/sst_srl_woz.en_0.0/sst/lm.csv ...
2023-06-29 11:51:54,768 - 0:57:01 - 0.0s - INFO - __main__ - extra training data size: 0
2023-06-29 11:51:55,015 - 0:57:01 - 0.2s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[sst]
The task with which model is saved sst
2023-06-29 11:52:00,228 - 0:57:06 - 5.2s - INFO - __main__ - len of train dataset: 6414 , max train batch size 4 , num of opt steps: 128280
2023-06-29 11:54:40,110 - 0:59:46 - 159.9s - INFO - __main__ - progress 0.624 , lr 6.1E-05 , loss 3.138 , qa loss 3.138 , lm loss 0.000 , avg batch size 4.0
2023-06-29 11:56:16,884 - 1:01:23 - 96.8s - INFO - __main__ - epoch 1/20 done , tot steps 1604 , lr 5.9E-05 , loss 2.34 , qa loss 2.34 , lm loss 0.00 , avg batch size 4.0
2023-06-29 11:58:55,015 - 1:04:01 - 158.1s - INFO - __main__ - progress 1.624 , lr 5.7E-05 , loss 0.902 , qa loss 0.902 , lm loss 0.000 , avg batch size 4.0
2023-06-29 12:00:32,737 - 1:05:39 - 97.7s - INFO - __main__ - epoch 2/20 done , tot steps 3208 , lr 5.6E-05 , loss 0.87 , qa loss 0.87 , lm loss 0.00 , avg batch size 4.0
2023-06-29 12:03:15,619 - 1:08:22 - 162.9s - INFO - __main__ - progress 2.624 , lr 5.4E-05 , loss 0.734 , qa loss 0.734 , lm loss 0.000 , avg batch size 4.0
2023-06-29 12:04:51,638 - 1:09:58 - 96.0s - INFO - __main__ - epoch 3/20 done , tot steps 4812 , lr 5.3E-05 , loss 0.71 , qa loss 0.71 , lm loss 0.00 , avg batch size 4.0
2023-06-29 12:07:33,561 - 1:12:40 - 161.9s - INFO - __main__ - progress 3.624 , lr 5.1E-05 , loss 0.643 , qa loss 0.643 , lm loss 0.000 , avg batch size 4.0
2023-06-29 12:09:09,309 - 1:14:15 - 95.7s - INFO - __main__ - epoch 4/20 done , tot steps 6416 , lr 5.0E-05 , loss 0.65 , qa loss 0.65 , lm loss 0.00 , avg batch size 4.0
2023-06-29 12:11:50,389 - 1:16:56 - 161.1s - INFO - __main__ - progress 4.624 , lr 4.8E-05 , loss 0.600 , qa loss 0.600 , lm loss 0.000 , avg batch size 4.0
2023-06-29 12:13:24,913 - 1:18:31 - 94.5s - INFO - __main__ - epoch 5/20 done , tot steps 8020 , lr 4.7E-05 , loss 0.59 , qa loss 0.59 , lm loss 0.00 , avg batch size 4.0
2023-06-29 12:16:06,352 - 1:21:12 - 161.4s - INFO - __main__ - progress 5.624 , lr 4.5E-05 , loss 0.565 , qa loss 0.565 , lm loss 0.000 , avg batch size 4.0
2023-06-29 12:17:40,815 - 1:22:47 - 94.5s - INFO - __main__ - epoch 6/20 done , tot steps 9624 , lr 4.4E-05 , loss 0.56 , qa loss 0.56 , lm loss 0.00 , avg batch size 4.0
2023-06-29 12:20:22,767 - 1:25:29 - 162.0s - INFO - __main__ - progress 6.624 , lr 4.2E-05 , loss 0.512 , qa loss 0.512 , lm loss 0.000 , avg batch size 4.0
2023-06-29 12:21:57,408 - 1:27:03 - 94.6s - INFO - __main__ - epoch 7/20 done , tot steps 11228 , lr 4.1E-05 , loss 0.51 , qa loss 0.51 , lm loss 0.00 , avg batch size 4.0
2023-06-29 12:24:40,468 - 1:29:46 - 163.1s - INFO - __main__ - progress 7.624 , lr 3.9E-05 , loss 0.503 , qa loss 0.503 , lm loss 0.000 , avg batch size 4.0
2023-06-29 12:26:16,714 - 1:31:23 - 96.2s - INFO - __main__ - epoch 8/20 done , tot steps 12832 , lr 3.8E-05 , loss 0.50 , qa loss 0.50 , lm loss 0.00 , avg batch size 4.0
2023-06-29 12:28:59,821 - 1:34:06 - 163.1s - INFO - __main__ - progress 8.624 , lr 3.6E-05 , loss 0.465 , qa loss 0.465 , lm loss 0.000 , avg batch size 4.0
2023-06-29 12:30:34,956 - 1:35:41 - 95.1s - INFO - __main__ - epoch 9/20 done , tot steps 14436 , lr 3.4E-05 , loss 0.46 , qa loss 0.46 , lm loss 0.00 , avg batch size 4.0
2023-06-29 12:33:15,691 - 1:38:22 - 160.7s - INFO - __main__ - progress 9.624 , lr 3.2E-05 , loss 0.447 , qa loss 0.447 , lm loss 0.000 , avg batch size 4.0
2023-06-29 12:34:52,528 - 1:39:59 - 96.8s - INFO - __main__ - epoch 10/20 done , tot steps 16040 , lr 3.1E-05 , loss 0.44 , qa loss 0.44 , lm loss 0.00 , avg batch size 4.0
2023-06-29 12:37:35,225 - 1:42:41 - 162.7s - INFO - __main__ - progress 10.624 , lr 2.9E-05 , loss 0.417 , qa loss 0.417 , lm loss 0.000 , avg batch size 4.0
2023-06-29 12:39:08,214 - 1:44:14 - 93.0s - INFO - __main__ - epoch 11/20 done , tot steps 17644 , lr 2.8E-05 , loss 0.42 , qa loss 0.42 , lm loss 0.00 , avg batch size 4.0
2023-06-29 12:41:47,724 - 1:46:54 - 159.5s - INFO - __main__ - progress 11.624 , lr 2.6E-05 , loss 0.390 , qa loss 0.390 , lm loss 0.000 , avg batch size 4.0
2023-06-29 12:43:24,609 - 1:48:31 - 96.9s - INFO - __main__ - epoch 12/20 done , tot steps 19248 , lr 2.5E-05 , loss 0.40 , qa loss 0.40 , lm loss 0.00 , avg batch size 4.0
2023-06-29 12:46:04,603 - 1:51:11 - 160.0s - INFO - __main__ - progress 12.624 , lr 2.3E-05 , loss 0.393 , qa loss 0.393 , lm loss 0.000 , avg batch size 4.0
2023-06-29 12:47:42,955 - 1:52:49 - 98.4s - INFO - __main__ - epoch 13/20 done , tot steps 20852 , lr 2.2E-05 , loss 0.40 , qa loss 0.40 , lm loss 0.00 , avg batch size 4.0
2023-06-29 12:50:25,000 - 1:55:31 - 162.0s - INFO - __main__ - progress 13.624 , lr 2.0E-05 , loss 0.370 , qa loss 0.370 , lm loss 0.000 , avg batch size 4.0
2023-06-29 12:52:00,774 - 1:57:07 - 95.8s - INFO - __main__ - epoch 14/20 done , tot steps 22456 , lr 1.9E-05 , loss 0.37 , qa loss 0.37 , lm loss 0.00 , avg batch size 4.0
2023-06-29 12:54:39,654 - 1:59:46 - 158.9s - INFO - __main__ - progress 14.624 , lr 1.7E-05 , loss 0.369 , qa loss 0.369 , lm loss 0.000 , avg batch size 4.0
2023-06-29 12:56:15,415 - 2:01:21 - 95.8s - INFO - __main__ - epoch 15/20 done , tot steps 24060 , lr 1.6E-05 , loss 0.37 , qa loss 0.37 , lm loss 0.00 , avg batch size 4.0
2023-06-29 12:58:56,949 - 2:04:03 - 161.5s - INFO - __main__ - progress 15.624 , lr 1.4E-05 , loss 0.353 , qa loss 0.353 , lm loss 0.000 , avg batch size 4.0
2023-06-29 13:00:31,089 - 2:05:37 - 94.1s - INFO - __main__ - epoch 16/20 done , tot steps 25664 , lr 1.3E-05 , loss 0.36 , qa loss 0.36 , lm loss 0.00 , avg batch size 4.0
2023-06-29 13:03:11,149 - 2:08:17 - 160.1s - INFO - __main__ - progress 16.624 , lr 1.1E-05 , loss 0.351 , qa loss 0.351 , lm loss 0.000 , avg batch size 4.0
2023-06-29 13:04:46,243 - 2:09:52 - 95.1s - INFO - __main__ - epoch 17/20 done , tot steps 27268 , lr 9.4E-06 , loss 0.35 , qa loss 0.35 , lm loss 0.00 , avg batch size 4.0
2023-06-29 13:07:27,598 - 2:12:34 - 161.4s - INFO - __main__ - progress 17.624 , lr 7.4E-06 , loss 0.338 , qa loss 0.338 , lm loss 0.000 , avg batch size 4.0
2023-06-29 13:09:05,236 - 2:14:11 - 97.6s - INFO - __main__ - epoch 18/20 done , tot steps 28872 , lr 6.3E-06 , loss 0.34 , qa loss 0.34 , lm loss 0.00 , avg batch size 4.0
2023-06-29 13:11:46,523 - 2:16:53 - 161.3s - INFO - __main__ - progress 18.624 , lr 4.3E-06 , loss 0.334 , qa loss 0.334 , lm loss 0.000 , avg batch size 4.0
2023-06-29 13:13:22,636 - 2:18:29 - 96.1s - INFO - __main__ - epoch 19/20 done , tot steps 30476 , lr 3.1E-06 , loss 0.34 , qa loss 0.34 , lm loss 0.00 , avg batch size 4.0
2023-06-29 13:16:01,194 - 2:21:07 - 158.6s - INFO - __main__ - progress 19.624 , lr 1.2E-06 , loss 0.323 , qa loss 0.323 , lm loss 0.000 , avg batch size 4.0
2023-06-29 13:17:42,985 - 2:22:49 - 101.8s - INFO - __main__ - epoch 20/20 done , tot steps 32080 , lr 1.6E-08 , loss 0.33 , qa loss 0.33 , lm loss 0.00 , avg batch size 4.0
2023-06-29 13:17:51,782 - 2:22:58 - 8.8s - INFO - __main__ - start to train { task: ['woz.en'], seq train type: lll }
2023-06-29 13:17:54,460 - 2:23:00 - 2.7s - INFO - utils - writing extra data in ../../model_ccmm1/gpt2/lll/sst_srl_woz.en_0.0/srl/lm.csv ...
2023-06-29 13:17:54,518 - 2:23:01 - 0.1s - INFO - __main__ - extra training data size: 0
2023-06-29 13:17:54,762 - 2:23:01 - 0.2s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[srl]
The task with which model is saved srl
2023-06-29 13:17:59,197 - 2:23:05 - 4.4s - INFO - __main__ - len of train dataset: 2536 , max train batch size 4 , num of opt steps: 50720
2023-06-29 13:19:03,830 - 2:24:10 - 64.6s - INFO - __main__ - epoch 1/20 done , tot steps 634 , lr 5.9E-05 , loss 3.30 , qa loss 3.30 , lm loss 0.00 , avg batch size 4.0
2023-06-29 13:20:09,002 - 2:25:15 - 65.2s - INFO - __main__ - epoch 2/20 done , tot steps 1268 , lr 5.6E-05 , loss 0.58 , qa loss 0.58 , lm loss 0.00 , avg batch size 4.0
2023-06-29 13:21:13,899 - 2:26:20 - 64.9s - INFO - __main__ - epoch 3/20 done , tot steps 1902 , lr 5.3E-05 , loss 0.42 , qa loss 0.42 , lm loss 0.00 , avg batch size 4.0
2023-06-29 13:22:18,821 - 2:27:25 - 64.9s - INFO - __main__ - epoch 4/20 done , tot steps 2536 , lr 5.0E-05 , loss 0.35 , qa loss 0.35 , lm loss 0.00 , avg batch size 4.0
2023-06-29 13:23:23,626 - 2:28:30 - 64.8s - INFO - __main__ - epoch 5/20 done , tot steps 3170 , lr 4.7E-05 , loss 0.28 , qa loss 0.28 , lm loss 0.00 , avg batch size 4.0
2023-06-29 13:24:28,271 - 2:29:34 - 64.6s - INFO - __main__ - epoch 6/20 done , tot steps 3804 , lr 4.4E-05 , loss 0.25 , qa loss 0.25 , lm loss 0.00 , avg batch size 4.0
2023-06-29 13:25:33,295 - 2:30:39 - 65.0s - INFO - __main__ - epoch 7/20 done , tot steps 4438 , lr 4.1E-05 , loss 0.23 , qa loss 0.23 , lm loss 0.00 , avg batch size 4.0
2023-06-29 13:26:37,509 - 2:31:44 - 64.2s - INFO - __main__ - epoch 8/20 done , tot steps 5072 , lr 3.8E-05 , loss 0.22 , qa loss 0.22 , lm loss 0.00 , avg batch size 4.0
2023-06-29 13:27:42,905 - 2:32:49 - 65.4s - INFO - __main__ - epoch 9/20 done , tot steps 5706 , lr 3.4E-05 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-06-29 13:28:47,103 - 2:33:53 - 64.2s - INFO - __main__ - epoch 10/20 done , tot steps 6340 , lr 3.1E-05 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-06-29 13:29:51,902 - 2:34:58 - 64.8s - INFO - __main__ - epoch 11/20 done , tot steps 6974 , lr 2.8E-05 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
2023-06-29 13:30:57,055 - 2:36:03 - 65.2s - INFO - __main__ - epoch 12/20 done , tot steps 7608 , lr 2.5E-05 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-06-29 13:32:01,851 - 2:37:08 - 64.8s - INFO - __main__ - epoch 13/20 done , tot steps 8242 , lr 2.2E-05 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-06-29 13:33:06,939 - 2:38:13 - 65.1s - INFO - __main__ - epoch 14/20 done , tot steps 8876 , lr 1.9E-05 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-06-29 13:34:12,718 - 2:39:19 - 65.8s - INFO - __main__ - epoch 15/20 done , tot steps 9510 , lr 1.6E-05 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-06-29 13:35:18,216 - 2:40:24 - 65.5s - INFO - __main__ - epoch 16/20 done , tot steps 10144 , lr 1.3E-05 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
2023-06-29 13:36:23,543 - 2:41:30 - 65.3s - INFO - __main__ - epoch 17/20 done , tot steps 10778 , lr 9.4E-06 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
2023-06-29 13:37:28,335 - 2:42:34 - 64.8s - INFO - __main__ - epoch 18/20 done , tot steps 11412 , lr 6.3E-06 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-06-29 13:38:32,873 - 2:43:39 - 64.5s - INFO - __main__ - epoch 19/20 done , tot steps 12046 , lr 3.1E-06 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-06-29 13:39:44,239 - 2:44:50 - 71.4s - INFO - __main__ - epoch 20/20 done , tot steps 12680 , lr 1.5E-08 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
The current active adapter is Stack[woz_en]
The task with which model is saved woz_en
Wall Execution time: 02:44:17
CPU Execution time: 02:45:26
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:      Epoch ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà
wandb: Train Loss ‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñà‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb:      Epoch 20
wandb: Train Loss 0.13085
wandb: 
wandb: üöÄ View run test run at: https://wandb.ai/cl-nlp/cl-nlp/runs/tqx3p2yj
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230629_105524-tqx3p2yj/logs
