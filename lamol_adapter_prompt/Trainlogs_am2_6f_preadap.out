Not all gpus support fp16 training! Will use fp32 instead.
2023-06-14 02:34:50,097 - 0:10:44 - 0.0s - INFO - __main__ - args = Namespace(REG_TYPE_KEYS=['mas', 'ewc'], adam_epsilon=0.0001, add_task_tokens=False, data_dir='../../data', debug=False, decay_style='linear', device_ids=[2, 3], dynamic_epochs=False, fp32=True, gen_lm_sample_percentage=0.0, learning_rate=6.25e-05, lm_lambda=0.0, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[38915.52, 42158.48], min_batch_size=4, min_n_steps=1500, model_dir_root='../../model_am2/gpt2/lll/wikisql_ag_amazon_sst_srl_woz.en_0.0', model_name='gpt2', n_gpus=2, n_train_epochs={'wikisql': 20, 'ag': 20, 'amazon': 20, 'sst': 20, 'srl': 20, 'woz.en': 20}, n_warmup_ratio=0.005, n_workers=75, qp_margin=0.5, real_sample=False, reg_lambda=1.0, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['wikisql', 'ag', 'amazon', 'sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[13620, 14755], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[13620, 14755], unbound=0, use_sep=False, weight_decay=0.01)
2023-06-14 02:34:50,097 - 0:10:44 - 0.0s - INFO - __main__ - start to train { task: ['wikisql'], seq train type: lll }
2023-06-14 02:34:50,121 - 0:10:44 - 0.0s - INFO - __main__ - extra training data size: 0
2023-06-14 02:34:56,746 - 0:10:50 - 6.6s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
2023-06-14 02:38:06,853 - 0:14:01 - 190.1s - INFO - __main__ - len of train dataset: 56355 , max train batch size 37 , num of opt steps: 1127100
/home/student/2021/cs21mtech11006/anaconda3/envs/lamol/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/home/student/2021/cs21mtech11006/anaconda3/envs/lamol/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
2023-06-14 02:51:04,175 - 0:26:58 - 777.3s - INFO - __main__ - progress 0.657 , lr 6.0E-05 , loss 2.080 , qa loss 2.080 , lm loss 0.000 , avg batch size 37.0
2023-06-14 02:57:56,800 - 0:33:51 - 412.6s - INFO - __main__ - epoch 1/20 done , tot steps 1524 , lr 5.9E-05 , loss 1.48 , qa loss 1.48 , lm loss 0.00 , avg batch size 37.0
2023-06-14 03:10:48,709 - 0:46:42 - 771.9s - INFO - __main__ - progress 1.656 , lr 5.7E-05 , loss 0.272 , qa loss 0.272 , lm loss 0.000 , avg batch size 37.0
2023-06-14 03:17:34,405 - 0:53:28 - 405.7s - INFO - __main__ - epoch 2/20 done , tot steps 3048 , lr 5.6E-05 , loss 0.26 , qa loss 0.26 , lm loss 0.00 , avg batch size 37.0
2023-06-14 03:30:27,525 - 1:06:21 - 773.1s - INFO - __main__ - progress 2.656 , lr 5.4E-05 , loss 0.203 , qa loss 0.203 , lm loss 0.000 , avg batch size 37.0
2023-06-14 03:37:12,446 - 1:13:06 - 404.9s - INFO - __main__ - epoch 3/20 done , tot steps 4572 , lr 5.3E-05 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 37.0
2023-06-14 03:50:05,174 - 1:25:59 - 772.7s - INFO - __main__ - progress 3.657 , lr 5.1E-05 , loss 0.170 , qa loss 0.170 , lm loss 0.000 , avg batch size 37.0
2023-06-14 03:56:52,450 - 1:32:46 - 407.3s - INFO - __main__ - epoch 4/20 done , tot steps 6096 , lr 5.0E-05 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 37.0
2023-06-14 04:09:47,059 - 1:45:41 - 774.6s - INFO - __main__ - progress 4.656 , lr 4.8E-05 , loss 0.156 , qa loss 0.156 , lm loss 0.000 , avg batch size 37.0
2023-06-14 04:16:32,640 - 1:52:26 - 405.6s - INFO - __main__ - epoch 5/20 done , tot steps 7620 , lr 4.7E-05 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 37.0
2023-06-14 04:29:27,359 - 2:05:21 - 774.7s - INFO - __main__ - progress 5.656 , lr 4.5E-05 , loss 0.143 , qa loss 0.143 , lm loss 0.000 , avg batch size 37.0
2023-06-14 04:36:12,242 - 2:12:06 - 404.9s - INFO - __main__ - epoch 6/20 done , tot steps 9144 , lr 4.4E-05 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 37.0
2023-06-14 04:49:05,640 - 2:24:59 - 773.4s - INFO - __main__ - progress 6.656 , lr 4.2E-05 , loss 0.134 , qa loss 0.134 , lm loss 0.000 , avg batch size 37.0
2023-06-14 04:55:51,588 - 2:31:45 - 405.9s - INFO - __main__ - epoch 7/20 done , tot steps 10668 , lr 4.1E-05 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 37.0
2023-06-14 05:08:43,570 - 2:44:37 - 772.0s - INFO - __main__ - progress 7.657 , lr 3.9E-05 , loss 0.126 , qa loss 0.126 , lm loss 0.000 , avg batch size 37.0
2023-06-14 05:15:29,136 - 2:51:23 - 405.6s - INFO - __main__ - epoch 8/20 done , tot steps 12192 , lr 3.8E-05 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 37.0
2023-06-14 05:28:24,216 - 3:04:18 - 775.1s - INFO - __main__ - progress 8.656 , lr 3.5E-05 , loss 0.120 , qa loss 0.120 , lm loss 0.000 , avg batch size 37.0
2023-06-14 05:35:07,388 - 3:11:01 - 403.2s - INFO - __main__ - epoch 9/20 done , tot steps 13716 , lr 3.4E-05 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 37.0
2023-06-14 05:47:59,263 - 3:23:53 - 771.9s - INFO - __main__ - progress 9.656 , lr 3.2E-05 , loss 0.114 , qa loss 0.114 , lm loss 0.000 , avg batch size 37.0
2023-06-14 05:54:43,182 - 3:30:37 - 403.9s - INFO - __main__ - epoch 10/20 done , tot steps 15240 , lr 3.1E-05 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 37.0
2023-06-14 06:07:37,443 - 3:43:31 - 774.3s - INFO - __main__ - progress 10.656 , lr 2.9E-05 , loss 0.109 , qa loss 0.109 , lm loss 0.000 , avg batch size 37.0
2023-06-14 06:14:21,817 - 3:50:16 - 404.4s - INFO - __main__ - epoch 11/20 done , tot steps 16764 , lr 2.8E-05 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 37.0
2023-06-14 06:27:16,915 - 4:03:11 - 775.1s - INFO - __main__ - progress 11.656 , lr 2.6E-05 , loss 0.105 , qa loss 0.105 , lm loss 0.000 , avg batch size 37.0
2023-06-14 06:33:59,798 - 4:09:54 - 402.9s - INFO - __main__ - epoch 12/20 done , tot steps 18288 , lr 2.5E-05 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 37.0
2023-06-14 06:46:55,918 - 4:22:50 - 776.1s - INFO - __main__ - progress 12.656 , lr 2.3E-05 , loss 0.103 , qa loss 0.103 , lm loss 0.000 , avg batch size 37.0
2023-06-14 06:53:40,496 - 4:29:34 - 404.6s - INFO - __main__ - epoch 13/20 done , tot steps 19812 , lr 2.2E-05 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 37.0
2023-06-14 07:06:33,764 - 4:42:28 - 773.3s - INFO - __main__ - progress 13.657 , lr 2.0E-05 , loss 0.103 , qa loss 0.103 , lm loss 0.000 , avg batch size 37.0
2023-06-14 07:13:20,145 - 4:49:14 - 406.4s - INFO - __main__ - epoch 14/20 done , tot steps 21336 , lr 1.9E-05 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 37.0
2023-06-14 07:26:14,608 - 5:02:08 - 774.5s - INFO - __main__ - progress 14.656 , lr 1.7E-05 , loss 0.097 , qa loss 0.097 , lm loss 0.000 , avg batch size 37.0
2023-06-14 07:32:59,726 - 5:08:53 - 405.1s - INFO - __main__ - epoch 15/20 done , tot steps 22860 , lr 1.6E-05 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 37.0
2023-06-14 07:45:56,457 - 5:21:50 - 776.7s - INFO - __main__ - progress 15.656 , lr 1.4E-05 , loss 0.098 , qa loss 0.098 , lm loss 0.000 , avg batch size 37.0
2023-06-14 07:52:40,557 - 5:28:34 - 404.1s - INFO - __main__ - epoch 16/20 done , tot steps 24384 , lr 1.3E-05 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 37.0
2023-06-14 08:05:34,225 - 5:41:28 - 773.7s - INFO - __main__ - progress 16.657 , lr 1.0E-05 , loss 0.092 , qa loss 0.092 , lm loss 0.000 , avg batch size 37.0
2023-06-14 08:12:17,550 - 5:48:11 - 403.3s - INFO - __main__ - epoch 17/20 done , tot steps 25908 , lr 9.4E-06 , loss 0.09 , qa loss 0.09 , lm loss 0.00 , avg batch size 37.0
2023-06-14 08:25:10,151 - 6:01:04 - 772.6s - INFO - __main__ - progress 17.657 , lr 7.3E-06 , loss 0.092 , qa loss 0.092 , lm loss 0.000 , avg batch size 37.0
2023-06-14 08:31:52,664 - 6:07:46 - 402.5s - INFO - __main__ - epoch 18/20 done , tot steps 27432 , lr 6.3E-06 , loss 0.09 , qa loss 0.09 , lm loss 0.00 , avg batch size 37.0
2023-06-14 08:44:47,224 - 6:20:41 - 774.6s - INFO - __main__ - progress 18.657 , lr 4.2E-06 , loss 0.091 , qa loss 0.091 , lm loss 0.000 , avg batch size 37.0
2023-06-14 08:51:31,312 - 6:27:25 - 404.1s - INFO - __main__ - epoch 19/20 done , tot steps 28956 , lr 3.1E-06 , loss 0.09 , qa loss 0.09 , lm loss 0.00 , avg batch size 37.0
2023-06-14 09:04:24,248 - 6:40:18 - 772.9s - INFO - __main__ - progress 19.656 , lr 1.1E-06 , loss 0.090 , qa loss 0.090 , lm loss 0.000 , avg batch size 37.0
2023-06-14 09:11:08,029 - 6:47:02 - 403.8s - INFO - __main__ - epoch 20/20 done , tot steps 30480 , lr 1.6E-08 , loss 0.09 , qa loss 0.09 , lm loss 0.00 , avg batch size 37.0
2023-06-14 09:11:19,349 - 6:47:13 - 11.3s - INFO - __main__ - start to train { task: ['ag'], seq train type: lll }
2023-06-14 09:11:24,560 - 6:47:18 - 5.2s - INFO - utils - writing extra data in ../../model_am2/gpt2/lll/wikisql_ag_amazon_sst_srl_woz.en_0.0/wikisql/lm.csv ...
2023-06-14 09:11:24,879 - 6:47:19 - 0.3s - INFO - __main__ - extra training data size: 0
2023-06-14 09:11:26,766 - 6:47:21 - 1.9s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[wikisql]
The task with which model is saved wikisql
2023-06-14 09:11:39,511 - 6:47:33 - 12.7s - INFO - __main__ - len of train dataset: 115000 , max train batch size 76 , num of opt steps: 2300000
2023-06-14 09:25:16,516 - 7:01:10 - 817.0s - INFO - __main__ - progress 0.648 , lr 6.0E-05 , loss 1.223 , qa loss 1.223 , lm loss 0.000 , avg batch size 74.6
2023-06-14 09:32:43,055 - 7:08:37 - 446.5s - INFO - __main__ - epoch 1/20 done , tot steps 1541 , lr 5.9E-05 , loss 0.83 , qa loss 0.83 , lm loss 0.00 , avg batch size 74.6
2023-06-14 09:46:14,227 - 7:22:08 - 811.2s - INFO - __main__ - progress 1.649 , lr 5.7E-05 , loss 0.097 , qa loss 0.097 , lm loss 0.000 , avg batch size 74.6
2023-06-14 09:53:32,158 - 7:29:26 - 437.9s - INFO - __main__ - epoch 2/20 done , tot steps 3080 , lr 5.6E-05 , loss 0.09 , qa loss 0.09 , lm loss 0.00 , avg batch size 74.7
2023-06-14 10:07:04,049 - 7:42:58 - 811.9s - INFO - __main__ - progress 2.649 , lr 5.4E-05 , loss 0.082 , qa loss 0.082 , lm loss 0.000 , avg batch size 74.7
2023-06-14 10:14:24,255 - 7:50:18 - 440.2s - INFO - __main__ - epoch 3/20 done , tot steps 4621 , lr 5.3E-05 , loss 0.08 , qa loss 0.08 , lm loss 0.00 , avg batch size 74.6
2023-06-14 10:27:55,552 - 8:03:49 - 811.3s - INFO - __main__ - progress 3.649 , lr 5.1E-05 , loss 0.076 , qa loss 0.076 , lm loss 0.000 , avg batch size 74.7
2023-06-14 10:35:14,970 - 8:11:09 - 439.4s - INFO - __main__ - epoch 4/20 done , tot steps 6163 , lr 5.0E-05 , loss 0.08 , qa loss 0.08 , lm loss 0.00 , avg batch size 74.6
2023-06-14 10:48:49,287 - 8:24:43 - 814.3s - INFO - __main__ - progress 4.649 , lr 4.8E-05 , loss 0.071 , qa loss 0.071 , lm loss 0.000 , avg batch size 74.6
2023-06-14 10:56:09,990 - 8:32:04 - 440.7s - INFO - __main__ - epoch 5/20 done , tot steps 7705 , lr 4.7E-05 , loss 0.07 , qa loss 0.07 , lm loss 0.00 , avg batch size 74.6
2023-06-14 11:09:42,997 - 8:45:37 - 813.0s - INFO - __main__ - progress 5.648 , lr 4.5E-05 , loss 0.068 , qa loss 0.068 , lm loss 0.000 , avg batch size 74.6
2023-06-14 11:17:01,726 - 8:52:55 - 438.7s - INFO - __main__ - epoch 6/20 done , tot steps 9246 , lr 4.4E-05 , loss 0.07 , qa loss 0.07 , lm loss 0.00 , avg batch size 74.6
2023-06-14 11:30:35,364 - 9:06:29 - 813.6s - INFO - __main__ - progress 6.650 , lr 4.2E-05 , loss 0.065 , qa loss 0.065 , lm loss 0.000 , avg batch size 74.7
2023-06-14 11:37:56,344 - 9:13:50 - 441.0s - INFO - __main__ - epoch 7/20 done , tot steps 10787 , lr 4.1E-05 , loss 0.06 , qa loss 0.06 , lm loss 0.00 , avg batch size 74.6
2023-06-14 11:51:30,446 - 9:27:24 - 814.1s - INFO - __main__ - progress 7.650 , lr 3.9E-05 , loss 0.062 , qa loss 0.062 , lm loss 0.000 , avg batch size 74.7
2023-06-14 11:58:50,664 - 9:34:44 - 440.2s - INFO - __main__ - epoch 8/20 done , tot steps 12328 , lr 3.8E-05 , loss 0.06 , qa loss 0.06 , lm loss 0.00 , avg batch size 74.6
2023-06-14 12:12:25,204 - 9:48:19 - 814.5s - INFO - __main__ - progress 8.648 , lr 3.5E-05 , loss 0.059 , qa loss 0.059 , lm loss 0.000 , avg batch size 74.6
2023-06-14 12:19:45,983 - 9:55:40 - 440.8s - INFO - __main__ - epoch 9/20 done , tot steps 13869 , lr 3.4E-05 , loss 0.06 , qa loss 0.06 , lm loss 0.00 , avg batch size 74.6
2023-06-14 12:33:20,874 - 10:09:15 - 814.9s - INFO - __main__ - progress 9.648 , lr 3.2E-05 , loss 0.059 , qa loss 0.059 , lm loss 0.000 , avg batch size 74.6
2023-06-14 12:40:43,166 - 10:16:37 - 442.3s - INFO - __main__ - epoch 10/20 done , tot steps 15412 , lr 3.1E-05 , loss 0.06 , qa loss 0.06 , lm loss 0.00 , avg batch size 74.5
2023-06-14 12:54:19,981 - 10:30:14 - 816.8s - INFO - __main__ - progress 10.649 , lr 2.9E-05 , loss 0.057 , qa loss 0.057 , lm loss 0.000 , avg batch size 74.6
2023-06-14 13:01:40,130 - 10:37:34 - 440.1s - INFO - __main__ - epoch 11/20 done , tot steps 16953 , lr 2.8E-05 , loss 0.06 , qa loss 0.06 , lm loss 0.00 , avg batch size 74.6
2023-06-14 13:15:15,328 - 10:51:09 - 815.2s - INFO - __main__ - progress 11.648 , lr 2.6E-05 , loss 0.055 , qa loss 0.055 , lm loss 0.000 , avg batch size 74.5
2023-06-14 13:22:38,934 - 10:58:33 - 443.6s - INFO - __main__ - epoch 12/20 done , tot steps 18497 , lr 2.5E-05 , loss 0.05 , qa loss 0.05 , lm loss 0.00 , avg batch size 74.5
2023-06-14 13:36:14,962 - 11:12:09 - 816.0s - INFO - __main__ - progress 12.650 , lr 2.3E-05 , loss 0.054 , qa loss 0.054 , lm loss 0.000 , avg batch size 74.8
2023-06-14 13:43:32,265 - 11:19:26 - 437.3s - INFO - __main__ - epoch 13/20 done , tot steps 20035 , lr 2.2E-05 , loss 0.05 , qa loss 0.05 , lm loss 0.00 , avg batch size 74.8
2023-06-14 13:57:05,878 - 11:33:00 - 813.6s - INFO - __main__ - progress 13.649 , lr 2.0E-05 , loss 0.052 , qa loss 0.052 , lm loss 0.000 , avg batch size 74.6
2023-06-14 14:04:26,177 - 11:40:20 - 440.3s - INFO - __main__ - epoch 14/20 done , tot steps 21576 , lr 1.9E-05 , loss 0.05 , qa loss 0.05 , lm loss 0.00 , avg batch size 74.6
2023-06-14 14:18:00,214 - 11:53:54 - 814.0s - INFO - __main__ - progress 14.649 , lr 1.7E-05 , loss 0.051 , qa loss 0.051 , lm loss 0.000 , avg batch size 74.6
2023-06-14 14:25:20,252 - 12:01:14 - 440.0s - INFO - __main__ - epoch 15/20 done , tot steps 23116 , lr 1.6E-05 , loss 0.05 , qa loss 0.05 , lm loss 0.00 , avg batch size 74.7
2023-06-14 14:38:57,401 - 12:14:51 - 817.1s - INFO - __main__ - progress 15.649 , lr 1.4E-05 , loss 0.051 , qa loss 0.051 , lm loss 0.000 , avg batch size 74.6
2023-06-14 14:46:15,040 - 12:22:09 - 437.6s - INFO - __main__ - epoch 16/20 done , tot steps 24655 , lr 1.3E-05 , loss 0.05 , qa loss 0.05 , lm loss 0.00 , avg batch size 74.7
2023-06-14 14:59:49,107 - 12:35:43 - 814.1s - INFO - __main__ - progress 16.649 , lr 1.0E-05 , loss 0.049 , qa loss 0.049 , lm loss 0.000 , avg batch size 74.7
2023-06-14 15:07:08,814 - 12:43:03 - 439.7s - INFO - __main__ - epoch 17/20 done , tot steps 26197 , lr 9.4E-06 , loss 0.05 , qa loss 0.05 , lm loss 0.00 , avg batch size 74.6
2023-06-14 15:20:43,800 - 12:56:38 - 815.0s - INFO - __main__ - progress 17.648 , lr 7.4E-06 , loss 0.047 , qa loss 0.047 , lm loss 0.000 , avg batch size 74.5
2023-06-14 15:28:07,517 - 13:04:01 - 443.7s - INFO - __main__ - epoch 18/20 done , tot steps 27740 , lr 6.3E-06 , loss 0.05 , qa loss 0.05 , lm loss 0.00 , avg batch size 74.5
2023-06-14 15:41:44,513 - 13:17:38 - 817.0s - INFO - __main__ - progress 18.649 , lr 4.2E-06 , loss 0.048 , qa loss 0.048 , lm loss 0.000 , avg batch size 74.7
2023-06-14 15:49:05,495 - 13:24:59 - 441.0s - INFO - __main__ - epoch 19/20 done , tot steps 29280 , lr 3.1E-06 , loss 0.05 , qa loss 0.05 , lm loss 0.00 , avg batch size 74.7
2023-06-14 16:02:42,037 - 13:38:36 - 816.5s - INFO - __main__ - progress 19.648 , lr 1.1E-06 , loss 0.047 , qa loss 0.047 , lm loss 0.000 , avg batch size 74.6
2023-06-14 16:10:00,709 - 13:45:54 - 438.7s - INFO - __main__ - epoch 20/20 done , tot steps 30821 , lr 1.6E-08 , loss 0.05 , qa loss 0.05 , lm loss 0.00 , avg batch size 74.6
2023-06-14 16:10:13,162 - 13:46:07 - 12.5s - INFO - __main__ - start to train { task: ['amazon'], seq train type: lll }
2023-06-14 16:10:27,794 - 13:46:22 - 14.6s - INFO - utils - writing extra data in ../../model_am2/gpt2/lll/wikisql_ag_amazon_sst_srl_woz.en_0.0/ag/lm.csv ...
2023-06-14 16:10:28,048 - 13:46:22 - 0.3s - INFO - __main__ - extra training data size: 0
2023-06-14 16:10:29,563 - 13:46:23 - 1.5s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[ag]
The task with which model is saved ag
2023-06-14 16:10:46,543 - 13:46:40 - 17.0s - INFO - __main__ - len of train dataset: 115000 , max train batch size 76 , num of opt steps: 2300000
2023-06-14 16:24:55,632 - 14:00:49 - 849.1s - INFO - __main__ - progress 0.428 , lr 6.1E-05 , loss 1.723 , qa loss 1.723 , lm loss 0.000 , avg batch size 49.2
2023-06-14 16:38:54,079 - 14:14:48 - 838.4s - INFO - __main__ - progress 0.856 , lr 6.0E-05 , loss 1.086 , qa loss 1.086 , lm loss 0.000 , avg batch size 49.2
2023-06-14 16:43:47,350 - 14:19:41 - 293.3s - INFO - __main__ - epoch 1/20 done , tot steps 2336 , lr 5.9E-05 , loss 0.99 , qa loss 0.99 , lm loss 0.00 , avg batch size 49.2
2023-06-14 16:57:56,878 - 14:33:51 - 849.5s - INFO - __main__ - progress 1.429 , lr 5.8E-05 , loss 0.424 , qa loss 0.424 , lm loss 0.000 , avg batch size 49.3
2023-06-14 17:11:56,959 - 14:47:51 - 840.1s - INFO - __main__ - progress 1.857 , lr 5.7E-05 , loss 0.419 , qa loss 0.419 , lm loss 0.000 , avg batch size 49.3
2023-06-14 17:16:40,136 - 14:52:34 - 283.2s - INFO - __main__ - epoch 2/20 done , tot steps 4668 , lr 5.6E-05 , loss 0.42 , qa loss 0.42 , lm loss 0.00 , avg batch size 49.3
2023-06-14 17:30:47,487 - 15:06:41 - 847.4s - INFO - __main__ - progress 2.429 , lr 5.5E-05 , loss 0.403 , qa loss 0.403 , lm loss 0.000 , avg batch size 49.3
2023-06-14 17:44:45,412 - 15:20:39 - 837.9s - INFO - __main__ - progress 2.856 , lr 5.4E-05 , loss 0.402 , qa loss 0.402 , lm loss 0.000 , avg batch size 49.2
2023-06-14 17:49:31,471 - 15:25:25 - 286.1s - INFO - __main__ - epoch 3/20 done , tot steps 7004 , lr 5.3E-05 , loss 0.40 , qa loss 0.40 , lm loss 0.00 , avg batch size 49.2
2023-06-14 18:03:41,339 - 15:39:35 - 849.9s - INFO - __main__ - progress 3.429 , lr 5.2E-05 , loss 0.394 , qa loss 0.394 , lm loss 0.000 , avg batch size 49.3
2023-06-14 18:17:40,361 - 15:53:34 - 839.0s - INFO - __main__ - progress 3.858 , lr 5.0E-05 , loss 0.391 , qa loss 0.391 , lm loss 0.000 , avg batch size 49.3
2023-06-14 18:22:23,595 - 15:58:17 - 283.2s - INFO - __main__ - epoch 4/20 done , tot steps 9338 , lr 5.0E-05 , loss 0.39 , qa loss 0.39 , lm loss 0.00 , avg batch size 49.3
2023-06-14 18:36:31,666 - 16:12:25 - 848.1s - INFO - __main__ - progress 4.427 , lr 4.9E-05 , loss 0.382 , qa loss 0.382 , lm loss 0.000 , avg batch size 49.1
2023-06-14 18:50:27,302 - 16:26:21 - 835.6s - INFO - __main__ - progress 4.857 , lr 4.7E-05 , loss 0.381 , qa loss 0.381 , lm loss 0.000 , avg batch size 49.3
2023-06-14 18:55:12,808 - 16:31:07 - 285.5s - INFO - __main__ - epoch 5/20 done , tot steps 11674 , lr 4.7E-05 , loss 0.38 , qa loss 0.38 , lm loss 0.00 , avg batch size 49.2
2023-06-14 19:09:18,812 - 16:45:13 - 846.0s - INFO - __main__ - progress 5.429 , lr 4.6E-05 , loss 0.373 , qa loss 0.373 , lm loss 0.000 , avg batch size 49.3
2023-06-14 19:23:16,789 - 16:59:11 - 838.0s - INFO - __main__ - progress 5.855 , lr 4.4E-05 , loss 0.375 , qa loss 0.375 , lm loss 0.000 , avg batch size 49.2
2023-06-14 19:28:02,163 - 17:03:56 - 285.4s - INFO - __main__ - epoch 6/20 done , tot steps 14010 , lr 4.4E-05 , loss 0.37 , qa loss 0.37 , lm loss 0.00 , avg batch size 49.2
2023-06-14 19:42:09,722 - 17:18:03 - 847.6s - INFO - __main__ - progress 6.427 , lr 4.2E-05 , loss 0.368 , qa loss 0.368 , lm loss 0.000 , avg batch size 49.2
2023-06-14 19:56:08,866 - 17:32:03 - 839.1s - INFO - __main__ - progress 6.857 , lr 4.1E-05 , loss 0.368 , qa loss 0.368 , lm loss 0.000 , avg batch size 49.3
2023-06-14 20:00:53,101 - 17:36:47 - 284.2s - INFO - __main__ - epoch 7/20 done , tot steps 16345 , lr 4.1E-05 , loss 0.37 , qa loss 0.37 , lm loss 0.00 , avg batch size 49.3
2023-06-14 20:14:59,445 - 17:50:53 - 846.3s - INFO - __main__ - progress 7.429 , lr 3.9E-05 , loss 0.364 , qa loss 0.364 , lm loss 0.000 , avg batch size 49.3
2023-06-14 20:28:55,188 - 18:04:49 - 835.7s - INFO - __main__ - progress 7.857 , lr 3.8E-05 , loss 0.363 , qa loss 0.363 , lm loss 0.000 , avg batch size 49.3
2023-06-14 20:33:39,116 - 18:09:33 - 283.9s - INFO - __main__ - epoch 8/20 done , tot steps 18680 , lr 3.8E-05 , loss 0.36 , qa loss 0.36 , lm loss 0.00 , avg batch size 49.3
2023-06-14 20:47:43,780 - 18:23:38 - 844.7s - INFO - __main__ - progress 8.430 , lr 3.6E-05 , loss 0.356 , qa loss 0.356 , lm loss 0.000 , avg batch size 49.4
2023-06-14 21:01:40,309 - 18:37:34 - 836.5s - INFO - __main__ - progress 8.857 , lr 3.5E-05 , loss 0.358 , qa loss 0.358 , lm loss 0.000 , avg batch size 49.3
2023-06-14 21:06:25,921 - 18:42:20 - 285.6s - INFO - __main__ - epoch 9/20 done , tot steps 21016 , lr 3.4E-05 , loss 0.36 , qa loss 0.36 , lm loss 0.00 , avg batch size 49.2
2023-06-14 21:20:31,528 - 18:56:25 - 845.6s - INFO - __main__ - progress 9.431 , lr 3.3E-05 , loss 0.356 , qa loss 0.356 , lm loss 0.000 , avg batch size 49.5
2023-06-14 21:34:27,867 - 19:10:22 - 836.3s - INFO - __main__ - progress 9.859 , lr 3.2E-05 , loss 0.355 , qa loss 0.355 , lm loss 0.000 , avg batch size 49.4
2023-06-14 21:39:08,411 - 19:15:02 - 280.5s - INFO - __main__ - epoch 10/20 done , tot steps 23346 , lr 3.1E-05 , loss 0.35 , qa loss 0.35 , lm loss 0.00 , avg batch size 49.4
2023-06-14 21:53:15,195 - 19:29:09 - 846.8s - INFO - __main__ - progress 10.429 , lr 3.0E-05 , loss 0.351 , qa loss 0.351 , lm loss 0.000 , avg batch size 49.3
2023-06-14 22:07:11,665 - 19:43:05 - 836.5s - INFO - __main__ - progress 10.858 , lr 2.9E-05 , loss 0.351 , qa loss 0.351 , lm loss 0.000 , avg batch size 49.3
2023-06-14 22:11:57,114 - 19:47:51 - 285.4s - INFO - __main__ - epoch 11/20 done , tot steps 25681 , lr 2.8E-05 , loss 0.35 , qa loss 0.35 , lm loss 0.00 , avg batch size 49.3
2023-06-14 22:26:02,758 - 20:01:57 - 845.6s - INFO - __main__ - progress 11.428 , lr 2.7E-05 , loss 0.348 , qa loss 0.348 , lm loss 0.000 , avg batch size 49.2
2023-06-14 22:40:03,982 - 20:15:58 - 841.2s - INFO - __main__ - progress 11.856 , lr 2.5E-05 , loss 0.348 , qa loss 0.348 , lm loss 0.000 , avg batch size 49.2
2023-06-14 22:44:50,504 - 20:20:44 - 286.5s - INFO - __main__ - epoch 12/20 done , tot steps 28015 , lr 2.5E-05 , loss 0.35 , qa loss 0.35 , lm loss 0.00 , avg batch size 49.3
2023-06-14 22:59:03,686 - 20:34:57 - 853.2s - INFO - __main__ - progress 12.430 , lr 2.4E-05 , loss 0.342 , qa loss 0.342 , lm loss 0.000 , avg batch size 49.5
2023-06-14 23:13:09,287 - 20:49:03 - 845.6s - INFO - __main__ - progress 12.860 , lr 2.2E-05 , loss 0.344 , qa loss 0.344 , lm loss 0.000 , avg batch size 49.4
2023-06-14 23:17:51,419 - 20:53:45 - 282.1s - INFO - __main__ - epoch 13/20 done , tot steps 30344 , lr 2.2E-05 , loss 0.34 , qa loss 0.34 , lm loss 0.00 , avg batch size 49.4
2023-06-14 23:32:03,298 - 21:07:57 - 851.9s - INFO - __main__ - progress 13.426 , lr 2.1E-05 , loss 0.341 , qa loss 0.341 , lm loss 0.000 , avg batch size 49.0
2023-06-14 23:46:06,967 - 21:22:01 - 843.7s - INFO - __main__ - progress 13.855 , lr 1.9E-05 , loss 0.341 , qa loss 0.341 , lm loss 0.000 , avg batch size 49.1
2023-06-14 23:50:57,485 - 21:26:51 - 290.5s - INFO - __main__ - epoch 14/20 done , tot steps 32682 , lr 1.9E-05 , loss 0.34 , qa loss 0.34 , lm loss 0.00 , avg batch size 49.2
2023-06-15 00:05:11,160 - 21:41:05 - 853.7s - INFO - __main__ - progress 14.429 , lr 1.7E-05 , loss 0.336 , qa loss 0.336 , lm loss 0.000 , avg batch size 49.3
2023-06-15 00:19:15,364 - 21:55:09 - 844.2s - INFO - __main__ - progress 14.857 , lr 1.6E-05 , loss 0.338 , qa loss 0.338 , lm loss 0.000 , avg batch size 49.3
2023-06-15 00:24:01,452 - 21:59:55 - 286.1s - INFO - __main__ - epoch 15/20 done , tot steps 35015 , lr 1.6E-05 , loss 0.34 , qa loss 0.34 , lm loss 0.00 , avg batch size 49.3
2023-06-15 00:38:16,082 - 22:14:10 - 854.6s - INFO - __main__ - progress 15.429 , lr 1.4E-05 , loss 0.334 , qa loss 0.334 , lm loss 0.000 , avg batch size 49.4
2023-06-15 00:52:22,470 - 22:28:16 - 846.4s - INFO - __main__ - progress 15.857 , lr 1.3E-05 , loss 0.336 , qa loss 0.336 , lm loss 0.000 , avg batch size 49.3
2023-06-15 00:57:10,506 - 22:33:04 - 288.0s - INFO - __main__ - epoch 16/20 done , tot steps 37349 , lr 1.3E-05 , loss 0.34 , qa loss 0.34 , lm loss 0.00 , avg batch size 49.3
2023-06-15 01:11:25,474 - 22:47:19 - 855.0s - INFO - __main__ - progress 16.428 , lr 1.1E-05 , loss 0.335 , qa loss 0.335 , lm loss 0.000 , avg batch size 49.2
2023-06-15 01:25:31,708 - 23:01:25 - 846.2s - INFO - __main__ - progress 16.856 , lr 9.8E-06 , loss 0.334 , qa loss 0.334 , lm loss 0.000 , avg batch size 49.2
2023-06-15 01:30:19,251 - 23:06:13 - 287.5s - INFO - __main__ - epoch 17/20 done , tot steps 39684 , lr 9.4E-06 , loss 0.33 , qa loss 0.33 , lm loss 0.00 , avg batch size 49.3
2023-06-15 01:44:35,528 - 23:20:29 - 856.3s - INFO - __main__ - progress 17.427 , lr 8.1E-06 , loss 0.331 , qa loss 0.331 , lm loss 0.000 , avg batch size 49.1
2023-06-15 01:58:41,184 - 23:34:35 - 845.7s - INFO - __main__ - progress 17.856 , lr 6.7E-06 , loss 0.332 , qa loss 0.332 , lm loss 0.000 , avg batch size 49.2
2023-06-15 02:03:31,416 - 23:39:25 - 290.2s - INFO - __main__ - epoch 18/20 done , tot steps 42022 , lr 6.3E-06 , loss 0.33 , qa loss 0.33 , lm loss 0.00 , avg batch size 49.2
2023-06-15 02:17:46,621 - 23:53:40 - 855.2s - INFO - __main__ - progress 18.427 , lr 4.9E-06 , loss 0.329 , qa loss 0.329 , lm loss 0.000 , avg batch size 49.1
2023-06-15 02:31:52,736 - 1 day, 0:07:46 - 846.1s - INFO - __main__ - progress 18.856 , lr 3.6E-06 , loss 0.329 , qa loss 0.329 , lm loss 0.000 , avg batch size 49.2
2023-06-15 02:36:41,435 - 1 day, 0:12:35 - 288.7s - INFO - __main__ - epoch 19/20 done , tot steps 44358 , lr 3.1E-06 , loss 0.33 , qa loss 0.33 , lm loss 0.00 , avg batch size 49.2
2023-06-15 02:50:56,829 - 1 day, 0:26:51 - 855.4s - INFO - __main__ - progress 19.429 , lr 1.8E-06 , loss 0.328 , qa loss 0.328 , lm loss 0.000 , avg batch size 49.3
2023-06-15 03:05:04,166 - 1 day, 0:40:58 - 847.3s - INFO - __main__ - progress 19.857 , lr 4.6E-07 , loss 0.328 , qa loss 0.328 , lm loss 0.000 , avg batch size 49.3
2023-06-15 03:09:50,902 - 1 day, 0:45:45 - 286.7s - INFO - __main__ - epoch 20/20 done , tot steps 46691 , lr 1.6E-08 , loss 0.33 , qa loss 0.33 , lm loss 0.00 , avg batch size 49.3
2023-06-15 03:10:02,706 - 1 day, 0:45:56 - 11.8s - INFO - __main__ - start to train { task: ['sst'], seq train type: lll }
2023-06-15 03:10:09,011 - 1 day, 0:46:03 - 6.3s - INFO - utils - writing extra data in ../../model_am2/gpt2/lll/wikisql_ag_amazon_sst_srl_woz.en_0.0/amazon/lm.csv ...
2023-06-15 03:10:09,278 - 1 day, 0:46:03 - 0.3s - INFO - __main__ - extra training data size: 0
2023-06-15 03:10:10,765 - 1 day, 0:46:05 - 1.5s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[amazon]
The task with which model is saved amazon
2023-06-15 03:10:18,118 - 1 day, 0:46:12 - 7.4s - INFO - __main__ - len of train dataset: 6920 , max train batch size 4 , num of opt steps: 138400
2023-06-15 03:12:26,211 - 1 day, 0:48:20 - 128.1s - INFO - __main__ - progress 0.578 , lr 6.1E-05 , loss 2.140 , qa loss 2.140 , lm loss 0.000 , avg batch size 4.0
2023-06-15 03:14:04,636 - 1 day, 0:49:58 - 98.4s - INFO - __main__ - epoch 1/20 done , tot steps 1730 , lr 5.9E-05 , loss 1.34 , qa loss 1.34 , lm loss 0.00 , avg batch size 4.0
2023-06-15 03:16:09,468 - 1 day, 0:52:03 - 124.8s - INFO - __main__ - progress 1.578 , lr 5.8E-05 , loss 0.212 , qa loss 0.212 , lm loss 0.000 , avg batch size 4.0
2023-06-15 03:17:40,751 - 1 day, 0:53:35 - 91.3s - INFO - __main__ - epoch 2/20 done , tot steps 3460 , lr 5.6E-05 , loss 0.21 , qa loss 0.21 , lm loss 0.00 , avg batch size 4.0
2023-06-15 03:19:49,553 - 1 day, 0:55:43 - 128.8s - INFO - __main__ - progress 2.578 , lr 5.4E-05 , loss 0.175 , qa loss 0.175 , lm loss 0.000 , avg batch size 4.0
2023-06-15 03:21:19,487 - 1 day, 0:57:13 - 89.9s - INFO - __main__ - epoch 3/20 done , tot steps 5190 , lr 5.3E-05 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
2023-06-15 03:23:25,360 - 1 day, 0:59:19 - 125.9s - INFO - __main__ - progress 3.578 , lr 5.1E-05 , loss 0.153 , qa loss 0.153 , lm loss 0.000 , avg batch size 4.0
2023-06-15 03:24:56,672 - 1 day, 1:00:50 - 91.3s - INFO - __main__ - epoch 4/20 done , tot steps 6920 , lr 5.0E-05 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-06-15 03:27:01,691 - 1 day, 1:02:55 - 125.0s - INFO - __main__ - progress 4.578 , lr 4.8E-05 , loss 0.136 , qa loss 0.136 , lm loss 0.000 , avg batch size 4.0
2023-06-15 03:28:33,477 - 1 day, 1:04:27 - 91.8s - INFO - __main__ - epoch 5/20 done , tot steps 8650 , lr 4.7E-05 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-06-15 03:30:39,197 - 1 day, 1:06:33 - 125.7s - INFO - __main__ - progress 5.578 , lr 4.5E-05 , loss 0.121 , qa loss 0.121 , lm loss 0.000 , avg batch size 4.0
2023-06-15 03:32:09,053 - 1 day, 1:08:03 - 89.9s - INFO - __main__ - epoch 6/20 done , tot steps 10380 , lr 4.4E-05 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2023-06-15 03:34:16,546 - 1 day, 1:10:10 - 127.5s - INFO - __main__ - progress 6.578 , lr 4.2E-05 , loss 0.102 , qa loss 0.102 , lm loss 0.000 , avg batch size 4.0
2023-06-15 03:35:46,764 - 1 day, 1:11:41 - 90.2s - INFO - __main__ - epoch 7/20 done , tot steps 12110 , lr 4.1E-05 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-06-15 03:37:52,278 - 1 day, 1:13:46 - 125.5s - INFO - __main__ - progress 7.578 , lr 3.9E-05 , loss 0.094 , qa loss 0.094 , lm loss 0.000 , avg batch size 4.0
2023-06-15 03:39:22,802 - 1 day, 1:15:17 - 90.5s - INFO - __main__ - epoch 8/20 done , tot steps 13840 , lr 3.8E-05 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 4.0
2023-06-15 03:41:30,866 - 1 day, 1:17:25 - 128.1s - INFO - __main__ - progress 8.578 , lr 3.6E-05 , loss 0.079 , qa loss 0.079 , lm loss 0.000 , avg batch size 4.0
2023-06-15 03:43:00,983 - 1 day, 1:18:55 - 90.1s - INFO - __main__ - epoch 9/20 done , tot steps 15570 , lr 3.4E-05 , loss 0.08 , qa loss 0.08 , lm loss 0.00 , avg batch size 4.0
2023-06-15 03:45:07,630 - 1 day, 1:21:01 - 126.6s - INFO - __main__ - progress 9.578 , lr 3.3E-05 , loss 0.072 , qa loss 0.072 , lm loss 0.000 , avg batch size 4.0
2023-06-15 03:46:39,908 - 1 day, 1:22:34 - 92.3s - INFO - __main__ - epoch 10/20 done , tot steps 17300 , lr 3.1E-05 , loss 0.07 , qa loss 0.07 , lm loss 0.00 , avg batch size 4.0
2023-06-15 03:48:46,275 - 1 day, 1:24:40 - 126.4s - INFO - __main__ - progress 10.578 , lr 2.9E-05 , loss 0.064 , qa loss 0.064 , lm loss 0.000 , avg batch size 4.0
2023-06-15 03:50:15,593 - 1 day, 1:26:09 - 89.3s - INFO - __main__ - epoch 11/20 done , tot steps 19030 , lr 2.8E-05 , loss 0.06 , qa loss 0.06 , lm loss 0.00 , avg batch size 4.0
2023-06-15 03:52:22,001 - 1 day, 1:28:16 - 126.4s - INFO - __main__ - progress 11.578 , lr 2.6E-05 , loss 0.046 , qa loss 0.046 , lm loss 0.000 , avg batch size 4.0
2023-06-15 03:53:53,194 - 1 day, 1:29:47 - 91.2s - INFO - __main__ - epoch 12/20 done , tot steps 20760 , lr 2.5E-05 , loss 0.05 , qa loss 0.05 , lm loss 0.00 , avg batch size 4.0
2023-06-15 03:55:58,840 - 1 day, 1:31:53 - 125.6s - INFO - __main__ - progress 12.578 , lr 2.3E-05 , loss 0.049 , qa loss 0.049 , lm loss 0.000 , avg batch size 4.0
2023-06-15 03:57:30,688 - 1 day, 1:33:24 - 91.8s - INFO - __main__ - epoch 13/20 done , tot steps 22490 , lr 2.2E-05 , loss 0.05 , qa loss 0.05 , lm loss 0.00 , avg batch size 4.0
2023-06-15 03:59:35,049 - 1 day, 1:35:29 - 124.4s - INFO - __main__ - progress 13.578 , lr 2.0E-05 , loss 0.042 , qa loss 0.042 , lm loss 0.000 , avg batch size 4.0
2023-06-15 04:01:04,699 - 1 day, 1:36:58 - 89.6s - INFO - __main__ - epoch 14/20 done , tot steps 24220 , lr 1.9E-05 , loss 0.04 , qa loss 0.04 , lm loss 0.00 , avg batch size 4.0
2023-06-15 04:03:11,896 - 1 day, 1:39:06 - 127.2s - INFO - __main__ - progress 14.578 , lr 1.7E-05 , loss 0.042 , qa loss 0.042 , lm loss 0.000 , avg batch size 4.0
2023-06-15 04:04:43,285 - 1 day, 1:40:37 - 91.4s - INFO - __main__ - epoch 15/20 done , tot steps 25950 , lr 1.6E-05 , loss 0.04 , qa loss 0.04 , lm loss 0.00 , avg batch size 4.0
2023-06-15 04:06:49,869 - 1 day, 1:42:44 - 126.6s - INFO - __main__ - progress 15.578 , lr 1.4E-05 , loss 0.034 , qa loss 0.034 , lm loss 0.000 , avg batch size 4.0
2023-06-15 04:08:20,049 - 1 day, 1:44:14 - 90.2s - INFO - __main__ - epoch 16/20 done , tot steps 27680 , lr 1.3E-05 , loss 0.03 , qa loss 0.03 , lm loss 0.00 , avg batch size 4.0
2023-06-15 04:10:25,120 - 1 day, 1:46:19 - 125.1s - INFO - __main__ - progress 16.578 , lr 1.1E-05 , loss 0.035 , qa loss 0.035 , lm loss 0.000 , avg batch size 4.0
2023-06-15 04:11:54,853 - 1 day, 1:47:49 - 89.7s - INFO - __main__ - epoch 17/20 done , tot steps 29410 , lr 9.4E-06 , loss 0.04 , qa loss 0.04 , lm loss 0.00 , avg batch size 4.0
2023-06-15 04:14:02,181 - 1 day, 1:49:56 - 127.3s - INFO - __main__ - progress 17.578 , lr 7.6E-06 , loss 0.025 , qa loss 0.025 , lm loss 0.000 , avg batch size 4.0
2023-06-15 04:15:34,514 - 1 day, 1:51:28 - 92.3s - INFO - __main__ - epoch 18/20 done , tot steps 31140 , lr 6.3E-06 , loss 0.03 , qa loss 0.03 , lm loss 0.00 , avg batch size 4.0
2023-06-15 04:17:39,886 - 1 day, 1:53:34 - 125.4s - INFO - __main__ - progress 18.578 , lr 4.5E-06 , loss 0.031 , qa loss 0.031 , lm loss 0.000 , avg batch size 4.0
2023-06-15 04:19:10,953 - 1 day, 1:55:05 - 91.1s - INFO - __main__ - epoch 19/20 done , tot steps 32870 , lr 3.1E-06 , loss 0.03 , qa loss 0.03 , lm loss 0.00 , avg batch size 4.0
2023-06-15 04:21:18,291 - 1 day, 1:57:12 - 127.3s - INFO - __main__ - progress 19.578 , lr 1.3E-06 , loss 0.030 , qa loss 0.030 , lm loss 0.000 , avg batch size 4.0
2023-06-15 04:22:48,188 - 1 day, 1:58:42 - 89.9s - INFO - __main__ - epoch 20/20 done , tot steps 34600 , lr 1.5E-08 , loss 0.03 , qa loss 0.03 , lm loss 0.00 , avg batch size 4.0
2023-06-15 04:22:59,656 - 1 day, 1:58:53 - 11.5s - INFO - __main__ - start to train { task: ['srl'], seq train type: lll }
2023-06-15 04:23:15,046 - 1 day, 1:59:09 - 15.4s - INFO - utils - writing extra data in ../../model_am2/gpt2/lll/wikisql_ag_amazon_sst_srl_woz.en_0.0/sst/lm.csv ...
2023-06-15 04:23:15,339 - 1 day, 1:59:09 - 0.3s - INFO - __main__ - extra training data size: 0
2023-06-15 04:23:17,157 - 1 day, 1:59:11 - 1.8s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[sst]
The task with which model is saved sst
2023-06-15 04:23:23,840 - 1 day, 1:59:18 - 6.7s - INFO - __main__ - len of train dataset: 6414 , max train batch size 4 , num of opt steps: 128280
2023-06-15 04:26:34,063 - 1 day, 2:02:28 - 190.2s - INFO - __main__ - progress 0.624 , lr 6.1E-05 , loss 3.007 , qa loss 3.007 , lm loss 0.000 , avg batch size 4.0
2023-06-15 04:28:35,826 - 1 day, 2:04:30 - 121.8s - INFO - __main__ - epoch 1/20 done , tot steps 1604 , lr 5.9E-05 , loss 2.21 , qa loss 2.21 , lm loss 0.00 , avg batch size 4.0
2023-06-15 04:31:44,598 - 1 day, 2:07:38 - 188.8s - INFO - __main__ - progress 1.624 , lr 5.7E-05 , loss 0.756 , qa loss 0.756 , lm loss 0.000 , avg batch size 4.0
2023-06-15 04:33:38,951 - 1 day, 2:09:33 - 114.4s - INFO - __main__ - epoch 2/20 done , tot steps 3208 , lr 5.6E-05 , loss 0.73 , qa loss 0.73 , lm loss 0.00 , avg batch size 4.0
2023-06-15 04:36:53,461 - 1 day, 2:12:47 - 194.5s - INFO - __main__ - progress 2.624 , lr 5.4E-05 , loss 0.614 , qa loss 0.614 , lm loss 0.000 , avg batch size 4.0
2023-06-15 04:38:45,735 - 1 day, 2:14:39 - 112.3s - INFO - __main__ - epoch 3/20 done , tot steps 4812 , lr 5.3E-05 , loss 0.60 , qa loss 0.60 , lm loss 0.00 , avg batch size 4.0
2023-06-15 04:41:56,964 - 1 day, 2:17:51 - 191.2s - INFO - __main__ - progress 3.624 , lr 5.1E-05 , loss 0.544 , qa loss 0.544 , lm loss 0.000 , avg batch size 4.0
2023-06-15 04:43:53,452 - 1 day, 2:19:47 - 116.5s - INFO - __main__ - epoch 4/20 done , tot steps 6416 , lr 5.0E-05 , loss 0.54 , qa loss 0.54 , lm loss 0.00 , avg batch size 4.0
2023-06-15 04:47:00,775 - 1 day, 2:22:55 - 187.3s - INFO - __main__ - progress 4.624 , lr 4.8E-05 , loss 0.482 , qa loss 0.482 , lm loss 0.000 , avg batch size 4.0
2023-06-15 04:48:58,441 - 1 day, 2:24:52 - 117.7s - INFO - __main__ - epoch 5/20 done , tot steps 8020 , lr 4.7E-05 , loss 0.48 , qa loss 0.48 , lm loss 0.00 , avg batch size 4.0
2023-06-15 04:52:08,528 - 1 day, 2:28:02 - 190.1s - INFO - __main__ - progress 5.624 , lr 4.5E-05 , loss 0.431 , qa loss 0.431 , lm loss 0.000 , avg batch size 4.0
2023-06-15 04:54:03,277 - 1 day, 2:29:57 - 114.7s - INFO - __main__ - epoch 6/20 done , tot steps 9624 , lr 4.4E-05 , loss 0.42 , qa loss 0.42 , lm loss 0.00 , avg batch size 4.0
2023-06-15 04:57:15,898 - 1 day, 2:33:10 - 192.6s - INFO - __main__ - progress 6.624 , lr 4.2E-05 , loss 0.390 , qa loss 0.390 , lm loss 0.000 , avg batch size 4.0
2023-06-15 04:59:07,664 - 1 day, 2:35:01 - 111.8s - INFO - __main__ - epoch 7/20 done , tot steps 11228 , lr 4.1E-05 , loss 0.39 , qa loss 0.39 , lm loss 0.00 , avg batch size 4.0
2023-06-15 05:02:22,566 - 1 day, 2:38:16 - 194.9s - INFO - __main__ - progress 7.624 , lr 3.9E-05 , loss 0.368 , qa loss 0.368 , lm loss 0.000 , avg batch size 4.0
2023-06-15 05:04:16,352 - 1 day, 2:40:10 - 113.8s - INFO - __main__ - epoch 8/20 done , tot steps 12832 , lr 3.8E-05 , loss 0.36 , qa loss 0.36 , lm loss 0.00 , avg batch size 4.0
2023-06-15 05:07:28,583 - 1 day, 2:43:22 - 192.2s - INFO - __main__ - progress 8.624 , lr 3.6E-05 , loss 0.337 , qa loss 0.337 , lm loss 0.000 , avg batch size 4.0
2023-06-15 05:09:21,201 - 1 day, 2:45:15 - 112.6s - INFO - __main__ - epoch 9/20 done , tot steps 14436 , lr 3.4E-05 , loss 0.34 , qa loss 0.34 , lm loss 0.00 , avg batch size 4.0
2023-06-15 05:12:35,616 - 1 day, 2:48:29 - 194.4s - INFO - __main__ - progress 9.624 , lr 3.2E-05 , loss 0.310 , qa loss 0.310 , lm loss 0.000 , avg batch size 4.0
2023-06-15 05:14:29,488 - 1 day, 2:50:23 - 113.9s - INFO - __main__ - epoch 10/20 done , tot steps 16040 , lr 3.1E-05 , loss 0.31 , qa loss 0.31 , lm loss 0.00 , avg batch size 4.0
2023-06-15 05:17:40,879 - 1 day, 2:53:35 - 191.4s - INFO - __main__ - progress 10.624 , lr 2.9E-05 , loss 0.290 , qa loss 0.290 , lm loss 0.000 , avg batch size 4.0
2023-06-15 05:19:33,511 - 1 day, 2:55:27 - 112.6s - INFO - __main__ - epoch 11/20 done , tot steps 17644 , lr 2.8E-05 , loss 0.29 , qa loss 0.29 , lm loss 0.00 , avg batch size 4.0
2023-06-15 05:22:43,910 - 1 day, 2:58:38 - 190.4s - INFO - __main__ - progress 11.624 , lr 2.6E-05 , loss 0.272 , qa loss 0.272 , lm loss 0.000 , avg batch size 4.0
2023-06-15 05:24:38,688 - 1 day, 3:00:32 - 114.8s - INFO - __main__ - epoch 12/20 done , tot steps 19248 , lr 2.5E-05 , loss 0.27 , qa loss 0.27 , lm loss 0.00 , avg batch size 4.0
2023-06-15 05:27:50,300 - 1 day, 3:03:44 - 191.6s - INFO - __main__ - progress 12.624 , lr 2.3E-05 , loss 0.256 , qa loss 0.256 , lm loss 0.000 , avg batch size 4.0
2023-06-15 05:29:45,142 - 1 day, 3:05:39 - 114.8s - INFO - __main__ - epoch 13/20 done , tot steps 20852 , lr 2.2E-05 , loss 0.26 , qa loss 0.26 , lm loss 0.00 , avg batch size 4.0
2023-06-15 05:32:53,512 - 1 day, 3:08:47 - 188.4s - INFO - __main__ - progress 13.624 , lr 2.0E-05 , loss 0.234 , qa loss 0.234 , lm loss 0.000 , avg batch size 4.0
2023-06-15 05:34:49,420 - 1 day, 3:10:43 - 115.9s - INFO - __main__ - epoch 14/20 done , tot steps 22456 , lr 1.9E-05 , loss 0.24 , qa loss 0.24 , lm loss 0.00 , avg batch size 4.0
2023-06-15 05:38:00,008 - 1 day, 3:13:54 - 190.6s - INFO - __main__ - progress 14.624 , lr 1.7E-05 , loss 0.232 , qa loss 0.232 , lm loss 0.000 , avg batch size 4.0
2023-06-15 05:39:55,368 - 1 day, 3:15:49 - 115.4s - INFO - __main__ - epoch 15/20 done , tot steps 24060 , lr 1.6E-05 , loss 0.23 , qa loss 0.23 , lm loss 0.00 , avg batch size 4.0
2023-06-15 05:43:07,622 - 1 day, 3:19:01 - 192.3s - INFO - __main__ - progress 15.624 , lr 1.4E-05 , loss 0.210 , qa loss 0.210 , lm loss 0.000 , avg batch size 4.0
2023-06-15 05:45:00,580 - 1 day, 3:20:54 - 113.0s - INFO - __main__ - epoch 16/20 done , tot steps 25664 , lr 1.3E-05 , loss 0.22 , qa loss 0.22 , lm loss 0.00 , avg batch size 4.0
2023-06-15 05:48:11,409 - 1 day, 3:24:05 - 190.8s - INFO - __main__ - progress 16.624 , lr 1.1E-05 , loss 0.208 , qa loss 0.208 , lm loss 0.000 , avg batch size 4.0
2023-06-15 05:50:03,468 - 1 day, 3:25:57 - 112.1s - INFO - __main__ - epoch 17/20 done , tot steps 27268 , lr 9.4E-06 , loss 0.21 , qa loss 0.21 , lm loss 0.00 , avg batch size 4.0
2023-06-15 05:53:16,958 - 1 day, 3:29:11 - 193.5s - INFO - __main__ - progress 17.624 , lr 7.4E-06 , loss 0.203 , qa loss 0.203 , lm loss 0.000 , avg batch size 4.0
2023-06-15 05:55:11,738 - 1 day, 3:31:05 - 114.8s - INFO - __main__ - epoch 18/20 done , tot steps 28872 , lr 6.3E-06 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-06-15 05:58:23,092 - 1 day, 3:34:17 - 191.4s - INFO - __main__ - progress 18.624 , lr 4.3E-06 , loss 0.193 , qa loss 0.193 , lm loss 0.000 , avg batch size 4.0
2023-06-15 06:00:20,153 - 1 day, 3:36:14 - 117.1s - INFO - __main__ - epoch 19/20 done , tot steps 30476 , lr 3.1E-06 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-06-15 06:03:28,663 - 1 day, 3:39:22 - 188.5s - INFO - __main__ - progress 19.624 , lr 1.2E-06 , loss 0.187 , qa loss 0.187 , lm loss 0.000 , avg batch size 4.0
2023-06-15 06:05:27,183 - 1 day, 3:41:21 - 118.5s - INFO - __main__ - epoch 20/20 done , tot steps 32080 , lr 1.6E-08 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-06-15 06:05:39,326 - 1 day, 3:41:33 - 12.1s - INFO - __main__ - start to train { task: ['woz.en'], seq train type: lll }
2023-06-15 06:05:45,156 - 1 day, 3:41:39 - 5.8s - INFO - utils - writing extra data in ../../model_am2/gpt2/lll/wikisql_ag_amazon_sst_srl_woz.en_0.0/srl/lm.csv ...
2023-06-15 06:05:45,480 - 1 day, 3:41:39 - 0.3s - INFO - __main__ - extra training data size: 0
2023-06-15 06:05:47,601 - 1 day, 3:41:41 - 2.1s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[srl]
The task with which model is saved srl
2023-06-15 06:05:53,754 - 1 day, 3:41:48 - 6.2s - INFO - __main__ - len of train dataset: 2536 , max train batch size 4 , num of opt steps: 50720
2023-06-15 06:07:31,127 - 1 day, 3:43:25 - 97.4s - INFO - __main__ - epoch 1/20 done , tot steps 634 , lr 5.9E-05 , loss 3.57 , qa loss 3.57 , lm loss 0.00 , avg batch size 4.0
2023-06-15 06:09:00,855 - 1 day, 3:44:55 - 89.7s - INFO - __main__ - epoch 2/20 done , tot steps 1268 , lr 5.6E-05 , loss 0.43 , qa loss 0.43 , lm loss 0.00 , avg batch size 4.0
2023-06-15 06:10:30,290 - 1 day, 3:46:24 - 89.4s - INFO - __main__ - epoch 3/20 done , tot steps 1902 , lr 5.3E-05 , loss 0.32 , qa loss 0.32 , lm loss 0.00 , avg batch size 4.0
2023-06-15 06:11:58,785 - 1 day, 3:47:53 - 88.5s - INFO - __main__ - epoch 4/20 done , tot steps 2536 , lr 5.0E-05 , loss 0.24 , qa loss 0.24 , lm loss 0.00 , avg batch size 4.0
2023-06-15 06:13:25,434 - 1 day, 3:49:19 - 86.6s - INFO - __main__ - epoch 5/20 done , tot steps 3170 , lr 4.7E-05 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-06-15 06:14:52,554 - 1 day, 3:50:46 - 87.1s - INFO - __main__ - epoch 6/20 done , tot steps 3804 , lr 4.4E-05 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-06-15 06:16:21,174 - 1 day, 3:52:15 - 88.6s - INFO - __main__ - epoch 7/20 done , tot steps 4438 , lr 4.1E-05 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
2023-06-15 06:17:50,065 - 1 day, 3:53:44 - 88.9s - INFO - __main__ - epoch 8/20 done , tot steps 5072 , lr 3.8E-05 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-06-15 06:19:18,912 - 1 day, 3:55:13 - 88.8s - INFO - __main__ - epoch 9/20 done , tot steps 5706 , lr 3.4E-05 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-06-15 06:20:46,119 - 1 day, 3:56:40 - 87.2s - INFO - __main__ - epoch 10/20 done , tot steps 6340 , lr 3.1E-05 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-06-15 06:22:12,294 - 1 day, 3:58:06 - 86.2s - INFO - __main__ - epoch 11/20 done , tot steps 6974 , lr 2.8E-05 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-06-15 06:23:39,778 - 1 day, 3:59:34 - 87.5s - INFO - __main__ - epoch 12/20 done , tot steps 7608 , lr 2.5E-05 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2023-06-15 06:25:06,997 - 1 day, 4:01:01 - 87.2s - INFO - __main__ - epoch 13/20 done , tot steps 8242 , lr 2.2E-05 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2023-06-15 06:26:33,955 - 1 day, 4:02:28 - 87.0s - INFO - __main__ - epoch 14/20 done , tot steps 8876 , lr 1.9E-05 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-06-15 06:28:02,654 - 1 day, 4:03:56 - 88.7s - INFO - __main__ - epoch 15/20 done , tot steps 9510 , lr 1.6E-05 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 4.0
2023-06-15 06:29:30,975 - 1 day, 4:05:25 - 88.3s - INFO - __main__ - epoch 16/20 done , tot steps 10144 , lr 1.3E-05 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 4.0
2023-06-15 06:30:57,340 - 1 day, 4:06:51 - 86.4s - INFO - __main__ - epoch 17/20 done , tot steps 10778 , lr 9.4E-06 , loss 0.09 , qa loss 0.09 , lm loss 0.00 , avg batch size 4.0
2023-06-15 06:32:24,500 - 1 day, 4:08:18 - 87.2s - INFO - __main__ - epoch 18/20 done , tot steps 11412 , lr 6.3E-06 , loss 0.09 , qa loss 0.09 , lm loss 0.00 , avg batch size 4.0
2023-06-15 06:33:53,411 - 1 day, 4:09:47 - 88.9s - INFO - __main__ - epoch 19/20 done , tot steps 12046 , lr 3.1E-06 , loss 0.09 , qa loss 0.09 , lm loss 0.00 , avg batch size 4.0
2023-06-15 06:35:22,677 - 1 day, 4:11:16 - 89.3s - INFO - __main__ - epoch 20/20 done , tot steps 12680 , lr 1.5E-08 , loss 0.09 , qa loss 0.09 , lm loss 0.00 , avg batch size 4.0
The current active adapter is Stack[woz_en]
The task with which model is saved woz_en
Wall Execution time: 04:00:44
CPU Execution time: 14:30:34
