Not all gpus support fp16 training! Will use fp32 instead.
2023-05-30 17:14:13,518 - 0:00:05 - 0.0s - INFO - __main__ - args = Namespace(REG_TYPE_KEYS=['mas', 'ewc'], adam_epsilon=0.0001, add_task_tokens=False, data_dir='../../data', debug=False, decay_style='linear', device_ids=[1, 2, 3, 4], dynamic_epochs=False, fp32=True, gen_lm_sample_percentage=0.0, learning_rate=6.25e-05, lm_lambda=0.0, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[35672.56, 42158.48, 42158.48, 42158.48], min_batch_size=4, min_n_steps=1500, model_dir_root='../../model_cm1/gpt2/lll/sst_srl_woz.en_0.0', model_name='gpt2', n_gpus=4, n_train_epochs={'sst': 20, 'srl': 20, 'woz.en': 20}, n_warmup_ratio=0.005, n_workers=75, qp_margin=0.5, real_sample=False, reg_lambda=1.0, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[12485, 14755, 14755, 14755], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[12485, 14755, 14755, 14755], unbound=0, use_sep=False, weight_decay=0.01)
2023-05-30 17:14:13,518 - 0:00:05 - 0.0s - INFO - __main__ - start to train { task: ['sst'], seq train type: lll }
2023-05-30 17:14:13,519 - 0:00:05 - 0.0s - INFO - __main__ - extra training data size: 0
2023-05-30 17:14:16,325 - 0:00:08 - 2.8s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
2023-05-30 17:14:19,790 - 0:00:11 - 3.5s - INFO - __main__ - len of train dataset: 6920 , max train batch size 4 , num of opt steps: 138400
/home/student/2021/cs21mtech11006/anaconda3/envs/lamol/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/home/student/2021/cs21mtech11006/anaconda3/envs/lamol/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
2023-05-30 17:16:47,899 - 0:02:39 - 148.1s - INFO - __main__ - progress 0.578 , lr 6.1E-05 , loss 2.768 , qa loss 2.768 , lm loss 0.000 , avg batch size 4.0
2023-05-30 17:18:48,727 - 0:04:40 - 120.8s - INFO - __main__ - epoch 1/20 done , tot steps 1730 , lr 5.9E-05 , loss 1.70 , qa loss 1.70 , lm loss 0.00 , avg batch size 4.0
2023-05-30 17:21:24,127 - 0:07:16 - 155.4s - INFO - __main__ - progress 1.578 , lr 5.8E-05 , loss 0.229 , qa loss 0.229 , lm loss 0.000 , avg batch size 4.0
2023-05-30 17:23:17,821 - 0:09:09 - 113.7s - INFO - __main__ - epoch 2/20 done , tot steps 3460 , lr 5.6E-05 , loss 0.22 , qa loss 0.22 , lm loss 0.00 , avg batch size 4.0
2023-05-30 17:25:53,731 - 0:11:45 - 155.9s - INFO - __main__ - progress 2.578 , lr 5.4E-05 , loss 0.168 , qa loss 0.168 , lm loss 0.000 , avg batch size 4.0
2023-05-30 17:27:48,924 - 0:13:40 - 115.2s - INFO - __main__ - epoch 3/20 done , tot steps 5190 , lr 5.3E-05 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
2023-05-30 17:30:26,374 - 0:16:18 - 157.5s - INFO - __main__ - progress 3.578 , lr 5.1E-05 , loss 0.151 , qa loss 0.151 , lm loss 0.000 , avg batch size 4.0
2023-05-30 17:32:20,834 - 0:18:12 - 114.5s - INFO - __main__ - epoch 4/20 done , tot steps 6920 , lr 5.0E-05 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-05-30 17:34:57,299 - 0:20:49 - 156.5s - INFO - __main__ - progress 4.578 , lr 4.8E-05 , loss 0.138 , qa loss 0.138 , lm loss 0.000 , avg batch size 4.0
2023-05-30 17:36:51,568 - 0:22:43 - 114.3s - INFO - __main__ - epoch 5/20 done , tot steps 8650 , lr 4.7E-05 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
2023-05-30 17:39:28,278 - 0:25:20 - 156.7s - INFO - __main__ - progress 5.578 , lr 4.5E-05 , loss 0.121 , qa loss 0.121 , lm loss 0.000 , avg batch size 4.0
2023-05-30 17:41:22,695 - 0:27:14 - 114.4s - INFO - __main__ - epoch 6/20 done , tot steps 10380 , lr 4.4E-05 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2023-05-30 17:44:00,686 - 0:29:52 - 158.0s - INFO - __main__ - progress 6.578 , lr 4.2E-05 , loss 0.103 , qa loss 0.103 , lm loss 0.000 , avg batch size 4.0
2023-05-30 17:45:53,889 - 0:31:45 - 113.2s - INFO - __main__ - epoch 7/20 done , tot steps 12110 , lr 4.1E-05 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 4.0
2023-05-30 17:48:31,386 - 0:34:23 - 157.5s - INFO - __main__ - progress 7.578 , lr 3.9E-05 , loss 0.091 , qa loss 0.091 , lm loss 0.000 , avg batch size 4.0
2023-05-30 17:50:24,674 - 0:36:16 - 113.3s - INFO - __main__ - epoch 8/20 done , tot steps 13840 , lr 3.8E-05 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 4.0
2023-05-30 17:53:00,325 - 0:38:52 - 155.7s - INFO - __main__ - progress 8.578 , lr 3.6E-05 , loss 0.077 , qa loss 0.077 , lm loss 0.000 , avg batch size 4.0
2023-05-30 17:54:54,841 - 0:40:46 - 114.5s - INFO - __main__ - epoch 9/20 done , tot steps 15570 , lr 3.4E-05 , loss 0.08 , qa loss 0.08 , lm loss 0.00 , avg batch size 4.0
2023-05-30 17:57:31,538 - 0:43:23 - 156.7s - INFO - __main__ - progress 9.578 , lr 3.3E-05 , loss 0.067 , qa loss 0.067 , lm loss 0.000 , avg batch size 4.0
2023-05-30 17:59:26,470 - 0:45:18 - 114.9s - INFO - __main__ - epoch 10/20 done , tot steps 17300 , lr 3.1E-05 , loss 0.07 , qa loss 0.07 , lm loss 0.00 , avg batch size 4.0
2023-05-30 18:02:03,901 - 0:47:55 - 157.4s - INFO - __main__ - progress 10.578 , lr 2.9E-05 , loss 0.062 , qa loss 0.062 , lm loss 0.000 , avg batch size 4.0
2023-05-30 18:03:56,939 - 0:49:48 - 113.0s - INFO - __main__ - epoch 11/20 done , tot steps 19030 , lr 2.8E-05 , loss 0.06 , qa loss 0.06 , lm loss 0.00 , avg batch size 4.0
2023-05-30 18:06:32,625 - 0:52:24 - 155.7s - INFO - __main__ - progress 11.578 , lr 2.6E-05 , loss 0.057 , qa loss 0.057 , lm loss 0.000 , avg batch size 4.0
2023-05-30 18:08:27,667 - 0:54:19 - 115.0s - INFO - __main__ - epoch 12/20 done , tot steps 20760 , lr 2.5E-05 , loss 0.06 , qa loss 0.06 , lm loss 0.00 , avg batch size 4.0
2023-05-30 18:11:05,774 - 0:56:57 - 158.1s - INFO - __main__ - progress 12.578 , lr 2.3E-05 , loss 0.048 , qa loss 0.048 , lm loss 0.000 , avg batch size 4.0
2023-05-30 18:12:58,989 - 0:58:50 - 113.2s - INFO - __main__ - epoch 13/20 done , tot steps 22490 , lr 2.2E-05 , loss 0.05 , qa loss 0.05 , lm loss 0.00 , avg batch size 4.0
2023-05-30 18:15:36,814 - 1:01:28 - 157.8s - INFO - __main__ - progress 13.578 , lr 2.0E-05 , loss 0.049 , qa loss 0.049 , lm loss 0.000 , avg batch size 4.0
2023-05-30 18:17:30,995 - 1:03:22 - 114.2s - INFO - __main__ - epoch 14/20 done , tot steps 24220 , lr 1.9E-05 , loss 0.05 , qa loss 0.05 , lm loss 0.00 , avg batch size 4.0
2023-05-30 18:20:08,924 - 1:06:00 - 157.9s - INFO - __main__ - progress 14.578 , lr 1.7E-05 , loss 0.045 , qa loss 0.045 , lm loss 0.000 , avg batch size 4.0
2023-05-30 18:22:04,538 - 1:07:56 - 115.6s - INFO - __main__ - epoch 15/20 done , tot steps 25950 , lr 1.6E-05 , loss 0.04 , qa loss 0.04 , lm loss 0.00 , avg batch size 4.0
2023-05-30 18:24:42,746 - 1:10:34 - 158.2s - INFO - __main__ - progress 15.578 , lr 1.4E-05 , loss 0.032 , qa loss 0.032 , lm loss 0.000 , avg batch size 4.0
2023-05-30 18:26:35,910 - 1:12:27 - 113.2s - INFO - __main__ - epoch 16/20 done , tot steps 27680 , lr 1.3E-05 , loss 0.04 , qa loss 0.04 , lm loss 0.00 , avg batch size 4.0
2023-05-30 18:29:13,736 - 1:15:05 - 157.8s - INFO - __main__ - progress 16.578 , lr 1.1E-05 , loss 0.036 , qa loss 0.036 , lm loss 0.000 , avg batch size 4.0
2023-05-30 18:31:06,318 - 1:16:58 - 112.6s - INFO - __main__ - epoch 17/20 done , tot steps 29410 , lr 9.4E-06 , loss 0.03 , qa loss 0.03 , lm loss 0.00 , avg batch size 4.0
2023-05-30 18:33:43,061 - 1:19:35 - 156.7s - INFO - __main__ - progress 17.578 , lr 7.6E-06 , loss 0.032 , qa loss 0.032 , lm loss 0.000 , avg batch size 4.0
2023-05-30 18:35:36,832 - 1:21:28 - 113.8s - INFO - __main__ - epoch 18/20 done , tot steps 31140 , lr 6.3E-06 , loss 0.03 , qa loss 0.03 , lm loss 0.00 , avg batch size 4.0
2023-05-30 18:38:15,985 - 1:24:07 - 159.2s - INFO - __main__ - progress 18.578 , lr 4.5E-06 , loss 0.030 , qa loss 0.030 , lm loss 0.000 , avg batch size 4.0
2023-05-30 18:40:08,387 - 1:26:00 - 112.4s - INFO - __main__ - epoch 19/20 done , tot steps 32870 , lr 3.1E-06 , loss 0.03 , qa loss 0.03 , lm loss 0.00 , avg batch size 4.0
2023-05-30 18:42:45,277 - 1:28:37 - 156.9s - INFO - __main__ - progress 19.578 , lr 1.3E-06 , loss 0.021 , qa loss 0.021 , lm loss 0.000 , avg batch size 4.0
2023-05-30 18:44:37,105 - 1:30:29 - 111.8s - INFO - __main__ - epoch 20/20 done , tot steps 34600 , lr 1.5E-08 , loss 0.03 , qa loss 0.03 , lm loss 0.00 , avg batch size 4.0
2023-05-30 18:44:44,266 - 1:30:36 - 7.2s - INFO - __main__ - start to train { task: ['srl'], seq train type: lll }
2023-05-30 18:44:48,130 - 1:30:40 - 3.9s - INFO - utils - extra data exists in ../../model_cm1/gpt2/lll/sst_srl_woz.en_0.0/sst/lm.csv, read it!
2023-05-30 18:44:48,131 - 1:30:40 - 0.0s - INFO - __main__ - extra training data size: 0
2023-05-30 18:44:48,260 - 1:30:40 - 0.1s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[sst]
The task with which model is saved sst
2023-05-30 18:44:51,994 - 1:30:43 - 3.7s - INFO - __main__ - len of train dataset: 6414 , max train batch size 4 , num of opt steps: 128280
2023-05-30 18:49:40,413 - 1:35:32 - 288.4s - INFO - __main__ - progress 0.624 , lr 6.1E-05 , loss 3.085 , qa loss 3.085 , lm loss 0.000 , avg batch size 4.0
2023-05-30 18:52:38,096 - 1:38:30 - 177.7s - INFO - __main__ - epoch 1/20 done , tot steps 1604 , lr 5.9E-05 , loss 2.24 , qa loss 2.24 , lm loss 0.00 , avg batch size 4.0
2023-05-30 18:57:38,743 - 1:43:30 - 300.6s - INFO - __main__ - progress 1.624 , lr 5.7E-05 , loss 0.739 , qa loss 0.739 , lm loss 0.000 , avg batch size 4.0
2023-05-30 19:00:26,125 - 1:46:18 - 167.4s - INFO - __main__ - epoch 2/20 done , tot steps 3208 , lr 5.6E-05 , loss 0.72 , qa loss 0.72 , lm loss 0.00 , avg batch size 4.0
2023-05-30 19:05:28,550 - 1:51:20 - 302.4s - INFO - __main__ - progress 2.624 , lr 5.4E-05 , loss 0.617 , qa loss 0.617 , lm loss 0.000 , avg batch size 4.0
2023-05-30 19:08:17,556 - 1:54:09 - 169.0s - INFO - __main__ - epoch 3/20 done , tot steps 4812 , lr 5.3E-05 , loss 0.60 , qa loss 0.60 , lm loss 0.00 , avg batch size 4.0
2023-05-30 19:13:04,398 - 1:58:56 - 286.8s - INFO - __main__ - progress 3.624 , lr 5.1E-05 , loss 0.539 , qa loss 0.539 , lm loss 0.000 , avg batch size 4.0
2023-05-30 19:16:01,444 - 2:01:53 - 177.0s - INFO - __main__ - epoch 4/20 done , tot steps 6416 , lr 5.0E-05 , loss 0.53 , qa loss 0.53 , lm loss 0.00 , avg batch size 4.0
2023-05-30 19:20:52,413 - 2:06:44 - 291.0s - INFO - __main__ - progress 4.624 , lr 4.8E-05 , loss 0.479 , qa loss 0.479 , lm loss 0.000 , avg batch size 4.0
2023-05-30 19:23:48,657 - 2:09:40 - 176.2s - INFO - __main__ - epoch 5/20 done , tot steps 8020 , lr 4.7E-05 , loss 0.48 , qa loss 0.48 , lm loss 0.00 , avg batch size 4.0
2023-05-30 19:28:35,822 - 2:14:27 - 287.2s - INFO - __main__ - progress 5.624 , lr 4.5E-05 , loss 0.431 , qa loss 0.431 , lm loss 0.000 , avg batch size 4.0
2023-05-30 19:31:33,035 - 2:17:25 - 177.2s - INFO - __main__ - epoch 6/20 done , tot steps 9624 , lr 4.4E-05 , loss 0.43 , qa loss 0.43 , lm loss 0.00 , avg batch size 4.0
2023-05-30 19:36:23,996 - 2:22:15 - 291.0s - INFO - __main__ - progress 6.624 , lr 4.2E-05 , loss 0.402 , qa loss 0.402 , lm loss 0.000 , avg batch size 4.0
2023-05-30 19:39:19,087 - 2:25:11 - 175.1s - INFO - __main__ - epoch 7/20 done , tot steps 11228 , lr 4.1E-05 , loss 0.40 , qa loss 0.40 , lm loss 0.00 , avg batch size 4.0
2023-05-30 19:44:10,266 - 2:30:02 - 291.2s - INFO - __main__ - progress 7.624 , lr 3.9E-05 , loss 0.370 , qa loss 0.370 , lm loss 0.000 , avg batch size 4.0
2023-05-30 19:47:04,974 - 2:32:56 - 174.7s - INFO - __main__ - epoch 8/20 done , tot steps 12832 , lr 3.8E-05 , loss 0.37 , qa loss 0.37 , lm loss 0.00 , avg batch size 4.0
2023-05-30 19:51:55,149 - 2:37:47 - 290.2s - INFO - __main__ - progress 8.624 , lr 3.6E-05 , loss 0.326 , qa loss 0.326 , lm loss 0.000 , avg batch size 4.0
2023-05-30 19:54:57,592 - 2:40:49 - 182.4s - INFO - __main__ - epoch 9/20 done , tot steps 14436 , lr 3.4E-05 , loss 0.33 , qa loss 0.33 , lm loss 0.00 , avg batch size 4.0
2023-05-30 19:59:51,997 - 2:45:43 - 294.4s - INFO - __main__ - progress 9.624 , lr 3.2E-05 , loss 0.312 , qa loss 0.312 , lm loss 0.000 , avg batch size 4.0
2023-05-30 20:02:44,016 - 2:48:36 - 172.0s - INFO - __main__ - epoch 10/20 done , tot steps 16040 , lr 3.1E-05 , loss 0.31 , qa loss 0.31 , lm loss 0.00 , avg batch size 4.0
2023-05-30 20:07:35,184 - 2:53:27 - 291.2s - INFO - __main__ - progress 10.624 , lr 2.9E-05 , loss 0.282 , qa loss 0.282 , lm loss 0.000 , avg batch size 4.0
2023-05-30 20:10:30,004 - 2:56:21 - 174.8s - INFO - __main__ - epoch 11/20 done , tot steps 17644 , lr 2.8E-05 , loss 0.29 , qa loss 0.29 , lm loss 0.00 , avg batch size 4.0
2023-05-30 20:15:23,972 - 3:01:15 - 294.0s - INFO - __main__ - progress 11.624 , lr 2.6E-05 , loss 0.265 , qa loss 0.265 , lm loss 0.000 , avg batch size 4.0
2023-05-30 20:18:20,211 - 3:04:12 - 176.2s - INFO - __main__ - epoch 12/20 done , tot steps 19248 , lr 2.5E-05 , loss 0.27 , qa loss 0.27 , lm loss 0.00 , avg batch size 4.0
2023-05-30 20:23:15,634 - 3:09:07 - 295.4s - INFO - __main__ - progress 12.624 , lr 2.3E-05 , loss 0.257 , qa loss 0.257 , lm loss 0.000 , avg batch size 4.0
2023-05-30 20:26:06,009 - 3:11:58 - 170.4s - INFO - __main__ - epoch 13/20 done , tot steps 20852 , lr 2.2E-05 , loss 0.26 , qa loss 0.26 , lm loss 0.00 , avg batch size 4.0
2023-05-30 20:30:57,324 - 3:16:49 - 291.3s - INFO - __main__ - progress 13.624 , lr 2.0E-05 , loss 0.248 , qa loss 0.248 , lm loss 0.000 , avg batch size 4.0
2023-05-30 20:33:52,730 - 3:19:44 - 175.4s - INFO - __main__ - epoch 14/20 done , tot steps 22456 , lr 1.9E-05 , loss 0.24 , qa loss 0.24 , lm loss 0.00 , avg batch size 4.0
2023-05-30 20:38:51,915 - 3:24:43 - 299.2s - INFO - __main__ - progress 14.624 , lr 1.7E-05 , loss 0.236 , qa loss 0.236 , lm loss 0.000 , avg batch size 4.0
2023-05-30 20:41:39,617 - 3:27:31 - 167.7s - INFO - __main__ - epoch 15/20 done , tot steps 24060 , lr 1.6E-05 , loss 0.23 , qa loss 0.23 , lm loss 0.00 , avg batch size 4.0
2023-05-30 20:46:32,272 - 3:32:24 - 292.7s - INFO - __main__ - progress 15.624 , lr 1.4E-05 , loss 0.216 , qa loss 0.216 , lm loss 0.000 , avg batch size 4.0
2023-05-30 20:49:25,819 - 3:35:17 - 173.5s - INFO - __main__ - epoch 16/20 done , tot steps 25664 , lr 1.3E-05 , loss 0.22 , qa loss 0.22 , lm loss 0.00 , avg batch size 4.0
2023-05-30 20:54:19,197 - 3:40:11 - 293.4s - INFO - __main__ - progress 16.624 , lr 1.1E-05 , loss 0.214 , qa loss 0.214 , lm loss 0.000 , avg batch size 4.0
2023-05-30 20:57:11,663 - 3:43:03 - 172.5s - INFO - __main__ - epoch 17/20 done , tot steps 27268 , lr 9.4E-06 , loss 0.21 , qa loss 0.21 , lm loss 0.00 , avg batch size 4.0
2023-05-30 21:02:08,997 - 3:48:00 - 297.3s - INFO - __main__ - progress 17.624 , lr 7.4E-06 , loss 0.206 , qa loss 0.206 , lm loss 0.000 , avg batch size 4.0
2023-05-30 21:05:01,378 - 3:50:53 - 172.4s - INFO - __main__ - epoch 18/20 done , tot steps 28872 , lr 6.3E-06 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-05-30 21:09:55,005 - 3:55:46 - 293.6s - INFO - __main__ - progress 18.624 , lr 4.3E-06 , loss 0.194 , qa loss 0.194 , lm loss 0.000 , avg batch size 4.0
2023-05-30 21:12:46,387 - 3:58:38 - 171.4s - INFO - __main__ - epoch 19/20 done , tot steps 30476 , lr 3.1E-06 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-05-30 21:17:39,975 - 4:03:31 - 293.6s - INFO - __main__ - progress 19.624 , lr 1.2E-06 , loss 0.194 , qa loss 0.194 , lm loss 0.000 , avg batch size 4.0
2023-05-30 21:20:17,797 - 4:06:09 - 157.8s - INFO - __main__ - epoch 20/20 done , tot steps 32080 , lr 1.6E-08 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-05-30 21:20:24,680 - 4:06:16 - 6.9s - INFO - __main__ - start to train { task: ['woz.en'], seq train type: lll }
2023-05-30 21:20:29,700 - 4:06:21 - 5.0s - INFO - utils - writing extra data in ../../model_cm1/gpt2/lll/sst_srl_woz.en_0.0/srl/lm.csv ...
2023-05-30 21:20:29,701 - 4:06:21 - 0.0s - INFO - __main__ - extra training data size: 0
2023-05-30 21:20:30,046 - 4:06:22 - 0.3s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[srl]
The task with which model is saved srl
2023-05-30 21:20:34,456 - 4:06:26 - 4.4s - INFO - __main__ - len of train dataset: 2536 , max train batch size 4 , num of opt steps: 50720
2023-05-30 21:22:12,009 - 4:08:04 - 97.6s - INFO - __main__ - epoch 1/20 done , tot steps 634 , lr 5.9E-05 , loss 3.44 , qa loss 3.44 , lm loss 0.00 , avg batch size 4.0
2023-05-30 21:23:45,006 - 4:09:36 - 93.0s - INFO - __main__ - epoch 2/20 done , tot steps 1268 , lr 5.6E-05 , loss 0.42 , qa loss 0.42 , lm loss 0.00 , avg batch size 4.0
2023-05-30 21:25:17,954 - 4:11:09 - 92.9s - INFO - __main__ - epoch 3/20 done , tot steps 1902 , lr 5.3E-05 , loss 0.30 , qa loss 0.30 , lm loss 0.00 , avg batch size 4.0
2023-05-30 21:26:50,427 - 4:12:42 - 92.5s - INFO - __main__ - epoch 4/20 done , tot steps 2536 , lr 5.0E-05 , loss 0.24 , qa loss 0.24 , lm loss 0.00 , avg batch size 4.0
2023-05-30 21:28:22,895 - 4:14:14 - 92.5s - INFO - __main__ - epoch 5/20 done , tot steps 3170 , lr 4.7E-05 , loss 0.21 , qa loss 0.21 , lm loss 0.00 , avg batch size 4.0
2023-05-30 21:29:55,361 - 4:15:47 - 92.5s - INFO - __main__ - epoch 6/20 done , tot steps 3804 , lr 4.4E-05 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-05-30 21:31:28,299 - 4:17:20 - 92.9s - INFO - __main__ - epoch 7/20 done , tot steps 4438 , lr 4.1E-05 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-05-30 21:33:00,875 - 4:18:52 - 92.6s - INFO - __main__ - epoch 8/20 done , tot steps 5072 , lr 3.8E-05 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-05-30 21:34:33,679 - 4:20:25 - 92.8s - INFO - __main__ - epoch 9/20 done , tot steps 5706 , lr 3.4E-05 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
2023-05-30 21:36:06,171 - 4:21:58 - 92.5s - INFO - __main__ - epoch 10/20 done , tot steps 6340 , lr 3.1E-05 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-05-30 21:37:38,452 - 4:23:30 - 92.3s - INFO - __main__ - epoch 11/20 done , tot steps 6974 , lr 2.8E-05 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2023-05-30 21:39:10,835 - 4:25:02 - 92.4s - INFO - __main__ - epoch 12/20 done , tot steps 7608 , lr 2.5E-05 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-05-30 21:40:43,116 - 4:26:35 - 92.3s - INFO - __main__ - epoch 13/20 done , tot steps 8242 , lr 2.2E-05 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-05-30 21:42:15,874 - 4:28:07 - 92.8s - INFO - __main__ - epoch 14/20 done , tot steps 8876 , lr 1.9E-05 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-05-30 21:43:49,013 - 4:29:41 - 93.1s - INFO - __main__ - epoch 15/20 done , tot steps 9510 , lr 1.6E-05 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 4.0
2023-05-30 21:45:21,631 - 4:31:13 - 92.6s - INFO - __main__ - epoch 16/20 done , tot steps 10144 , lr 1.3E-05 , loss 0.09 , qa loss 0.09 , lm loss 0.00 , avg batch size 4.0
2023-05-30 21:46:54,193 - 4:32:46 - 92.6s - INFO - __main__ - epoch 17/20 done , tot steps 10778 , lr 9.4E-06 , loss 0.09 , qa loss 0.09 , lm loss 0.00 , avg batch size 4.0
2023-05-30 21:48:26,389 - 4:34:18 - 92.2s - INFO - __main__ - epoch 18/20 done , tot steps 11412 , lr 6.3E-06 , loss 0.09 , qa loss 0.09 , lm loss 0.00 , avg batch size 4.0
2023-05-30 21:49:58,977 - 4:35:50 - 92.6s - INFO - __main__ - epoch 19/20 done , tot steps 12046 , lr 3.1E-06 , loss 0.09 , qa loss 0.09 , lm loss 0.00 , avg batch size 4.0
2023-05-30 21:51:23,588 - 4:37:15 - 84.6s - INFO - __main__ - epoch 20/20 done , tot steps 12680 , lr 1.5E-08 , loss 0.08 , qa loss 0.08 , lm loss 0.00 , avg batch size 4.0
The current active adapter is Stack[woz_en]
The task with which model is saved woz_en
Wall Execution time: 04:37:16
CPU Execution time: 04:38:43
