Not all gpus support fp16 training! Will use fp32 instead.
2023-05-30 17:14:43,167 - 0:00:25 - 0.0s - INFO - __main__ - args = Namespace(REG_TYPE_KEYS=['mas', 'ewc'], adam_epsilon=0.0001, add_task_tokens=False, data_dir='../../data', debug=False, decay_style='linear', device_ids=[1, 2, 3, 4], dynamic_epochs=False, fp32=True, gen_lm_sample_percentage=0.0, learning_rate=6.25e-05, lm_lambda=0.0, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[35672.56, 42158.48, 42158.48, 42158.48], min_batch_size=4, min_n_steps=1500, model_dir_root='../../model_am1/gpt2/lll/sst_srl_woz.en_0.0', model_name='gpt2', n_gpus=4, n_train_epochs={'sst': 20, 'srl': 20, 'woz.en': 20}, n_warmup_ratio=0.005, n_workers=75, qp_margin=0.5, real_sample=False, reg_lambda=1.0, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[12485, 14755, 14755, 14755], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[12485, 14755, 14755, 14755], unbound=0, use_sep=False, weight_decay=0.01)
2023-05-30 17:14:43,167 - 0:00:25 - 0.0s - INFO - __main__ - start to train { task: ['sst'], seq train type: lll }
2023-05-30 17:14:43,167 - 0:00:25 - 0.0s - INFO - __main__ - extra training data size: 0
2023-05-30 17:14:46,078 - 0:00:28 - 2.9s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
2023-05-30 17:14:48,202 - 0:00:30 - 2.1s - INFO - __main__ - len of train dataset: 6920 , max train batch size 4 , num of opt steps: 138400
/home/student/2021/cs21mtech11006/anaconda3/envs/lamol/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/home/student/2021/cs21mtech11006/anaconda3/envs/lamol/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
2023-05-30 17:17:26,881 - 0:03:08 - 158.7s - INFO - __main__ - progress 0.578 , lr 6.1E-05 , loss 2.143 , qa loss 2.143 , lm loss 0.000 , avg batch size 4.0
2023-05-30 17:19:22,938 - 0:05:05 - 116.1s - INFO - __main__ - epoch 1/20 done , tot steps 1730 , lr 5.9E-05 , loss 1.34 , qa loss 1.34 , lm loss 0.00 , avg batch size 4.0
2023-05-30 17:22:00,494 - 0:07:42 - 157.6s - INFO - __main__ - progress 1.578 , lr 5.8E-05 , loss 0.205 , qa loss 0.205 , lm loss 0.000 , avg batch size 4.0
2023-05-30 17:23:52,436 - 0:09:34 - 111.9s - INFO - __main__ - epoch 2/20 done , tot steps 3460 , lr 5.6E-05 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-05-30 17:26:28,331 - 0:12:10 - 155.9s - INFO - __main__ - progress 2.578 , lr 5.4E-05 , loss 0.176 , qa loss 0.176 , lm loss 0.000 , avg batch size 4.0
2023-05-30 17:28:21,364 - 0:14:03 - 113.0s - INFO - __main__ - epoch 3/20 done , tot steps 5190 , lr 5.3E-05 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
2023-05-30 17:30:56,597 - 0:16:38 - 155.2s - INFO - __main__ - progress 3.578 , lr 5.1E-05 , loss 0.160 , qa loss 0.160 , lm loss 0.000 , avg batch size 4.0
2023-05-30 17:32:51,297 - 0:18:33 - 114.7s - INFO - __main__ - epoch 4/20 done , tot steps 6920 , lr 5.0E-05 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-05-30 17:35:27,543 - 0:21:09 - 156.2s - INFO - __main__ - progress 4.578 , lr 4.8E-05 , loss 0.130 , qa loss 0.130 , lm loss 0.000 , avg batch size 4.0
2023-05-30 17:37:18,812 - 0:23:00 - 111.3s - INFO - __main__ - epoch 5/20 done , tot steps 8650 , lr 4.7E-05 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-05-30 17:39:54,604 - 0:25:36 - 155.8s - INFO - __main__ - progress 5.578 , lr 4.5E-05 , loss 0.119 , qa loss 0.119 , lm loss 0.000 , avg batch size 4.0
2023-05-30 17:41:47,422 - 0:27:29 - 112.8s - INFO - __main__ - epoch 6/20 done , tot steps 10380 , lr 4.4E-05 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2023-05-30 17:44:23,995 - 0:30:06 - 156.6s - INFO - __main__ - progress 6.578 , lr 4.2E-05 , loss 0.111 , qa loss 0.111 , lm loss 0.000 , avg batch size 4.0
2023-05-30 17:46:16,132 - 0:31:58 - 112.1s - INFO - __main__ - epoch 7/20 done , tot steps 12110 , lr 4.1E-05 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-05-30 17:48:52,541 - 0:34:34 - 156.4s - INFO - __main__ - progress 7.578 , lr 3.9E-05 , loss 0.088 , qa loss 0.088 , lm loss 0.000 , avg batch size 4.0
2023-05-30 17:50:44,429 - 0:36:26 - 111.9s - INFO - __main__ - epoch 8/20 done , tot steps 13840 , lr 3.8E-05 , loss 0.09 , qa loss 0.09 , lm loss 0.00 , avg batch size 4.0
2023-05-30 17:53:19,530 - 0:39:01 - 155.1s - INFO - __main__ - progress 8.578 , lr 3.6E-05 , loss 0.084 , qa loss 0.084 , lm loss 0.000 , avg batch size 4.0
2023-05-30 17:55:11,778 - 0:40:53 - 112.2s - INFO - __main__ - epoch 9/20 done , tot steps 15570 , lr 3.4E-05 , loss 0.08 , qa loss 0.08 , lm loss 0.00 , avg batch size 4.0
2023-05-30 17:57:46,465 - 0:43:28 - 154.7s - INFO - __main__ - progress 9.578 , lr 3.3E-05 , loss 0.071 , qa loss 0.071 , lm loss 0.000 , avg batch size 4.0
2023-05-30 17:59:39,839 - 0:45:21 - 113.4s - INFO - __main__ - epoch 10/20 done , tot steps 17300 , lr 3.1E-05 , loss 0.07 , qa loss 0.07 , lm loss 0.00 , avg batch size 4.0
2023-05-30 18:02:15,916 - 0:47:58 - 156.1s - INFO - __main__ - progress 10.578 , lr 2.9E-05 , loss 0.066 , qa loss 0.066 , lm loss 0.000 , avg batch size 4.0
2023-05-30 18:04:07,443 - 0:49:49 - 111.5s - INFO - __main__ - epoch 11/20 done , tot steps 19030 , lr 2.8E-05 , loss 0.06 , qa loss 0.06 , lm loss 0.00 , avg batch size 4.0
2023-05-30 18:06:42,926 - 0:52:25 - 155.5s - INFO - __main__ - progress 11.578 , lr 2.6E-05 , loss 0.054 , qa loss 0.054 , lm loss 0.000 , avg batch size 4.0
2023-05-30 18:08:34,298 - 0:54:16 - 111.4s - INFO - __main__ - epoch 12/20 done , tot steps 20760 , lr 2.5E-05 , loss 0.06 , qa loss 0.06 , lm loss 0.00 , avg batch size 4.0
2023-05-30 18:11:10,578 - 0:56:52 - 156.3s - INFO - __main__ - progress 12.578 , lr 2.3E-05 , loss 0.047 , qa loss 0.047 , lm loss 0.000 , avg batch size 4.0
2023-05-30 18:13:03,285 - 0:58:45 - 112.7s - INFO - __main__ - epoch 13/20 done , tot steps 22490 , lr 2.2E-05 , loss 0.05 , qa loss 0.05 , lm loss 0.00 , avg batch size 4.0
2023-05-30 18:15:40,356 - 1:01:22 - 157.1s - INFO - __main__ - progress 13.578 , lr 2.0E-05 , loss 0.040 , qa loss 0.040 , lm loss 0.000 , avg batch size 4.0
2023-05-30 18:17:33,124 - 1:03:15 - 112.8s - INFO - __main__ - epoch 14/20 done , tot steps 24220 , lr 1.9E-05 , loss 0.05 , qa loss 0.05 , lm loss 0.00 , avg batch size 4.0
2023-05-30 18:20:08,314 - 1:05:50 - 155.2s - INFO - __main__ - progress 14.578 , lr 1.7E-05 , loss 0.043 , qa loss 0.043 , lm loss 0.000 , avg batch size 4.0
2023-05-30 18:22:03,926 - 1:07:46 - 115.6s - INFO - __main__ - epoch 15/20 done , tot steps 25950 , lr 1.6E-05 , loss 0.04 , qa loss 0.04 , lm loss 0.00 , avg batch size 4.0
2023-05-30 18:24:40,291 - 1:10:22 - 156.4s - INFO - __main__ - progress 15.578 , lr 1.4E-05 , loss 0.038 , qa loss 0.038 , lm loss 0.000 , avg batch size 4.0
2023-05-30 18:26:33,100 - 1:12:15 - 112.8s - INFO - __main__ - epoch 16/20 done , tot steps 27680 , lr 1.3E-05 , loss 0.04 , qa loss 0.04 , lm loss 0.00 , avg batch size 4.0
2023-05-30 18:29:07,287 - 1:14:49 - 154.2s - INFO - __main__ - progress 16.578 , lr 1.1E-05 , loss 0.038 , qa loss 0.038 , lm loss 0.000 , avg batch size 4.0
2023-05-30 18:31:01,321 - 1:16:43 - 114.0s - INFO - __main__ - epoch 17/20 done , tot steps 29410 , lr 9.4E-06 , loss 0.04 , qa loss 0.04 , lm loss 0.00 , avg batch size 4.0
2023-05-30 18:33:33,876 - 1:19:15 - 152.6s - INFO - __main__ - progress 17.578 , lr 7.6E-06 , loss 0.028 , qa loss 0.028 , lm loss 0.000 , avg batch size 4.0
2023-05-30 18:35:30,195 - 1:21:12 - 116.3s - INFO - __main__ - epoch 18/20 done , tot steps 31140 , lr 6.3E-06 , loss 0.03 , qa loss 0.03 , lm loss 0.00 , avg batch size 4.0
2023-05-30 18:38:05,074 - 1:23:47 - 154.9s - INFO - __main__ - progress 18.578 , lr 4.5E-06 , loss 0.020 , qa loss 0.020 , lm loss 0.000 , avg batch size 4.0
2023-05-30 18:39:58,983 - 1:25:41 - 113.9s - INFO - __main__ - epoch 19/20 done , tot steps 32870 , lr 3.1E-06 , loss 0.03 , qa loss 0.03 , lm loss 0.00 , avg batch size 4.0
2023-05-30 18:42:31,497 - 1:28:13 - 152.5s - INFO - __main__ - progress 19.578 , lr 1.3E-06 , loss 0.028 , qa loss 0.028 , lm loss 0.000 , avg batch size 4.0
2023-05-30 18:44:27,073 - 1:30:09 - 115.6s - INFO - __main__ - epoch 20/20 done , tot steps 34600 , lr 1.5E-08 , loss 0.03 , qa loss 0.03 , lm loss 0.00 , avg batch size 4.0
2023-05-30 18:44:34,079 - 1:30:16 - 7.0s - INFO - __main__ - start to train { task: ['srl'], seq train type: lll }
2023-05-30 18:44:39,258 - 1:30:21 - 5.2s - INFO - utils - extra data exists in ../../model_am1/gpt2/lll/sst_srl_woz.en_0.0/sst/lm.csv, read it!
2023-05-30 18:44:39,279 - 1:30:21 - 0.0s - INFO - __main__ - extra training data size: 0
2023-05-30 18:44:39,838 - 1:30:21 - 0.6s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[sst]
The task with which model is saved sst
2023-05-30 18:44:43,600 - 1:30:25 - 3.8s - INFO - __main__ - len of train dataset: 6414 , max train batch size 4 , num of opt steps: 128280
2023-05-30 18:49:31,577 - 1:35:13 - 288.0s - INFO - __main__ - progress 0.624 , lr 6.1E-05 , loss 3.194 , qa loss 3.194 , lm loss 0.000 , avg batch size 4.0
2023-05-30 18:52:29,572 - 1:38:11 - 178.0s - INFO - __main__ - epoch 1/20 done , tot steps 1604 , lr 5.9E-05 , loss 2.32 , qa loss 2.32 , lm loss 0.00 , avg batch size 4.0
2023-05-30 18:57:21,938 - 1:43:04 - 292.4s - INFO - __main__ - progress 1.624 , lr 5.7E-05 , loss 0.747 , qa loss 0.747 , lm loss 0.000 , avg batch size 4.0
2023-05-30 19:00:15,595 - 1:45:57 - 173.7s - INFO - __main__ - epoch 2/20 done , tot steps 3208 , lr 5.6E-05 , loss 0.73 , qa loss 0.73 , lm loss 0.00 , avg batch size 4.0
2023-05-30 19:05:07,926 - 1:50:50 - 292.3s - INFO - __main__ - progress 2.624 , lr 5.4E-05 , loss 0.598 , qa loss 0.598 , lm loss 0.000 , avg batch size 4.0
2023-05-30 19:08:01,185 - 1:53:43 - 173.3s - INFO - __main__ - epoch 3/20 done , tot steps 4812 , lr 5.3E-05 , loss 0.60 , qa loss 0.60 , lm loss 0.00 , avg batch size 4.0
2023-05-30 19:12:48,146 - 1:58:30 - 287.0s - INFO - __main__ - progress 3.624 , lr 5.1E-05 , loss 0.545 , qa loss 0.545 , lm loss 0.000 , avg batch size 4.0
2023-05-30 19:15:45,945 - 2:01:28 - 177.8s - INFO - __main__ - epoch 4/20 done , tot steps 6416 , lr 5.0E-05 , loss 0.53 , qa loss 0.53 , lm loss 0.00 , avg batch size 4.0
2023-05-30 19:20:33,854 - 2:06:15 - 287.9s - INFO - __main__ - progress 4.624 , lr 4.8E-05 , loss 0.482 , qa loss 0.482 , lm loss 0.000 , avg batch size 4.0
2023-05-30 19:23:29,152 - 2:09:11 - 175.3s - INFO - __main__ - epoch 5/20 done , tot steps 8020 , lr 4.7E-05 , loss 0.48 , qa loss 0.48 , lm loss 0.00 , avg batch size 4.0
2023-05-30 19:28:16,075 - 2:13:58 - 286.9s - INFO - __main__ - progress 5.624 , lr 4.5E-05 , loss 0.428 , qa loss 0.428 , lm loss 0.000 , avg batch size 4.0
2023-05-30 19:31:11,310 - 2:16:53 - 175.2s - INFO - __main__ - epoch 6/20 done , tot steps 9624 , lr 4.4E-05 , loss 0.43 , qa loss 0.43 , lm loss 0.00 , avg batch size 4.0
2023-05-30 19:36:03,163 - 2:21:45 - 291.9s - INFO - __main__ - progress 6.624 , lr 4.2E-05 , loss 0.396 , qa loss 0.396 , lm loss 0.000 , avg batch size 4.0
2023-05-30 19:38:57,840 - 2:24:39 - 174.7s - INFO - __main__ - epoch 7/20 done , tot steps 11228 , lr 4.1E-05 , loss 0.39 , qa loss 0.39 , lm loss 0.00 , avg batch size 4.0
2023-05-30 19:43:52,406 - 2:29:34 - 294.6s - INFO - __main__ - progress 7.624 , lr 3.9E-05 , loss 0.355 , qa loss 0.355 , lm loss 0.000 , avg batch size 4.0
2023-05-30 19:46:45,006 - 2:32:27 - 172.6s - INFO - __main__ - epoch 8/20 done , tot steps 12832 , lr 3.8E-05 , loss 0.36 , qa loss 0.36 , lm loss 0.00 , avg batch size 4.0
2023-05-30 19:51:29,999 - 2:37:12 - 285.0s - INFO - __main__ - progress 8.624 , lr 3.6E-05 , loss 0.328 , qa loss 0.328 , lm loss 0.000 , avg batch size 4.0
2023-05-30 19:54:28,495 - 2:40:10 - 178.5s - INFO - __main__ - epoch 9/20 done , tot steps 14436 , lr 3.4E-05 , loss 0.34 , qa loss 0.34 , lm loss 0.00 , avg batch size 4.0
2023-05-30 19:59:19,863 - 2:45:01 - 291.4s - INFO - __main__ - progress 9.624 , lr 3.2E-05 , loss 0.312 , qa loss 0.312 , lm loss 0.000 , avg batch size 4.0
2023-05-30 20:02:16,238 - 2:47:58 - 176.4s - INFO - __main__ - epoch 10/20 done , tot steps 16040 , lr 3.1E-05 , loss 0.31 , qa loss 0.31 , lm loss 0.00 , avg batch size 4.0
2023-05-30 20:07:02,369 - 2:52:44 - 286.1s - INFO - __main__ - progress 10.624 , lr 2.9E-05 , loss 0.290 , qa loss 0.290 , lm loss 0.000 , avg batch size 4.0
2023-05-30 20:10:01,536 - 2:55:43 - 179.2s - INFO - __main__ - epoch 11/20 done , tot steps 17644 , lr 2.8E-05 , loss 0.29 , qa loss 0.29 , lm loss 0.00 , avg batch size 4.0
2023-05-30 20:14:46,597 - 3:00:28 - 285.1s - INFO - __main__ - progress 11.624 , lr 2.6E-05 , loss 0.274 , qa loss 0.274 , lm loss 0.000 , avg batch size 4.0
2023-05-30 20:17:46,485 - 3:03:28 - 179.9s - INFO - __main__ - epoch 12/20 done , tot steps 19248 , lr 2.5E-05 , loss 0.27 , qa loss 0.27 , lm loss 0.00 , avg batch size 4.0
2023-05-30 20:22:32,377 - 3:08:14 - 285.9s - INFO - __main__ - progress 12.624 , lr 2.3E-05 , loss 0.262 , qa loss 0.262 , lm loss 0.000 , avg batch size 4.0
2023-05-30 20:25:31,940 - 3:11:14 - 179.6s - INFO - __main__ - epoch 13/20 done , tot steps 20852 , lr 2.2E-05 , loss 0.26 , qa loss 0.26 , lm loss 0.00 , avg batch size 4.0
2023-05-30 20:30:16,287 - 3:15:58 - 284.3s - INFO - __main__ - progress 13.624 , lr 2.0E-05 , loss 0.250 , qa loss 0.250 , lm loss 0.000 , avg batch size 4.0
2023-05-30 20:33:15,656 - 3:18:57 - 179.4s - INFO - __main__ - epoch 14/20 done , tot steps 22456 , lr 1.9E-05 , loss 0.25 , qa loss 0.25 , lm loss 0.00 , avg batch size 4.0
2023-05-30 20:38:06,730 - 3:23:48 - 291.1s - INFO - __main__ - progress 14.624 , lr 1.7E-05 , loss 0.221 , qa loss 0.221 , lm loss 0.000 , avg batch size 4.0
2023-05-30 20:40:58,933 - 3:26:41 - 172.2s - INFO - __main__ - epoch 15/20 done , tot steps 24060 , lr 1.6E-05 , loss 0.23 , qa loss 0.23 , lm loss 0.00 , avg batch size 4.0
2023-05-30 20:45:52,533 - 3:31:34 - 293.6s - INFO - __main__ - progress 15.624 , lr 1.4E-05 , loss 0.205 , qa loss 0.205 , lm loss 0.000 , avg batch size 4.0
2023-05-30 20:48:45,580 - 3:34:27 - 173.0s - INFO - __main__ - epoch 16/20 done , tot steps 25664 , lr 1.3E-05 , loss 0.22 , qa loss 0.22 , lm loss 0.00 , avg batch size 4.0
2023-05-30 20:53:33,415 - 3:39:15 - 287.8s - INFO - __main__ - progress 16.624 , lr 1.1E-05 , loss 0.207 , qa loss 0.207 , lm loss 0.000 , avg batch size 4.0
2023-05-30 20:56:29,561 - 3:42:11 - 176.1s - INFO - __main__ - epoch 17/20 done , tot steps 27268 , lr 9.4E-06 , loss 0.21 , qa loss 0.21 , lm loss 0.00 , avg batch size 4.0
2023-05-30 21:01:24,370 - 3:47:06 - 294.8s - INFO - __main__ - progress 17.624 , lr 7.4E-06 , loss 0.206 , qa loss 0.206 , lm loss 0.000 , avg batch size 4.0
2023-05-30 21:04:15,946 - 3:49:58 - 171.6s - INFO - __main__ - epoch 18/20 done , tot steps 28872 , lr 6.3E-06 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-05-30 21:09:06,166 - 3:54:48 - 290.2s - INFO - __main__ - progress 18.624 , lr 4.3E-06 , loss 0.196 , qa loss 0.196 , lm loss 0.000 , avg batch size 4.0
2023-05-30 21:11:58,339 - 3:57:40 - 172.2s - INFO - __main__ - epoch 19/20 done , tot steps 30476 , lr 3.1E-06 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-05-30 21:16:43,324 - 4:02:25 - 285.0s - INFO - __main__ - progress 19.624 , lr 1.2E-06 , loss 0.188 , qa loss 0.188 , lm loss 0.000 , avg batch size 4.0
2023-05-30 21:19:44,100 - 4:05:26 - 180.8s - INFO - __main__ - epoch 20/20 done , tot steps 32080 , lr 1.6E-08 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-05-30 21:19:51,046 - 4:05:33 - 6.9s - INFO - __main__ - start to train { task: ['woz.en'], seq train type: lll }
2023-05-30 21:19:54,816 - 4:05:36 - 3.8s - INFO - utils - writing extra data in ../../model_am1/gpt2/lll/sst_srl_woz.en_0.0/srl/lm.csv ...
2023-05-30 21:19:54,817 - 4:05:36 - 0.0s - INFO - __main__ - extra training data size: 0
2023-05-30 21:19:54,939 - 4:05:37 - 0.1s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[srl]
The task with which model is saved srl
2023-05-30 21:19:59,291 - 4:05:41 - 4.4s - INFO - __main__ - len of train dataset: 2536 , max train batch size 4 , num of opt steps: 50720
2023-05-30 21:21:34,332 - 4:07:16 - 95.0s - INFO - __main__ - epoch 1/20 done , tot steps 634 , lr 5.9E-05 , loss 3.64 , qa loss 3.64 , lm loss 0.00 , avg batch size 4.0
2023-05-30 21:23:05,728 - 4:08:47 - 91.4s - INFO - __main__ - epoch 2/20 done , tot steps 1268 , lr 5.6E-05 , loss 0.43 , qa loss 0.43 , lm loss 0.00 , avg batch size 4.0
2023-05-30 21:24:37,323 - 4:10:19 - 91.6s - INFO - __main__ - epoch 3/20 done , tot steps 1902 , lr 5.3E-05 , loss 0.30 , qa loss 0.30 , lm loss 0.00 , avg batch size 4.0
2023-05-30 21:26:10,135 - 4:11:52 - 92.8s - INFO - __main__ - epoch 4/20 done , tot steps 2536 , lr 5.0E-05 , loss 0.24 , qa loss 0.24 , lm loss 0.00 , avg batch size 4.0
2023-05-30 21:27:42,175 - 4:13:24 - 92.0s - INFO - __main__ - epoch 5/20 done , tot steps 3170 , lr 4.7E-05 , loss 0.21 , qa loss 0.21 , lm loss 0.00 , avg batch size 4.0
2023-05-30 21:29:13,984 - 4:14:56 - 91.8s - INFO - __main__ - epoch 6/20 done , tot steps 3804 , lr 4.4E-05 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-05-30 21:30:45,967 - 4:16:28 - 92.0s - INFO - __main__ - epoch 7/20 done , tot steps 4438 , lr 4.1E-05 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
2023-05-30 21:32:18,440 - 4:18:00 - 92.5s - INFO - __main__ - epoch 8/20 done , tot steps 5072 , lr 3.8E-05 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-05-30 21:33:50,406 - 4:19:32 - 92.0s - INFO - __main__ - epoch 9/20 done , tot steps 5706 , lr 3.4E-05 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
2023-05-30 21:35:22,104 - 4:21:04 - 91.7s - INFO - __main__ - epoch 10/20 done , tot steps 6340 , lr 3.1E-05 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
2023-05-30 21:36:54,334 - 4:22:36 - 92.2s - INFO - __main__ - epoch 11/20 done , tot steps 6974 , lr 2.8E-05 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2023-05-30 21:38:26,522 - 4:24:08 - 92.2s - INFO - __main__ - epoch 12/20 done , tot steps 7608 , lr 2.5E-05 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-05-30 21:39:58,023 - 4:25:40 - 91.5s - INFO - __main__ - epoch 13/20 done , tot steps 8242 , lr 2.2E-05 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-05-30 21:41:30,210 - 4:27:12 - 92.2s - INFO - __main__ - epoch 14/20 done , tot steps 8876 , lr 1.9E-05 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-05-30 21:43:02,298 - 4:28:44 - 92.1s - INFO - __main__ - epoch 15/20 done , tot steps 9510 , lr 1.6E-05 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 4.0
2023-05-30 21:44:34,558 - 4:30:16 - 92.3s - INFO - __main__ - epoch 16/20 done , tot steps 10144 , lr 1.3E-05 , loss 0.09 , qa loss 0.09 , lm loss 0.00 , avg batch size 4.0
2023-05-30 21:46:06,713 - 4:31:48 - 92.2s - INFO - __main__ - epoch 17/20 done , tot steps 10778 , lr 9.4E-06 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 4.0
2023-05-30 21:47:38,028 - 4:33:20 - 91.3s - INFO - __main__ - epoch 18/20 done , tot steps 11412 , lr 6.3E-06 , loss 0.09 , qa loss 0.09 , lm loss 0.00 , avg batch size 4.0
2023-05-30 21:49:09,547 - 4:34:51 - 91.5s - INFO - __main__ - epoch 19/20 done , tot steps 12046 , lr 3.1E-06 , loss 0.08 , qa loss 0.08 , lm loss 0.00 , avg batch size 4.0
2023-05-30 21:50:41,512 - 4:36:23 - 92.0s - INFO - __main__ - epoch 20/20 done , tot steps 12680 , lr 1.5E-08 , loss 0.08 , qa loss 0.08 , lm loss 0.00 , avg batch size 4.0
The current active adapter is Stack[woz_en]
The task with which model is saved woz_en
Wall Execution time: 04:36:05
CPU Execution time: 04:37:19
