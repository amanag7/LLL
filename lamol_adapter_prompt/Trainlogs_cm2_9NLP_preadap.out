Not all gpus support fp16 training! Will use fp32 instead.
2023-06-07 15:11:08,620 - 0:00:05 - 0.0s - INFO - __main__ - args = Namespace(REG_TYPE_KEYS=['mas', 'ewc'], adam_epsilon=0.0001, add_task_tokens=False, data_dir='../../data', debug=False, decay_style='linear', device_ids=[0, 1, 2, 3], dynamic_epochs=False, fp32=True, gen_lm_sample_percentage=0.0, learning_rate=6.25e-05, lm_lambda=0.0, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[35672.56, 42158.48, 42158.48, 42158.48], min_batch_size=4, min_n_steps=1500, model_dir_root='../../model_mam1/gpt2/lll/squad1_cnn_dailymail_sst_srl_zre_woz.en_wikisql_ag_dbpedia_0.0', model_name='gpt2', n_gpus=4, n_train_epochs={'squad1': 20, 'cnn_dailymail': 20, 'sst': 20, 'srl': 20, 'zre': 20, 'woz.en': 20, 'wikisql': 20, 'ag': 20, 'dbpedia': 20}, n_warmup_ratio=0.005, n_workers=75, qp_margin=0.5, real_sample=False, reg_lambda=1.0, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['squad1', 'cnn_dailymail', 'sst', 'srl', 'zre', 'woz.en', 'wikisql', 'ag', 'dbpedia'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[12485, 14755, 14755, 14755], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[12485, 14755, 14755, 14755], unbound=0, use_sep=False, weight_decay=0.01)
2023-06-07 15:11:08,620 - 0:00:05 - 0.0s - INFO - __main__ - start to train { task: ['squad1'], seq train type: lll }
2023-06-07 15:11:08,620 - 0:00:05 - 0.0s - INFO - __main__ - extra training data size: 0
2023-06-07 15:11:20,835 - 0:00:17 - 12.2s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
Token indices sequence length is longer than the specified maximum sequence length for this model (6410 > 1024). Running this sequence through the model will result in indexing errors
2023-06-07 15:11:26,340 - 0:00:22 - 5.5s - WARNING - utils - an example with len 6412 is too long!
2023-06-07 15:11:26,341 - 0:00:22 - 0.0s - WARNING - utils - an example with len 6416 is too long!
multiprocessing.pool.RemoteTraceback: 
"""
Traceback (most recent call last):
  File "/home/student/2021/cs21mtech11006/anaconda3/envs/lamol/lib/python3.8/multiprocessing/pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
  File "/home/student/2021/cs21mtech11006/anaconda3/envs/lamol/lib/python3.8/multiprocessing/pool.py", line 48, in mapstar
    return list(map(*args))
  File "/u/student/2021/cs21mtech11006/LLL/lamol_adapter_prompt/utils.py", line 266, in parallel_tokenization
    examples.append(self.parse_example(self.gen_token, context, question, answer, qa.get("id", 0)))
  File "/u/student/2021/cs21mtech11006/LLL/lamol_adapter_prompt/utils.py", line 238, in parse_example
    Y_example = [FILL_VAL] * (len(cqa_example) - len(Y_example)) + Y_example
TypeError: object of type 'NoneType' has no len()
"""

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "train_experiment_prefix_adapter.py", line 285, in <module>
    model = train([task_id], model)
  File "train_experiment_prefix_adapter.py", line 134, in train
    train_qadata = QADataset(train_dataset, "train", SPECIAL_TOKEN_IDS[tasks[0]], train_extra_data)
  File "/u/student/2021/cs21mtech11006/LLL/lamol_adapter_prompt/utils.py", line 193, in __init__
    self.data_tokenization(data)
  File "/u/student/2021/cs21mtech11006/LLL/lamol_adapter_prompt/utils.py", line 278, in data_tokenization
    data = pool.map(self.parallel_tokenization, data)
  File "/home/student/2021/cs21mtech11006/anaconda3/envs/lamol/lib/python3.8/multiprocessing/pool.py", line 364, in map
    return self._map_async(func, iterable, mapstar, chunksize).get()
  File "/home/student/2021/cs21mtech11006/anaconda3/envs/lamol/lib/python3.8/multiprocessing/pool.py", line 771, in get
    raise self._value
TypeError: object of type 'NoneType' has no len()
