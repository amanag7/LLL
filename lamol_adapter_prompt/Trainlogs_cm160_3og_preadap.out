Not all gpus support fp16 training! Will use fp32 instead.
2023-07-02 12:37:01,302 - 0:00:04 - 0.0s - INFO - __main__ - args = Namespace(REG_TYPE_KEYS=['mas', 'ewc'], adam_epsilon=0.0001, add_task_tokens=False, data_dir='../../data', debug=False, decay_style='linear', device_ids=[1], dynamic_epochs=False, fp32=True, gen_lm_sample_percentage=0.0, learning_rate=6.25e-05, lm_lambda=0.0, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[40537.0], min_batch_size=4, min_n_steps=1500, model_dir_root='../../model_cm160/gpt2/lll/sst_srl_woz.en_0.0', model_name='gpt2', n_gpus=1, n_train_epochs={'sst': 20, 'srl': 20, 'woz.en': 20}, n_warmup_ratio=0.005, n_workers=75, qp_margin=0.5, real_sample=False, reg_lambda=1.0, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[14187], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[14187], unbound=0, use_sep=False, weight_decay=0.01)
2023-07-02 12:37:01,302 - 0:00:04 - 0.0s - INFO - __main__ - start to train { task: ['sst'], seq train type: lll }
2023-07-02 12:37:01,310 - 0:00:04 - 0.0s - INFO - __main__ - extra training data size: 0
2023-07-02 12:37:03,889 - 0:00:07 - 2.6s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
2023-07-02 12:37:06,862 - 0:00:10 - 3.0s - INFO - __main__ - len of train dataset: 6920 , max train batch size 4 , num of opt steps: 138400
/home/student/2021/cs21mtech11006/anaconda3/envs/lamol/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
2023-07-02 12:39:16,430 - 0:02:19 - 129.6s - INFO - __main__ - progress 0.578 , lr 6.1E-05 , loss 2.437 , qa loss 2.437 , lm loss 0.000 , avg batch size 4.0
2023-07-02 12:40:46,229 - 0:03:49 - 89.8s - INFO - __main__ - epoch 1/20 done , tot steps 1730 , lr 5.9E-05 , loss 1.55 , qa loss 1.55 , lm loss 0.00 , avg batch size 4.0
2023-07-02 12:42:53,136 - 0:05:56 - 126.9s - INFO - __main__ - progress 1.578 , lr 5.8E-05 , loss 0.291 , qa loss 0.291 , lm loss 0.000 , avg batch size 4.0
2023-07-02 12:44:22,774 - 0:07:26 - 89.6s - INFO - __main__ - epoch 2/20 done , tot steps 3460 , lr 5.6E-05 , loss 0.28 , qa loss 0.28 , lm loss 0.00 , avg batch size 4.0
2023-07-02 12:46:29,581 - 0:09:33 - 126.8s - INFO - __main__ - progress 2.578 , lr 5.4E-05 , loss 0.240 , qa loss 0.240 , lm loss 0.000 , avg batch size 4.0
2023-07-02 12:47:59,208 - 0:11:02 - 89.6s - INFO - __main__ - epoch 3/20 done , tot steps 5190 , lr 5.3E-05 , loss 0.23 , qa loss 0.23 , lm loss 0.00 , avg batch size 4.0
2023-07-02 12:50:06,746 - 0:13:10 - 127.5s - INFO - __main__ - progress 3.578 , lr 5.1E-05 , loss 0.210 , qa loss 0.210 , lm loss 0.000 , avg batch size 4.0
2023-07-02 12:51:36,472 - 0:14:39 - 89.7s - INFO - __main__ - epoch 4/20 done , tot steps 6920 , lr 5.0E-05 , loss 0.21 , qa loss 0.21 , lm loss 0.00 , avg batch size 4.0
2023-07-02 12:53:42,091 - 0:16:45 - 125.6s - INFO - __main__ - progress 4.578 , lr 4.8E-05 , loss 0.187 , qa loss 0.187 , lm loss 0.000 , avg batch size 4.0
2023-07-02 12:55:13,222 - 0:18:16 - 91.1s - INFO - __main__ - epoch 5/20 done , tot steps 8650 , lr 4.7E-05 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-07-02 12:57:19,028 - 0:20:22 - 125.8s - INFO - __main__ - progress 5.578 , lr 4.5E-05 , loss 0.181 , qa loss 0.181 , lm loss 0.000 , avg batch size 4.0
2023-07-02 12:58:48,534 - 0:21:52 - 89.5s - INFO - __main__ - epoch 6/20 done , tot steps 10380 , lr 4.4E-05 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-07-02 13:00:55,167 - 0:23:58 - 126.6s - INFO - __main__ - progress 6.578 , lr 4.2E-05 , loss 0.160 , qa loss 0.160 , lm loss 0.000 , avg batch size 4.0
2023-07-02 13:02:24,149 - 0:25:27 - 89.0s - INFO - __main__ - epoch 7/20 done , tot steps 12110 , lr 4.1E-05 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-07-02 13:04:30,607 - 0:27:34 - 126.5s - INFO - __main__ - progress 7.578 , lr 3.9E-05 , loss 0.153 , qa loss 0.153 , lm loss 0.000 , avg batch size 4.0
2023-07-02 13:05:58,775 - 0:29:02 - 88.2s - INFO - __main__ - epoch 8/20 done , tot steps 13840 , lr 3.8E-05 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-07-02 13:08:06,223 - 0:31:09 - 127.4s - INFO - __main__ - progress 8.578 , lr 3.6E-05 , loss 0.145 , qa loss 0.145 , lm loss 0.000 , avg batch size 4.0
2023-07-02 13:09:32,013 - 0:32:35 - 85.8s - INFO - __main__ - epoch 9/20 done , tot steps 15570 , lr 3.4E-05 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
2023-07-02 13:11:40,819 - 0:34:44 - 128.8s - INFO - __main__ - progress 9.578 , lr 3.3E-05 , loss 0.136 , qa loss 0.136 , lm loss 0.000 , avg batch size 4.0
2023-07-02 13:13:08,078 - 0:36:11 - 87.3s - INFO - __main__ - epoch 10/20 done , tot steps 17300 , lr 3.1E-05 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
2023-07-02 13:15:16,667 - 0:38:20 - 128.6s - INFO - __main__ - progress 10.578 , lr 2.9E-05 , loss 0.128 , qa loss 0.128 , lm loss 0.000 , avg batch size 4.0
2023-07-02 13:16:44,613 - 0:39:48 - 87.9s - INFO - __main__ - epoch 11/20 done , tot steps 19030 , lr 2.8E-05 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-07-02 13:18:51,533 - 0:41:54 - 126.9s - INFO - __main__ - progress 11.578 , lr 2.6E-05 , loss 0.129 , qa loss 0.129 , lm loss 0.000 , avg batch size 4.0
2023-07-02 13:20:20,159 - 0:43:23 - 88.6s - INFO - __main__ - epoch 12/20 done , tot steps 20760 , lr 2.5E-05 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2023-07-02 13:22:25,342 - 0:45:28 - 125.2s - INFO - __main__ - progress 12.578 , lr 2.3E-05 , loss 0.119 , qa loss 0.119 , lm loss 0.000 , avg batch size 4.0
2023-07-02 13:23:55,149 - 0:46:58 - 89.8s - INFO - __main__ - epoch 13/20 done , tot steps 22490 , lr 2.2E-05 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2023-07-02 13:26:03,835 - 0:49:07 - 128.7s - INFO - __main__ - progress 13.578 , lr 2.0E-05 , loss 0.101 , qa loss 0.101 , lm loss 0.000 , avg batch size 4.0
2023-07-02 13:27:30,366 - 0:50:33 - 86.5s - INFO - __main__ - epoch 14/20 done , tot steps 24220 , lr 1.9E-05 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-07-02 13:29:38,134 - 0:52:41 - 127.8s - INFO - __main__ - progress 14.578 , lr 1.7E-05 , loss 0.107 , qa loss 0.107 , lm loss 0.000 , avg batch size 4.0
2023-07-02 13:31:06,302 - 0:54:09 - 88.2s - INFO - __main__ - epoch 15/20 done , tot steps 25950 , lr 1.6E-05 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-07-02 13:33:14,725 - 0:56:18 - 128.4s - INFO - __main__ - progress 15.578 , lr 1.4E-05 , loss 0.104 , qa loss 0.104 , lm loss 0.000 , avg batch size 4.0
2023-07-02 13:34:42,527 - 0:57:45 - 87.8s - INFO - __main__ - epoch 16/20 done , tot steps 27680 , lr 1.3E-05 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 4.0
2023-07-02 13:36:50,593 - 0:59:54 - 128.1s - INFO - __main__ - progress 16.578 , lr 1.1E-05 , loss 0.094 , qa loss 0.094 , lm loss 0.000 , avg batch size 4.0
2023-07-02 13:38:18,691 - 1:01:22 - 88.1s - INFO - __main__ - epoch 17/20 done , tot steps 29410 , lr 9.4E-06 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 4.0
2023-07-02 13:40:26,690 - 1:03:30 - 128.0s - INFO - __main__ - progress 17.578 , lr 7.6E-06 , loss 0.101 , qa loss 0.101 , lm loss 0.000 , avg batch size 4.0
2023-07-02 13:41:56,265 - 1:04:59 - 89.6s - INFO - __main__ - epoch 18/20 done , tot steps 31140 , lr 6.3E-06 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 4.0
2023-07-02 13:44:04,788 - 1:07:08 - 128.5s - INFO - __main__ - progress 18.578 , lr 4.5E-06 , loss 0.097 , qa loss 0.097 , lm loss 0.000 , avg batch size 4.0
2023-07-02 13:45:16,660 - 1:08:20 - 71.9s - INFO - __main__ - epoch 19/20 done , tot steps 32870 , lr 3.1E-06 , loss 0.09 , qa loss 0.09 , lm loss 0.00 , avg batch size 4.0
2023-07-02 13:47:43,169 - 1:10:46 - 146.5s - INFO - __main__ - progress 19.578 , lr 1.3E-06 , loss 0.090 , qa loss 0.090 , lm loss 0.000 , avg batch size 4.0
2023-07-02 13:49:36,852 - 1:12:40 - 113.7s - INFO - __main__ - epoch 20/20 done , tot steps 34600 , lr 1.5E-08 , loss 0.09 , qa loss 0.09 , lm loss 0.00 , avg batch size 4.0
2023-07-02 13:49:45,727 - 1:12:49 - 8.9s - INFO - __main__ - start to train { task: ['srl'], seq train type: lll }
2023-07-02 13:49:48,400 - 1:12:51 - 2.7s - INFO - utils - writing extra data in ../../model_cm160/gpt2/lll/sst_srl_woz.en_0.0/sst/lm.csv ...
2023-07-02 13:49:48,479 - 1:12:51 - 0.1s - INFO - __main__ - extra training data size: 0
2023-07-02 13:49:49,020 - 1:12:52 - 0.5s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[sst]
The task with which model is saved sst
2023-07-02 13:49:52,804 - 1:12:56 - 3.8s - INFO - __main__ - len of train dataset: 6414 , max train batch size 4 , num of opt steps: 128280
2023-07-02 13:54:09,732 - 1:17:13 - 256.9s - INFO - __main__ - progress 0.624 , lr 6.1E-05 , loss 2.951 , qa loss 2.951 , lm loss 0.000 , avg batch size 4.0
2023-07-02 13:56:56,485 - 1:19:59 - 166.8s - INFO - __main__ - epoch 1/20 done , tot steps 1604 , lr 5.9E-05 , loss 2.29 , qa loss 2.29 , lm loss 0.00 , avg batch size 4.0
2023-07-02 14:01:17,622 - 1:24:21 - 261.1s - INFO - __main__ - progress 1.624 , lr 5.7E-05 , loss 1.003 , qa loss 1.003 , lm loss 0.000 , avg batch size 4.0
2023-07-02 14:04:01,287 - 1:27:04 - 163.7s - INFO - __main__ - epoch 2/20 done , tot steps 3208 , lr 5.6E-05 , loss 0.98 , qa loss 0.98 , lm loss 0.00 , avg batch size 4.0
2023-07-02 14:08:25,815 - 1:31:29 - 264.5s - INFO - __main__ - progress 2.624 , lr 5.4E-05 , loss 0.840 , qa loss 0.840 , lm loss 0.000 , avg batch size 4.0
2023-07-02 14:11:05,075 - 1:34:08 - 159.3s - INFO - __main__ - epoch 3/20 done , tot steps 4812 , lr 5.3E-05 , loss 0.82 , qa loss 0.82 , lm loss 0.00 , avg batch size 4.0
2023-07-02 14:15:28,328 - 1:38:31 - 263.3s - INFO - __main__ - progress 3.624 , lr 5.1E-05 , loss 0.739 , qa loss 0.739 , lm loss 0.000 , avg batch size 4.0
2023-07-02 14:18:09,598 - 1:41:13 - 161.3s - INFO - __main__ - epoch 4/20 done , tot steps 6416 , lr 5.0E-05 , loss 0.74 , qa loss 0.74 , lm loss 0.00 , avg batch size 4.0
2023-07-02 14:22:28,877 - 1:45:32 - 259.3s - INFO - __main__ - progress 4.624 , lr 4.8E-05 , loss 0.685 , qa loss 0.685 , lm loss 0.000 , avg batch size 4.0
2023-07-02 14:25:07,496 - 1:48:10 - 158.6s - INFO - __main__ - epoch 5/20 done , tot steps 8020 , lr 4.7E-05 , loss 0.68 , qa loss 0.68 , lm loss 0.00 , avg batch size 4.0
2023-07-02 14:29:32,374 - 1:52:35 - 264.9s - INFO - __main__ - progress 5.624 , lr 4.5E-05 , loss 0.626 , qa loss 0.626 , lm loss 0.000 , avg batch size 4.0
2023-07-02 14:32:08,668 - 1:55:12 - 156.3s - INFO - __main__ - epoch 6/20 done , tot steps 9624 , lr 4.4E-05 , loss 0.62 , qa loss 0.62 , lm loss 0.00 , avg batch size 4.0
2023-07-02 14:36:29,552 - 1:59:33 - 260.9s - INFO - __main__ - progress 6.624 , lr 4.2E-05 , loss 0.573 , qa loss 0.573 , lm loss 0.000 , avg batch size 4.0
2023-07-02 14:39:11,711 - 2:02:15 - 162.2s - INFO - __main__ - epoch 7/20 done , tot steps 11228 , lr 4.1E-05 , loss 0.58 , qa loss 0.58 , lm loss 0.00 , avg batch size 4.0
2023-07-02 14:43:28,550 - 2:06:32 - 256.8s - INFO - __main__ - progress 7.624 , lr 3.9E-05 , loss 0.566 , qa loss 0.566 , lm loss 0.000 , avg batch size 4.0
2023-07-02 14:46:11,523 - 2:09:14 - 163.0s - INFO - __main__ - epoch 8/20 done , tot steps 12832 , lr 3.8E-05 , loss 0.56 , qa loss 0.56 , lm loss 0.00 , avg batch size 4.0
2023-07-02 14:50:33,343 - 2:13:36 - 261.8s - INFO - __main__ - progress 8.624 , lr 3.6E-05 , loss 0.532 , qa loss 0.532 , lm loss 0.000 , avg batch size 4.0
2023-07-02 14:53:15,503 - 2:16:18 - 162.2s - INFO - __main__ - epoch 9/20 done , tot steps 14436 , lr 3.4E-05 , loss 0.53 , qa loss 0.53 , lm loss 0.00 , avg batch size 4.0
2023-07-02 14:57:34,367 - 2:20:37 - 258.9s - INFO - __main__ - progress 9.624 , lr 3.2E-05 , loss 0.516 , qa loss 0.516 , lm loss 0.000 , avg batch size 4.0
2023-07-02 15:00:20,367 - 2:23:23 - 166.0s - INFO - __main__ - epoch 10/20 done , tot steps 16040 , lr 3.1E-05 , loss 0.51 , qa loss 0.51 , lm loss 0.00 , avg batch size 4.0
2023-07-02 15:04:37,925 - 2:27:41 - 257.6s - INFO - __main__ - progress 10.624 , lr 2.9E-05 , loss 0.494 , qa loss 0.494 , lm loss 0.000 , avg batch size 4.0
2023-07-02 15:07:25,330 - 2:30:28 - 167.4s - INFO - __main__ - epoch 11/20 done , tot steps 17644 , lr 2.8E-05 , loss 0.49 , qa loss 0.49 , lm loss 0.00 , avg batch size 4.0
2023-07-02 15:11:46,479 - 2:34:49 - 261.1s - INFO - __main__ - progress 11.624 , lr 2.6E-05 , loss 0.464 , qa loss 0.464 , lm loss 0.000 , avg batch size 4.0
2023-07-02 15:14:29,011 - 2:37:32 - 162.5s - INFO - __main__ - epoch 12/20 done , tot steps 19248 , lr 2.5E-05 , loss 0.47 , qa loss 0.47 , lm loss 0.00 , avg batch size 4.0
2023-07-02 15:18:43,567 - 2:41:47 - 254.6s - INFO - __main__ - progress 12.624 , lr 2.3E-05 , loss 0.459 , qa loss 0.459 , lm loss 0.000 , avg batch size 4.0
2023-07-02 15:21:29,605 - 2:44:33 - 166.0s - INFO - __main__ - epoch 13/20 done , tot steps 20852 , lr 2.2E-05 , loss 0.45 , qa loss 0.45 , lm loss 0.00 , avg batch size 4.0
2023-07-02 15:25:55,173 - 2:48:58 - 265.6s - INFO - __main__ - progress 13.624 , lr 2.0E-05 , loss 0.437 , qa loss 0.437 , lm loss 0.000 , avg batch size 4.0
2023-07-02 15:28:32,349 - 2:51:35 - 157.2s - INFO - __main__ - epoch 14/20 done , tot steps 22456 , lr 1.9E-05 , loss 0.44 , qa loss 0.44 , lm loss 0.00 , avg batch size 4.0
2023-07-02 15:32:54,399 - 2:55:57 - 262.1s - INFO - __main__ - progress 14.624 , lr 1.7E-05 , loss 0.417 , qa loss 0.417 , lm loss 0.000 , avg batch size 4.0
2023-07-02 15:35:37,562 - 2:58:41 - 163.2s - INFO - __main__ - epoch 15/20 done , tot steps 24060 , lr 1.6E-05 , loss 0.42 , qa loss 0.42 , lm loss 0.00 , avg batch size 4.0
2023-07-02 15:39:54,989 - 3:02:58 - 257.4s - INFO - __main__ - progress 15.624 , lr 1.4E-05 , loss 0.400 , qa loss 0.400 , lm loss 0.000 , avg batch size 4.0
2023-07-02 15:42:38,724 - 3:05:42 - 163.7s - INFO - __main__ - epoch 16/20 done , tot steps 25664 , lr 1.3E-05 , loss 0.40 , qa loss 0.40 , lm loss 0.00 , avg batch size 4.0
2023-07-02 15:46:58,852 - 3:10:02 - 260.1s - INFO - __main__ - progress 16.624 , lr 1.1E-05 , loss 0.405 , qa loss 0.405 , lm loss 0.000 , avg batch size 4.0
2023-07-02 15:49:43,163 - 3:12:46 - 164.3s - INFO - __main__ - epoch 17/20 done , tot steps 27268 , lr 9.4E-06 , loss 0.41 , qa loss 0.41 , lm loss 0.00 , avg batch size 4.0
2023-07-02 15:54:00,490 - 3:17:03 - 257.3s - INFO - __main__ - progress 17.624 , lr 7.4E-06 , loss 0.391 , qa loss 0.391 , lm loss 0.000 , avg batch size 4.0
2023-07-02 15:56:45,079 - 3:19:48 - 164.6s - INFO - __main__ - epoch 18/20 done , tot steps 28872 , lr 6.3E-06 , loss 0.39 , qa loss 0.39 , lm loss 0.00 , avg batch size 4.0
2023-07-02 16:01:04,488 - 3:24:07 - 259.4s - INFO - __main__ - progress 18.624 , lr 4.3E-06 , loss 0.380 , qa loss 0.380 , lm loss 0.000 , avg batch size 4.0
2023-07-02 16:03:50,288 - 3:26:53 - 165.8s - INFO - __main__ - epoch 19/20 done , tot steps 30476 , lr 3.1E-06 , loss 0.39 , qa loss 0.39 , lm loss 0.00 , avg batch size 4.0
2023-07-02 16:07:15,165 - 3:30:18 - 204.9s - INFO - __main__ - progress 19.624 , lr 1.2E-06 , loss 0.386 , qa loss 0.386 , lm loss 0.000 , avg batch size 4.0
2023-07-02 16:09:29,875 - 3:32:33 - 134.7s - INFO - __main__ - epoch 20/20 done , tot steps 32080 , lr 1.6E-08 , loss 0.38 , qa loss 0.38 , lm loss 0.00 , avg batch size 4.0
2023-07-02 16:09:38,350 - 3:32:41 - 8.5s - INFO - __main__ - start to train { task: ['woz.en'], seq train type: lll }
2023-07-02 16:09:42,370 - 3:32:45 - 4.0s - INFO - utils - writing extra data in ../../model_cm160/gpt2/lll/sst_srl_woz.en_0.0/srl/lm.csv ...
2023-07-02 16:09:42,459 - 3:32:45 - 0.1s - INFO - __main__ - extra training data size: 0
2023-07-02 16:09:42,857 - 3:32:46 - 0.4s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[srl]
The task with which model is saved srl
2023-07-02 16:09:46,293 - 3:32:49 - 3.4s - INFO - __main__ - len of train dataset: 2536 , max train batch size 4 , num of opt steps: 50720
2023-07-02 16:10:56,940 - 3:34:00 - 70.6s - INFO - __main__ - epoch 1/20 done , tot steps 634 , lr 5.9E-05 , loss 3.29 , qa loss 3.29 , lm loss 0.00 , avg batch size 4.0
2023-07-02 16:12:06,908 - 3:35:10 - 70.0s - INFO - __main__ - epoch 2/20 done , tot steps 1268 , lr 5.6E-05 , loss 0.63 , qa loss 0.63 , lm loss 0.00 , avg batch size 4.0
2023-07-02 16:13:17,010 - 3:36:20 - 70.1s - INFO - __main__ - epoch 3/20 done , tot steps 1902 , lr 5.3E-05 , loss 0.47 , qa loss 0.47 , lm loss 0.00 , avg batch size 4.0
2023-07-02 16:14:27,342 - 3:37:30 - 70.3s - INFO - __main__ - epoch 4/20 done , tot steps 2536 , lr 5.0E-05 , loss 0.38 , qa loss 0.38 , lm loss 0.00 , avg batch size 4.0
2023-07-02 16:15:37,567 - 3:38:41 - 70.2s - INFO - __main__ - epoch 5/20 done , tot steps 3170 , lr 4.7E-05 , loss 0.32 , qa loss 0.32 , lm loss 0.00 , avg batch size 4.0
2023-07-02 16:16:47,206 - 3:39:50 - 69.6s - INFO - __main__ - epoch 6/20 done , tot steps 3804 , lr 4.4E-05 , loss 0.29 , qa loss 0.29 , lm loss 0.00 , avg batch size 4.0
2023-07-02 16:17:57,460 - 3:41:00 - 70.3s - INFO - __main__ - epoch 7/20 done , tot steps 4438 , lr 4.1E-05 , loss 0.26 , qa loss 0.26 , lm loss 0.00 , avg batch size 4.0
2023-07-02 16:19:07,932 - 3:42:11 - 70.5s - INFO - __main__ - epoch 8/20 done , tot steps 5072 , lr 3.8E-05 , loss 0.22 , qa loss 0.22 , lm loss 0.00 , avg batch size 4.0
2023-07-02 16:20:18,835 - 3:43:22 - 70.9s - INFO - __main__ - epoch 9/20 done , tot steps 5706 , lr 3.4E-05 , loss 0.22 , qa loss 0.22 , lm loss 0.00 , avg batch size 4.0
2023-07-02 16:21:30,211 - 3:44:33 - 71.4s - INFO - __main__ - epoch 10/20 done , tot steps 6340 , lr 3.1E-05 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-07-02 16:22:42,049 - 3:45:45 - 71.8s - INFO - __main__ - epoch 11/20 done , tot steps 6974 , lr 2.8E-05 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-07-02 16:23:53,817 - 3:46:57 - 71.8s - INFO - __main__ - epoch 12/20 done , tot steps 7608 , lr 2.5E-05 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-07-02 16:25:05,275 - 3:48:08 - 71.5s - INFO - __main__ - epoch 13/20 done , tot steps 8242 , lr 2.2E-05 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-07-02 16:26:15,700 - 3:49:19 - 70.4s - INFO - __main__ - epoch 14/20 done , tot steps 8876 , lr 1.9E-05 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
2023-07-02 16:27:25,589 - 3:50:29 - 69.9s - INFO - __main__ - epoch 15/20 done , tot steps 9510 , lr 1.6E-05 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-07-02 16:28:35,920 - 3:51:39 - 70.3s - INFO - __main__ - epoch 16/20 done , tot steps 10144 , lr 1.3E-05 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-07-02 16:29:25,329 - 3:52:28 - 49.4s - INFO - __main__ - epoch 17/20 done , tot steps 10778 , lr 9.4E-06 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-07-02 16:30:13,377 - 3:53:16 - 48.0s - INFO - __main__ - epoch 18/20 done , tot steps 11412 , lr 6.3E-06 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
2023-07-02 16:31:01,163 - 3:54:04 - 47.8s - INFO - __main__ - epoch 19/20 done , tot steps 12046 , lr 3.1E-06 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
2023-07-02 16:31:55,272 - 3:54:58 - 54.1s - INFO - __main__ - epoch 20/20 done , tot steps 12680 , lr 1.5E-08 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
The current active adapter is Stack[woz_en]
The task with which model is saved woz_en
Wall Execution time: 03:55:06
CPU Execution time: 03:55:02
