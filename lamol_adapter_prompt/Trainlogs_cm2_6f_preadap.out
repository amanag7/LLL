Not all gpus support fp16 training! Will use fp32 instead.
2023-06-14 02:28:35,831 - 0:11:04 - 0.0s - INFO - __main__ - args = Namespace(REG_TYPE_KEYS=['mas', 'ewc'], adam_epsilon=0.0001, add_task_tokens=False, data_dir='../../data', debug=False, decay_style='linear', device_ids=[0, 1], dynamic_epochs=False, fp32=True, gen_lm_sample_percentage=0.0, learning_rate=6.25e-05, lm_lambda=0.0, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[38915.52, 42158.48], min_batch_size=4, min_n_steps=1500, model_dir_root='../../model_cm2/gpt2/lll/wikisql_ag_amazon_sst_srl_woz.en_0.0', model_name='gpt2', n_gpus=2, n_train_epochs={'wikisql': 20, 'ag': 20, 'amazon': 20, 'sst': 20, 'srl': 20, 'woz.en': 20}, n_warmup_ratio=0.005, n_workers=75, qp_margin=0.5, real_sample=False, reg_lambda=1.0, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['wikisql', 'ag', 'amazon', 'sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[13620, 14755], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[13620, 14755], unbound=0, use_sep=False, weight_decay=0.01)
2023-06-14 02:28:35,832 - 0:11:04 - 0.0s - INFO - __main__ - start to train { task: ['wikisql'], seq train type: lll }
2023-06-14 02:28:35,865 - 0:11:04 - 0.0s - INFO - __main__ - extra training data size: 0
2023-06-14 02:28:47,361 - 0:11:15 - 11.5s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
2023-06-14 02:32:00,893 - 0:14:29 - 193.5s - INFO - __main__ - len of train dataset: 56355 , max train batch size 37 , num of opt steps: 1127100
/home/student/2021/cs21mtech11006/anaconda3/envs/lamol/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/home/student/2021/cs21mtech11006/anaconda3/envs/lamol/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
2023-06-14 02:45:35,192 - 0:28:03 - 814.3s - INFO - __main__ - progress 0.656 , lr 6.0E-05 , loss 1.918 , qa loss 1.918 , lm loss 0.000 , avg batch size 37.0
2023-06-14 02:52:42,774 - 0:35:11 - 427.6s - INFO - __main__ - epoch 1/20 done , tot steps 1524 , lr 5.9E-05 , loss 1.37 , qa loss 1.37 , lm loss 0.00 , avg batch size 37.0
2023-06-14 03:06:00,791 - 0:48:29 - 798.0s - INFO - __main__ - progress 1.656 , lr 5.7E-05 , loss 0.264 , qa loss 0.264 , lm loss 0.000 , avg batch size 37.0
2023-06-14 03:12:59,693 - 0:55:28 - 418.9s - INFO - __main__ - epoch 2/20 done , tot steps 3048 , lr 5.6E-05 , loss 0.25 , qa loss 0.25 , lm loss 0.00 , avg batch size 37.0
2023-06-14 03:26:17,319 - 1:08:45 - 797.6s - INFO - __main__ - progress 2.657 , lr 5.4E-05 , loss 0.196 , qa loss 0.196 , lm loss 0.000 , avg batch size 37.0
2023-06-14 03:33:14,607 - 1:15:43 - 417.3s - INFO - __main__ - epoch 3/20 done , tot steps 4572 , lr 5.3E-05 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 37.0
2023-06-14 03:46:33,226 - 1:29:01 - 798.6s - INFO - __main__ - progress 3.657 , lr 5.1E-05 , loss 0.171 , qa loss 0.171 , lm loss 0.000 , avg batch size 37.0
2023-06-14 03:53:32,751 - 1:36:01 - 419.5s - INFO - __main__ - epoch 4/20 done , tot steps 6096 , lr 5.0E-05 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 37.0
2023-06-14 04:06:47,804 - 1:49:16 - 795.1s - INFO - __main__ - progress 4.657 , lr 4.8E-05 , loss 0.150 , qa loss 0.150 , lm loss 0.000 , avg batch size 37.0
2023-06-14 04:13:47,692 - 1:56:16 - 419.9s - INFO - __main__ - epoch 5/20 done , tot steps 7620 , lr 4.7E-05 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 37.0
2023-06-14 04:27:06,506 - 2:09:35 - 798.8s - INFO - __main__ - progress 5.656 , lr 4.5E-05 , loss 0.136 , qa loss 0.136 , lm loss 0.000 , avg batch size 37.0
2023-06-14 04:34:06,537 - 2:16:35 - 420.0s - INFO - __main__ - epoch 6/20 done , tot steps 9144 , lr 4.4E-05 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 37.0
2023-06-14 04:47:25,934 - 2:29:54 - 799.4s - INFO - __main__ - progress 6.656 , lr 4.2E-05 , loss 0.129 , qa loss 0.129 , lm loss 0.000 , avg batch size 37.0
2023-06-14 04:54:25,793 - 2:36:54 - 419.9s - INFO - __main__ - epoch 7/20 done , tot steps 10668 , lr 4.1E-05 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 37.0
2023-06-14 05:07:43,699 - 2:50:12 - 797.9s - INFO - __main__ - progress 7.656 , lr 3.9E-05 , loss 0.122 , qa loss 0.122 , lm loss 0.000 , avg batch size 37.0
2023-06-14 05:14:42,473 - 2:57:10 - 418.8s - INFO - __main__ - epoch 8/20 done , tot steps 12192 , lr 3.8E-05 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 37.0
2023-06-14 05:28:01,257 - 3:10:29 - 798.8s - INFO - __main__ - progress 8.656 , lr 3.5E-05 , loss 0.115 , qa loss 0.115 , lm loss 0.000 , avg batch size 37.0
2023-06-14 05:35:01,297 - 3:17:29 - 420.0s - INFO - __main__ - epoch 9/20 done , tot steps 13716 , lr 3.4E-05 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 37.0
2023-06-14 05:48:18,822 - 3:30:47 - 797.5s - INFO - __main__ - progress 9.656 , lr 3.2E-05 , loss 0.112 , qa loss 0.112 , lm loss 0.000 , avg batch size 37.0
2023-06-14 05:55:19,419 - 3:37:47 - 420.6s - INFO - __main__ - epoch 10/20 done , tot steps 15240 , lr 3.1E-05 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 37.0
2023-06-14 06:08:36,993 - 3:51:05 - 797.6s - INFO - __main__ - progress 10.656 , lr 2.9E-05 , loss 0.105 , qa loss 0.105 , lm loss 0.000 , avg batch size 37.0
2023-06-14 06:15:36,064 - 3:58:04 - 419.1s - INFO - __main__ - epoch 11/20 done , tot steps 16764 , lr 2.8E-05 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 37.0
2023-06-14 06:28:54,311 - 4:11:22 - 798.2s - INFO - __main__ - progress 11.656 , lr 2.6E-05 , loss 0.103 , qa loss 0.103 , lm loss 0.000 , avg batch size 37.0
2023-06-14 06:35:52,259 - 4:18:20 - 417.9s - INFO - __main__ - epoch 12/20 done , tot steps 18288 , lr 2.5E-05 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 37.0
2023-06-14 06:49:08,132 - 4:31:36 - 795.9s - INFO - __main__ - progress 12.657 , lr 2.3E-05 , loss 0.099 , qa loss 0.099 , lm loss 0.000 , avg batch size 37.0
2023-06-14 06:56:07,479 - 4:38:35 - 419.3s - INFO - __main__ - epoch 13/20 done , tot steps 19812 , lr 2.2E-05 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 37.0
2023-06-14 07:09:25,675 - 4:51:54 - 798.2s - INFO - __main__ - progress 13.656 , lr 2.0E-05 , loss 0.095 , qa loss 0.095 , lm loss 0.000 , avg batch size 37.0
2023-06-14 07:16:24,106 - 4:58:52 - 418.4s - INFO - __main__ - epoch 14/20 done , tot steps 21336 , lr 1.9E-05 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 37.0
2023-06-14 07:29:41,785 - 5:12:10 - 797.7s - INFO - __main__ - progress 14.657 , lr 1.7E-05 , loss 0.094 , qa loss 0.094 , lm loss 0.000 , avg batch size 37.0
2023-06-14 07:36:41,550 - 5:19:10 - 419.8s - INFO - __main__ - epoch 15/20 done , tot steps 22860 , lr 1.6E-05 , loss 0.09 , qa loss 0.09 , lm loss 0.00 , avg batch size 37.0
2023-06-14 07:49:58,872 - 5:32:27 - 797.3s - INFO - __main__ - progress 15.656 , lr 1.4E-05 , loss 0.090 , qa loss 0.090 , lm loss 0.000 , avg batch size 37.0
2023-06-14 07:56:56,897 - 5:39:25 - 418.0s - INFO - __main__ - epoch 16/20 done , tot steps 24384 , lr 1.3E-05 , loss 0.09 , qa loss 0.09 , lm loss 0.00 , avg batch size 37.0
2023-06-14 08:10:16,172 - 5:52:44 - 799.3s - INFO - __main__ - progress 16.656 , lr 1.0E-05 , loss 0.087 , qa loss 0.087 , lm loss 0.000 , avg batch size 37.0
2023-06-14 08:17:15,488 - 5:59:43 - 419.3s - INFO - __main__ - epoch 17/20 done , tot steps 25908 , lr 9.4E-06 , loss 0.09 , qa loss 0.09 , lm loss 0.00 , avg batch size 37.0
2023-06-14 08:30:33,107 - 6:13:01 - 797.6s - INFO - __main__ - progress 17.656 , lr 7.3E-06 , loss 0.085 , qa loss 0.085 , lm loss 0.000 , avg batch size 37.0
2023-06-14 08:37:32,082 - 6:20:00 - 419.0s - INFO - __main__ - epoch 18/20 done , tot steps 27432 , lr 6.3E-06 , loss 0.09 , qa loss 0.09 , lm loss 0.00 , avg batch size 37.0
2023-06-14 08:50:50,339 - 6:33:18 - 798.3s - INFO - __main__ - progress 18.656 , lr 4.2E-06 , loss 0.084 , qa loss 0.084 , lm loss 0.000 , avg batch size 37.0
2023-06-14 08:57:49,929 - 6:40:18 - 419.6s - INFO - __main__ - epoch 19/20 done , tot steps 28956 , lr 3.1E-06 , loss 0.08 , qa loss 0.08 , lm loss 0.00 , avg batch size 37.0
2023-06-14 09:11:09,485 - 6:53:37 - 799.6s - INFO - __main__ - progress 19.656 , lr 1.1E-06 , loss 0.084 , qa loss 0.084 , lm loss 0.000 , avg batch size 37.0
2023-06-14 09:18:06,951 - 7:00:35 - 417.5s - INFO - __main__ - epoch 20/20 done , tot steps 30480 , lr 1.6E-08 , loss 0.08 , qa loss 0.08 , lm loss 0.00 , avg batch size 37.0
2023-06-14 09:18:19,325 - 7:00:47 - 12.4s - INFO - __main__ - start to train { task: ['ag'], seq train type: lll }
2023-06-14 09:18:35,681 - 7:01:04 - 16.4s - INFO - utils - writing extra data in ../../model_cm2/gpt2/lll/wikisql_ag_amazon_sst_srl_woz.en_0.0/wikisql/lm.csv ...
2023-06-14 09:18:36,036 - 7:01:04 - 0.4s - INFO - __main__ - extra training data size: 0
2023-06-14 09:18:37,982 - 7:01:06 - 1.9s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[wikisql]
The task with which model is saved wikisql
2023-06-14 09:18:48,993 - 7:01:17 - 11.0s - INFO - __main__ - len of train dataset: 115000 , max train batch size 76 , num of opt steps: 2300000
2023-06-14 09:32:57,499 - 7:15:26 - 848.5s - INFO - __main__ - progress 0.649 , lr 6.0E-05 , loss 0.960 , qa loss 0.960 , lm loss 0.000 , avg batch size 74.6
2023-06-14 09:40:47,011 - 7:23:15 - 469.5s - INFO - __main__ - epoch 1/20 done , tot steps 1544 , lr 5.9E-05 , loss 0.67 , qa loss 0.67 , lm loss 0.00 , avg batch size 74.5
2023-06-14 09:54:54,348 - 7:37:22 - 847.3s - INFO - __main__ - progress 1.650 , lr 5.7E-05 , loss 0.106 , qa loss 0.106 , lm loss 0.000 , avg batch size 74.7
2023-06-14 10:02:34,326 - 7:45:02 - 460.0s - INFO - __main__ - epoch 2/20 done , tot steps 3084 , lr 5.6E-05 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 74.7
2023-06-14 10:16:40,913 - 7:59:09 - 846.6s - INFO - __main__ - progress 2.650 , lr 5.4E-05 , loss 0.091 , qa loss 0.091 , lm loss 0.000 , avg batch size 74.7
2023-06-14 10:24:20,255 - 8:06:48 - 459.3s - INFO - __main__ - epoch 3/20 done , tot steps 4626 , lr 5.3E-05 , loss 0.09 , qa loss 0.09 , lm loss 0.00 , avg batch size 74.6
2023-06-14 10:38:27,269 - 8:20:55 - 847.0s - INFO - __main__ - progress 3.648 , lr 5.1E-05 , loss 0.082 , qa loss 0.082 , lm loss 0.000 , avg batch size 74.5
2023-06-14 10:46:06,871 - 8:28:35 - 459.6s - INFO - __main__ - epoch 4/20 done , tot steps 6168 , lr 5.0E-05 , loss 0.08 , qa loss 0.08 , lm loss 0.00 , avg batch size 74.6
2023-06-14 11:00:16,312 - 8:42:44 - 849.4s - INFO - __main__ - progress 4.648 , lr 4.8E-05 , loss 0.075 , qa loss 0.075 , lm loss 0.000 , avg batch size 74.5
2023-06-14 11:07:55,548 - 8:50:24 - 459.2s - INFO - __main__ - epoch 5/20 done , tot steps 7710 , lr 4.7E-05 , loss 0.07 , qa loss 0.07 , lm loss 0.00 , avg batch size 74.6
2023-06-14 11:22:04,138 - 9:04:32 - 848.6s - INFO - __main__ - progress 5.649 , lr 4.5E-05 , loss 0.073 , qa loss 0.073 , lm loss 0.000 , avg batch size 74.7
2023-06-14 11:29:44,476 - 9:12:12 - 460.3s - INFO - __main__ - epoch 6/20 done , tot steps 9250 , lr 4.4E-05 , loss 0.07 , qa loss 0.07 , lm loss 0.00 , avg batch size 74.7
2023-06-14 11:43:54,414 - 9:26:22 - 849.9s - INFO - __main__ - progress 6.651 , lr 4.2E-05 , loss 0.068 , qa loss 0.068 , lm loss 0.000 , avg batch size 74.8
2023-06-14 11:51:32,841 - 9:34:01 - 458.4s - INFO - __main__ - epoch 7/20 done , tot steps 10790 , lr 4.1E-05 , loss 0.07 , qa loss 0.07 , lm loss 0.00 , avg batch size 74.7
2023-06-14 12:05:42,380 - 9:48:10 - 849.5s - INFO - __main__ - progress 7.650 , lr 3.9E-05 , loss 0.065 , qa loss 0.065 , lm loss 0.000 , avg batch size 74.7
2023-06-14 12:13:20,290 - 9:55:48 - 457.9s - INFO - __main__ - epoch 8/20 done , tot steps 12330 , lr 3.8E-05 , loss 0.07 , qa loss 0.07 , lm loss 0.00 , avg batch size 74.7
2023-06-14 12:27:28,090 - 10:09:56 - 847.8s - INFO - __main__ - progress 8.649 , lr 3.5E-05 , loss 0.064 , qa loss 0.064 , lm loss 0.000 , avg batch size 74.6
2023-06-14 12:35:08,443 - 10:17:36 - 460.4s - INFO - __main__ - epoch 9/20 done , tot steps 13872 , lr 3.4E-05 , loss 0.06 , qa loss 0.06 , lm loss 0.00 , avg batch size 74.6
2023-06-14 12:49:17,956 - 10:31:46 - 849.5s - INFO - __main__ - progress 9.650 , lr 3.2E-05 , loss 0.060 , qa loss 0.060 , lm loss 0.000 , avg batch size 74.8
2023-06-14 12:56:58,261 - 10:39:26 - 460.3s - INFO - __main__ - epoch 10/20 done , tot steps 15414 , lr 3.1E-05 , loss 0.06 , qa loss 0.06 , lm loss 0.00 , avg batch size 74.6
2023-06-14 13:11:04,961 - 10:53:33 - 846.7s - INFO - __main__ - progress 10.649 , lr 2.9E-05 , loss 0.060 , qa loss 0.060 , lm loss 0.000 , avg batch size 74.7
2023-06-14 13:18:45,351 - 11:01:13 - 460.4s - INFO - __main__ - epoch 11/20 done , tot steps 16957 , lr 2.8E-05 , loss 0.06 , qa loss 0.06 , lm loss 0.00 , avg batch size 74.5
2023-06-14 13:32:54,759 - 11:15:23 - 849.4s - INFO - __main__ - progress 11.648 , lr 2.6E-05 , loss 0.059 , qa loss 0.059 , lm loss 0.000 , avg batch size 74.5
2023-06-14 13:40:36,206 - 11:23:04 - 461.4s - INFO - __main__ - epoch 12/20 done , tot steps 18501 , lr 2.5E-05 , loss 0.06 , qa loss 0.06 , lm loss 0.00 , avg batch size 74.5
2023-06-14 13:54:43,398 - 11:37:11 - 847.2s - INFO - __main__ - progress 12.650 , lr 2.3E-05 , loss 0.056 , qa loss 0.056 , lm loss 0.000 , avg batch size 74.8
2023-06-14 14:02:23,926 - 11:44:52 - 460.5s - INFO - __main__ - epoch 13/20 done , tot steps 20042 , lr 2.2E-05 , loss 0.06 , qa loss 0.06 , lm loss 0.00 , avg batch size 74.6
2023-06-14 14:16:32,804 - 11:59:01 - 848.9s - INFO - __main__ - progress 13.650 , lr 2.0E-05 , loss 0.055 , qa loss 0.055 , lm loss 0.000 , avg batch size 74.7
2023-06-14 14:24:11,915 - 12:06:40 - 459.1s - INFO - __main__ - epoch 14/20 done , tot steps 21583 , lr 1.9E-05 , loss 0.05 , qa loss 0.05 , lm loss 0.00 , avg batch size 74.6
2023-06-14 14:38:20,622 - 12:20:49 - 848.7s - INFO - __main__ - progress 14.649 , lr 1.7E-05 , loss 0.054 , qa loss 0.054 , lm loss 0.000 , avg batch size 74.6
2023-06-14 14:46:01,163 - 12:28:29 - 460.5s - INFO - __main__ - epoch 15/20 done , tot steps 23125 , lr 1.6E-05 , loss 0.05 , qa loss 0.05 , lm loss 0.00 , avg batch size 74.6
2023-06-14 15:00:09,778 - 12:42:38 - 848.6s - INFO - __main__ - progress 15.648 , lr 1.4E-05 , loss 0.051 , qa loss 0.051 , lm loss 0.000 , avg batch size 74.5
2023-06-14 15:07:49,579 - 12:50:18 - 459.8s - INFO - __main__ - epoch 16/20 done , tot steps 24667 , lr 1.3E-05 , loss 0.05 , qa loss 0.05 , lm loss 0.00 , avg batch size 74.6
2023-06-14 15:21:56,094 - 13:04:24 - 846.5s - INFO - __main__ - progress 16.647 , lr 1.0E-05 , loss 0.051 , qa loss 0.051 , lm loss 0.000 , avg batch size 74.4
2023-06-14 15:29:37,504 - 13:12:06 - 461.4s - INFO - __main__ - epoch 17/20 done , tot steps 26209 , lr 9.4E-06 , loss 0.05 , qa loss 0.05 , lm loss 0.00 , avg batch size 74.6
2023-06-14 15:43:45,274 - 13:26:13 - 847.8s - INFO - __main__ - progress 17.649 , lr 7.4E-06 , loss 0.050 , qa loss 0.050 , lm loss 0.000 , avg batch size 74.6
2023-06-14 15:51:25,543 - 13:33:54 - 460.3s - INFO - __main__ - epoch 18/20 done , tot steps 27750 , lr 6.3E-06 , loss 0.05 , qa loss 0.05 , lm loss 0.00 , avg batch size 74.6
2023-06-14 16:05:34,521 - 13:48:03 - 849.0s - INFO - __main__ - progress 18.647 , lr 4.2E-06 , loss 0.050 , qa loss 0.050 , lm loss 0.000 , avg batch size 74.4
2023-06-14 16:13:16,643 - 13:55:45 - 462.1s - INFO - __main__ - epoch 19/20 done , tot steps 29292 , lr 3.1E-06 , loss 0.05 , qa loss 0.05 , lm loss 0.00 , avg batch size 74.6
2023-06-14 16:27:27,472 - 14:09:55 - 850.8s - INFO - __main__ - progress 19.649 , lr 1.1E-06 , loss 0.048 , qa loss 0.048 , lm loss 0.000 , avg batch size 74.6
2023-06-14 16:35:08,499 - 14:17:37 - 461.0s - INFO - __main__ - epoch 20/20 done , tot steps 30836 , lr 1.6E-08 , loss 0.05 , qa loss 0.05 , lm loss 0.00 , avg batch size 74.5
2023-06-14 16:35:22,222 - 14:17:50 - 13.7s - INFO - __main__ - start to train { task: ['amazon'], seq train type: lll }
2023-06-14 16:35:31,617 - 14:18:00 - 9.4s - INFO - utils - writing extra data in ../../model_cm2/gpt2/lll/wikisql_ag_amazon_sst_srl_woz.en_0.0/ag/lm.csv ...
2023-06-14 16:35:31,897 - 14:18:00 - 0.3s - INFO - __main__ - extra training data size: 0
2023-06-14 16:35:33,760 - 14:18:02 - 1.9s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[ag]
The task with which model is saved ag
2023-06-14 16:35:47,408 - 14:18:15 - 13.6s - INFO - __main__ - len of train dataset: 115000 , max train batch size 76 , num of opt steps: 2300000
2023-06-14 16:50:17,094 - 14:32:45 - 869.7s - INFO - __main__ - progress 0.429 , lr 6.1E-05 , loss 1.997 , qa loss 1.997 , lm loss 0.000 , avg batch size 49.4
2023-06-14 17:04:38,834 - 14:47:07 - 861.7s - INFO - __main__ - progress 0.858 , lr 6.0E-05 , loss 1.241 , qa loss 1.241 , lm loss 0.000 , avg batch size 49.3
2023-06-14 17:09:38,061 - 14:52:06 - 299.2s - INFO - __main__ - epoch 1/20 done , tot steps 2333 , lr 5.9E-05 , loss 1.13 , qa loss 1.13 , lm loss 0.00 , avg batch size 49.3
2023-06-14 17:24:07,274 - 15:06:35 - 869.2s - INFO - __main__ - progress 1.428 , lr 5.8E-05 , loss 0.442 , qa loss 0.442 , lm loss 0.000 , avg batch size 49.2
2023-06-14 17:38:28,485 - 15:20:56 - 861.2s - INFO - __main__ - progress 1.856 , lr 5.7E-05 , loss 0.433 , qa loss 0.433 , lm loss 0.000 , avg batch size 49.2
2023-06-14 17:43:21,903 - 15:25:50 - 293.4s - INFO - __main__ - epoch 2/20 done , tot steps 4668 , lr 5.6E-05 , loss 0.43 , qa loss 0.43 , lm loss 0.00 , avg batch size 49.3
2023-06-14 17:57:51,820 - 15:40:20 - 869.9s - INFO - __main__ - progress 2.430 , lr 5.5E-05 , loss 0.412 , qa loss 0.412 , lm loss 0.000 , avg batch size 49.4
2023-06-14 18:12:13,507 - 15:54:42 - 861.7s - INFO - __main__ - progress 2.857 , lr 5.4E-05 , loss 0.409 , qa loss 0.409 , lm loss 0.000 , avg batch size 49.3
2023-06-14 18:17:05,583 - 15:59:34 - 292.1s - INFO - __main__ - epoch 3/20 done , tot steps 7002 , lr 5.3E-05 , loss 0.41 , qa loss 0.41 , lm loss 0.00 , avg batch size 49.3
2023-06-14 18:31:35,445 - 16:14:03 - 869.9s - INFO - __main__ - progress 3.429 , lr 5.2E-05 , loss 0.396 , qa loss 0.396 , lm loss 0.000 , avg batch size 49.3
2023-06-14 18:45:57,600 - 16:28:26 - 862.2s - INFO - __main__ - progress 3.857 , lr 5.0E-05 , loss 0.395 , qa loss 0.395 , lm loss 0.000 , avg batch size 49.3
2023-06-14 18:50:49,111 - 16:33:17 - 291.5s - INFO - __main__ - epoch 4/20 done , tot steps 9336 , lr 5.0E-05 , loss 0.39 , qa loss 0.39 , lm loss 0.00 , avg batch size 49.3
2023-06-14 19:05:18,644 - 16:47:47 - 869.5s - INFO - __main__ - progress 4.430 , lr 4.9E-05 , loss 0.385 , qa loss 0.385 , lm loss 0.000 , avg batch size 49.4
2023-06-14 19:19:40,804 - 17:02:09 - 862.2s - INFO - __main__ - progress 4.858 , lr 4.7E-05 , loss 0.385 , qa loss 0.385 , lm loss 0.000 , avg batch size 49.3
2023-06-14 19:24:29,548 - 17:06:58 - 288.7s - INFO - __main__ - epoch 5/20 done , tot steps 11665 , lr 4.7E-05 , loss 0.39 , qa loss 0.39 , lm loss 0.00 , avg batch size 49.4
2023-06-14 19:38:59,874 - 17:21:28 - 870.3s - INFO - __main__ - progress 5.428 , lr 4.6E-05 , loss 0.378 , qa loss 0.378 , lm loss 0.000 , avg batch size 49.3
2023-06-14 19:53:21,866 - 17:35:50 - 862.0s - INFO - __main__ - progress 5.856 , lr 4.4E-05 , loss 0.378 , qa loss 0.378 , lm loss 0.000 , avg batch size 49.2
2023-06-14 19:58:13,671 - 17:40:42 - 291.8s - INFO - __main__ - epoch 6/20 done , tot steps 13999 , lr 4.4E-05 , loss 0.38 , qa loss 0.38 , lm loss 0.00 , avg batch size 49.3
2023-06-14 20:12:42,052 - 17:55:10 - 868.4s - INFO - __main__ - progress 6.430 , lr 4.2E-05 , loss 0.372 , qa loss 0.372 , lm loss 0.000 , avg batch size 49.4
2023-06-14 20:27:02,676 - 18:09:31 - 860.6s - INFO - __main__ - progress 6.858 , lr 4.1E-05 , loss 0.372 , qa loss 0.372 , lm loss 0.000 , avg batch size 49.3
2023-06-14 20:31:54,341 - 18:14:22 - 291.7s - INFO - __main__ - epoch 7/20 done , tot steps 16333 , lr 4.1E-05 , loss 0.37 , qa loss 0.37 , lm loss 0.00 , avg batch size 49.3
2023-06-14 20:46:24,547 - 18:28:53 - 870.2s - INFO - __main__ - progress 7.429 , lr 3.9E-05 , loss 0.367 , qa loss 0.367 , lm loss 0.000 , avg batch size 49.3
2023-06-14 21:00:45,389 - 18:43:13 - 860.8s - INFO - __main__ - progress 7.859 , lr 3.8E-05 , loss 0.367 , qa loss 0.367 , lm loss 0.000 , avg batch size 49.4
2023-06-14 21:05:35,021 - 18:48:03 - 289.6s - INFO - __main__ - epoch 8/20 done , tot steps 18663 , lr 3.8E-05 , loss 0.37 , qa loss 0.37 , lm loss 0.00 , avg batch size 49.4
2023-06-14 21:20:04,062 - 19:02:32 - 869.0s - INFO - __main__ - progress 8.427 , lr 3.6E-05 , loss 0.359 , qa loss 0.359 , lm loss 0.000 , avg batch size 49.1
2023-06-14 21:34:24,122 - 19:16:52 - 860.1s - INFO - __main__ - progress 8.857 , lr 3.5E-05 , loss 0.360 , qa loss 0.360 , lm loss 0.000 , avg batch size 49.3
2023-06-14 21:39:16,981 - 19:21:45 - 292.9s - INFO - __main__ - epoch 9/20 done , tot steps 20999 , lr 3.4E-05 , loss 0.36 , qa loss 0.36 , lm loss 0.00 , avg batch size 49.2
2023-06-14 21:53:46,968 - 19:36:15 - 870.0s - INFO - __main__ - progress 9.429 , lr 3.3E-05 , loss 0.356 , qa loss 0.356 , lm loss 0.000 , avg batch size 49.3
2023-06-14 22:08:06,279 - 19:50:34 - 859.3s - INFO - __main__ - progress 9.857 , lr 3.2E-05 , loss 0.357 , qa loss 0.357 , lm loss 0.000 , avg batch size 49.3
2023-06-14 22:12:56,409 - 19:55:24 - 290.1s - INFO - __main__ - epoch 10/20 done , tot steps 23332 , lr 3.1E-05 , loss 0.36 , qa loss 0.36 , lm loss 0.00 , avg batch size 49.3
2023-06-14 22:27:26,772 - 20:09:55 - 870.4s - INFO - __main__ - progress 10.428 , lr 3.0E-05 , loss 0.351 , qa loss 0.351 , lm loss 0.000 , avg batch size 49.3
2023-06-14 22:42:00,596 - 20:24:29 - 873.8s - INFO - __main__ - progress 10.857 , lr 2.9E-05 , loss 0.352 , qa loss 0.352 , lm loss 0.000 , avg batch size 49.3
2023-06-14 22:46:55,749 - 20:29:24 - 295.2s - INFO - __main__ - epoch 11/20 done , tot steps 25665 , lr 2.8E-05 , loss 0.35 , qa loss 0.35 , lm loss 0.00 , avg batch size 49.3
2023-06-14 23:01:39,229 - 20:44:07 - 883.5s - INFO - __main__ - progress 11.429 , lr 2.7E-05 , loss 0.348 , qa loss 0.348 , lm loss 0.000 , avg batch size 49.3
2023-06-14 23:16:10,812 - 20:58:39 - 871.6s - INFO - __main__ - progress 11.858 , lr 2.5E-05 , loss 0.348 , qa loss 0.348 , lm loss 0.000 , avg batch size 49.3
2023-06-14 23:21:06,031 - 21:03:34 - 295.2s - INFO - __main__ - epoch 12/20 done , tot steps 27997 , lr 2.5E-05 , loss 0.35 , qa loss 0.35 , lm loss 0.00 , avg batch size 49.3
2023-06-14 23:35:47,700 - 21:18:16 - 881.7s - INFO - __main__ - progress 12.430 , lr 2.4E-05 , loss 0.345 , qa loss 0.345 , lm loss 0.000 , avg batch size 49.5
2023-06-14 23:50:17,641 - 21:32:46 - 869.9s - INFO - __main__ - progress 12.860 , lr 2.2E-05 , loss 0.346 , qa loss 0.346 , lm loss 0.000 , avg batch size 49.4
2023-06-14 23:55:09,163 - 21:37:37 - 291.5s - INFO - __main__ - epoch 13/20 done , tot steps 30325 , lr 2.2E-05 , loss 0.35 , qa loss 0.35 , lm loss 0.00 , avg batch size 49.4
2023-06-15 00:09:49,889 - 21:52:18 - 880.7s - INFO - __main__ - progress 13.430 , lr 2.1E-05 , loss 0.342 , qa loss 0.342 , lm loss 0.000 , avg batch size 49.5
2023-06-15 00:24:22,284 - 22:06:50 - 872.4s - INFO - __main__ - progress 13.856 , lr 1.9E-05 , loss 0.343 , qa loss 0.343 , lm loss 0.000 , avg batch size 49.2
2023-06-15 00:29:18,925 - 22:11:47 - 296.6s - INFO - __main__ - epoch 14/20 done , tot steps 32660 , lr 1.9E-05 , loss 0.34 , qa loss 0.34 , lm loss 0.00 , avg batch size 49.3
2023-06-15 00:44:00,156 - 22:26:28 - 881.2s - INFO - __main__ - progress 14.429 , lr 1.7E-05 , loss 0.339 , qa loss 0.339 , lm loss 0.000 , avg batch size 49.3
2023-06-15 00:58:33,346 - 22:41:01 - 873.2s - INFO - __main__ - progress 14.859 , lr 1.6E-05 , loss 0.339 , qa loss 0.339 , lm loss 0.000 , avg batch size 49.4
2023-06-15 01:03:28,696 - 22:45:57 - 295.4s - INFO - __main__ - epoch 15/20 done , tot steps 34992 , lr 1.6E-05 , loss 0.34 , qa loss 0.34 , lm loss 0.00 , avg batch size 49.3
2023-06-15 01:18:10,922 - 23:00:39 - 882.2s - INFO - __main__ - progress 15.428 , lr 1.4E-05 , loss 0.336 , qa loss 0.336 , lm loss 0.000 , avg batch size 49.3
2023-06-15 01:32:43,224 - 23:15:11 - 872.3s - INFO - __main__ - progress 15.858 , lr 1.3E-05 , loss 0.337 , qa loss 0.337 , lm loss 0.000 , avg batch size 49.3
2023-06-15 01:37:40,905 - 23:20:09 - 297.7s - INFO - __main__ - epoch 16/20 done , tot steps 37327 , lr 1.3E-05 , loss 0.34 , qa loss 0.34 , lm loss 0.00 , avg batch size 49.3
2023-06-15 01:52:20,090 - 23:34:48 - 879.2s - INFO - __main__ - progress 16.427 , lr 1.1E-05 , loss 0.334 , qa loss 0.334 , lm loss 0.000 , avg batch size 49.1
2023-06-15 02:06:54,058 - 23:49:22 - 874.0s - INFO - __main__ - progress 16.856 , lr 9.8E-06 , loss 0.334 , qa loss 0.334 , lm loss 0.000 , avg batch size 49.2
2023-06-15 02:11:52,141 - 23:54:20 - 298.1s - INFO - __main__ - epoch 17/20 done , tot steps 39663 , lr 9.4E-06 , loss 0.33 , qa loss 0.33 , lm loss 0.00 , avg batch size 49.2
2023-06-15 02:26:34,496 - 1 day, 0:09:02 - 882.4s - INFO - __main__ - progress 17.428 , lr 8.1E-06 , loss 0.333 , qa loss 0.333 , lm loss 0.000 , avg batch size 49.2
2023-06-15 02:41:07,328 - 1 day, 0:23:35 - 872.8s - INFO - __main__ - progress 17.856 , lr 6.7E-06 , loss 0.332 , qa loss 0.332 , lm loss 0.000 , avg batch size 49.2
2023-06-15 02:46:05,768 - 1 day, 0:28:34 - 298.4s - INFO - __main__ - epoch 18/20 done , tot steps 41997 , lr 6.3E-06 , loss 0.33 , qa loss 0.33 , lm loss 0.00 , avg batch size 49.3
2023-06-15 03:00:47,498 - 1 day, 0:43:16 - 881.7s - INFO - __main__ - progress 18.429 , lr 4.9E-06 , loss 0.330 , qa loss 0.330 , lm loss 0.000 , avg batch size 49.3
2023-06-15 03:15:19,789 - 1 day, 0:57:48 - 872.3s - INFO - __main__ - progress 18.857 , lr 3.6E-06 , loss 0.330 , qa loss 0.330 , lm loss 0.000 , avg batch size 49.3
2023-06-15 03:20:17,521 - 1 day, 1:02:46 - 297.7s - INFO - __main__ - epoch 19/20 done , tot steps 44332 , lr 3.1E-06 , loss 0.33 , qa loss 0.33 , lm loss 0.00 , avg batch size 49.3
2023-06-15 03:34:58,703 - 1 day, 1:17:27 - 881.2s - INFO - __main__ - progress 19.430 , lr 1.8E-06 , loss 0.326 , qa loss 0.326 , lm loss 0.000 , avg batch size 49.4
2023-06-15 03:49:32,291 - 1 day, 1:32:00 - 873.6s - INFO - __main__ - progress 19.858 , lr 4.6E-07 , loss 0.328 , qa loss 0.328 , lm loss 0.000 , avg batch size 49.3
2023-06-15 03:54:27,930 - 1 day, 1:36:56 - 295.6s - INFO - __main__ - epoch 20/20 done , tot steps 46665 , lr 1.6E-08 , loss 0.33 , qa loss 0.33 , lm loss 0.00 , avg batch size 49.3
2023-06-15 03:54:41,921 - 1 day, 1:37:10 - 14.0s - INFO - __main__ - start to train { task: ['sst'], seq train type: lll }
2023-06-15 03:54:47,123 - 1 day, 1:37:15 - 5.2s - INFO - utils - writing extra data in ../../model_cm2/gpt2/lll/wikisql_ag_amazon_sst_srl_woz.en_0.0/amazon/lm.csv ...
2023-06-15 03:54:47,483 - 1 day, 1:37:15 - 0.4s - INFO - __main__ - extra training data size: 0
2023-06-15 03:54:49,402 - 1 day, 1:37:17 - 1.9s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[amazon]
The task with which model is saved amazon
2023-06-15 03:54:56,646 - 1 day, 1:37:25 - 7.2s - INFO - __main__ - len of train dataset: 6920 , max train batch size 4 , num of opt steps: 138400
2023-06-15 03:57:14,711 - 1 day, 1:39:43 - 138.1s - INFO - __main__ - progress 0.578 , lr 6.1E-05 , loss 1.747 , qa loss 1.747 , lm loss 0.000 , avg batch size 4.0
2023-06-15 03:59:03,024 - 1 day, 1:41:31 - 108.3s - INFO - __main__ - epoch 1/20 done , tot steps 1730 , lr 5.9E-05 , loss 1.14 , qa loss 1.14 , lm loss 0.00 , avg batch size 4.0
2023-06-15 04:01:17,147 - 1 day, 1:43:45 - 134.1s - INFO - __main__ - progress 1.578 , lr 5.8E-05 , loss 0.285 , qa loss 0.285 , lm loss 0.000 , avg batch size 4.0
2023-06-15 04:02:57,986 - 1 day, 1:45:26 - 100.8s - INFO - __main__ - epoch 2/20 done , tot steps 3460 , lr 5.6E-05 , loss 0.27 , qa loss 0.27 , lm loss 0.00 , avg batch size 4.0
2023-06-15 04:05:13,373 - 1 day, 1:47:41 - 135.4s - INFO - __main__ - progress 2.578 , lr 5.4E-05 , loss 0.235 , qa loss 0.235 , lm loss 0.000 , avg batch size 4.0
2023-06-15 04:06:52,614 - 1 day, 1:49:21 - 99.2s - INFO - __main__ - epoch 3/20 done , tot steps 5190 , lr 5.3E-05 , loss 0.23 , qa loss 0.23 , lm loss 0.00 , avg batch size 4.0
2023-06-15 04:09:10,880 - 1 day, 1:51:39 - 138.3s - INFO - __main__ - progress 3.578 , lr 5.1E-05 , loss 0.215 , qa loss 0.215 , lm loss 0.000 , avg batch size 4.0
2023-06-15 04:10:50,165 - 1 day, 1:53:18 - 99.3s - INFO - __main__ - epoch 4/20 done , tot steps 6920 , lr 5.0E-05 , loss 0.21 , qa loss 0.21 , lm loss 0.00 , avg batch size 4.0
2023-06-15 04:13:05,681 - 1 day, 1:55:34 - 135.5s - INFO - __main__ - progress 4.578 , lr 4.8E-05 , loss 0.179 , qa loss 0.179 , lm loss 0.000 , avg batch size 4.0
2023-06-15 04:14:45,413 - 1 day, 1:57:13 - 99.7s - INFO - __main__ - epoch 5/20 done , tot steps 8650 , lr 4.7E-05 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-06-15 04:17:02,542 - 1 day, 1:59:31 - 137.1s - INFO - __main__ - progress 5.578 , lr 4.5E-05 , loss 0.164 , qa loss 0.164 , lm loss 0.000 , avg batch size 4.0
2023-06-15 04:18:40,880 - 1 day, 2:01:09 - 98.3s - INFO - __main__ - epoch 6/20 done , tot steps 10380 , lr 4.4E-05 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-06-15 04:20:55,831 - 1 day, 2:03:24 - 135.0s - INFO - __main__ - progress 6.578 , lr 4.2E-05 , loss 0.141 , qa loss 0.141 , lm loss 0.000 , avg batch size 4.0
2023-06-15 04:22:35,189 - 1 day, 2:05:03 - 99.4s - INFO - __main__ - epoch 7/20 done , tot steps 12110 , lr 4.1E-05 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-06-15 04:24:53,013 - 1 day, 2:07:21 - 137.8s - INFO - __main__ - progress 7.578 , lr 3.9E-05 , loss 0.133 , qa loss 0.133 , lm loss 0.000 , avg batch size 4.0
2023-06-15 04:26:33,854 - 1 day, 2:09:02 - 100.8s - INFO - __main__ - epoch 8/20 done , tot steps 13840 , lr 3.8E-05 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-06-15 04:28:55,205 - 1 day, 2:11:23 - 141.4s - INFO - __main__ - progress 8.578 , lr 3.6E-05 , loss 0.112 , qa loss 0.112 , lm loss 0.000 , avg batch size 4.0
2023-06-15 04:30:39,037 - 1 day, 2:13:07 - 103.8s - INFO - __main__ - epoch 9/20 done , tot steps 15570 , lr 3.4E-05 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2023-06-15 04:33:00,227 - 1 day, 2:15:28 - 141.2s - INFO - __main__ - progress 9.578 , lr 3.3E-05 , loss 0.093 , qa loss 0.093 , lm loss 0.000 , avg batch size 4.0
2023-06-15 04:34:43,997 - 1 day, 2:17:12 - 103.8s - INFO - __main__ - epoch 10/20 done , tot steps 17300 , lr 3.1E-05 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 4.0
2023-06-15 04:37:07,295 - 1 day, 2:19:35 - 143.3s - INFO - __main__ - progress 10.578 , lr 2.9E-05 , loss 0.094 , qa loss 0.094 , lm loss 0.000 , avg batch size 4.0
2023-06-15 04:38:51,018 - 1 day, 2:21:19 - 103.7s - INFO - __main__ - epoch 11/20 done , tot steps 19030 , lr 2.8E-05 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 4.0
2023-06-15 04:41:12,817 - 1 day, 2:23:41 - 141.8s - INFO - __main__ - progress 11.578 , lr 2.6E-05 , loss 0.077 , qa loss 0.077 , lm loss 0.000 , avg batch size 4.0
2023-06-15 04:42:57,045 - 1 day, 2:25:25 - 104.2s - INFO - __main__ - epoch 12/20 done , tot steps 20760 , lr 2.5E-05 , loss 0.08 , qa loss 0.08 , lm loss 0.00 , avg batch size 4.0
2023-06-15 04:45:19,534 - 1 day, 2:27:48 - 142.5s - INFO - __main__ - progress 12.578 , lr 2.3E-05 , loss 0.067 , qa loss 0.067 , lm loss 0.000 , avg batch size 4.0
2023-06-15 04:47:04,034 - 1 day, 2:29:32 - 104.5s - INFO - __main__ - epoch 13/20 done , tot steps 22490 , lr 2.2E-05 , loss 0.07 , qa loss 0.07 , lm loss 0.00 , avg batch size 4.0
2023-06-15 04:49:27,133 - 1 day, 2:31:55 - 143.1s - INFO - __main__ - progress 13.578 , lr 2.0E-05 , loss 0.063 , qa loss 0.063 , lm loss 0.000 , avg batch size 4.0
2023-06-15 04:51:10,049 - 1 day, 2:33:38 - 102.9s - INFO - __main__ - epoch 14/20 done , tot steps 24220 , lr 1.9E-05 , loss 0.07 , qa loss 0.07 , lm loss 0.00 , avg batch size 4.0
2023-06-15 04:53:30,974 - 1 day, 2:35:59 - 140.9s - INFO - __main__ - progress 14.578 , lr 1.7E-05 , loss 0.048 , qa loss 0.048 , lm loss 0.000 , avg batch size 4.0
2023-06-15 04:55:14,488 - 1 day, 2:37:42 - 103.5s - INFO - __main__ - epoch 15/20 done , tot steps 25950 , lr 1.6E-05 , loss 0.06 , qa loss 0.06 , lm loss 0.00 , avg batch size 4.0
2023-06-15 04:57:37,224 - 1 day, 2:40:05 - 142.7s - INFO - __main__ - progress 15.578 , lr 1.4E-05 , loss 0.054 , qa loss 0.054 , lm loss 0.000 , avg batch size 4.0
2023-06-15 04:59:20,370 - 1 day, 2:41:48 - 103.1s - INFO - __main__ - epoch 16/20 done , tot steps 27680 , lr 1.3E-05 , loss 0.06 , qa loss 0.06 , lm loss 0.00 , avg batch size 4.0
2023-06-15 05:01:42,397 - 1 day, 2:44:10 - 142.0s - INFO - __main__ - progress 16.578 , lr 1.1E-05 , loss 0.050 , qa loss 0.050 , lm loss 0.000 , avg batch size 4.0
2023-06-15 05:03:25,025 - 1 day, 2:45:53 - 102.6s - INFO - __main__ - epoch 17/20 done , tot steps 29410 , lr 9.4E-06 , loss 0.05 , qa loss 0.05 , lm loss 0.00 , avg batch size 4.0
2023-06-15 05:05:47,799 - 1 day, 2:48:16 - 142.8s - INFO - __main__ - progress 17.578 , lr 7.6E-06 , loss 0.045 , qa loss 0.045 , lm loss 0.000 , avg batch size 4.0
2023-06-15 05:07:31,905 - 1 day, 2:50:00 - 104.1s - INFO - __main__ - epoch 18/20 done , tot steps 31140 , lr 6.3E-06 , loss 0.04 , qa loss 0.04 , lm loss 0.00 , avg batch size 4.0
2023-06-15 05:09:53,318 - 1 day, 2:52:21 - 141.4s - INFO - __main__ - progress 18.578 , lr 4.5E-06 , loss 0.046 , qa loss 0.046 , lm loss 0.000 , avg batch size 4.0
2023-06-15 05:11:36,869 - 1 day, 2:54:05 - 103.6s - INFO - __main__ - epoch 19/20 done , tot steps 32870 , lr 3.1E-06 , loss 0.05 , qa loss 0.05 , lm loss 0.00 , avg batch size 4.0
2023-06-15 05:13:58,295 - 1 day, 2:56:26 - 141.4s - INFO - __main__ - progress 19.578 , lr 1.3E-06 , loss 0.031 , qa loss 0.031 , lm loss 0.000 , avg batch size 4.0
2023-06-15 05:15:42,466 - 1 day, 2:58:10 - 104.2s - INFO - __main__ - epoch 20/20 done , tot steps 34600 , lr 1.5E-08 , loss 0.03 , qa loss 0.03 , lm loss 0.00 , avg batch size 4.0
2023-06-15 05:15:55,968 - 1 day, 2:58:24 - 13.5s - INFO - __main__ - start to train { task: ['srl'], seq train type: lll }
2023-06-15 05:16:00,890 - 1 day, 2:58:29 - 4.9s - INFO - utils - writing extra data in ../../model_cm2/gpt2/lll/wikisql_ag_amazon_sst_srl_woz.en_0.0/sst/lm.csv ...
2023-06-15 05:16:01,193 - 1 day, 2:58:29 - 0.3s - INFO - __main__ - extra training data size: 0
2023-06-15 05:16:02,719 - 1 day, 2:58:31 - 1.5s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[sst]
The task with which model is saved sst
2023-06-15 05:16:09,291 - 1 day, 2:58:37 - 6.6s - INFO - __main__ - len of train dataset: 6414 , max train batch size 4 , num of opt steps: 128280
2023-06-15 05:19:33,088 - 1 day, 3:02:01 - 203.8s - INFO - __main__ - progress 0.624 , lr 6.1E-05 , loss 3.716 , qa loss 3.716 , lm loss 0.000 , avg batch size 4.0
2023-06-15 05:21:43,659 - 1 day, 3:04:12 - 130.6s - INFO - __main__ - epoch 1/20 done , tot steps 1604 , lr 5.9E-05 , loss 2.66 , qa loss 2.66 , lm loss 0.00 , avg batch size 4.0
2023-06-15 05:25:02,908 - 1 day, 3:07:31 - 199.2s - INFO - __main__ - progress 1.624 , lr 5.7E-05 , loss 0.710 , qa loss 0.710 , lm loss 0.000 , avg batch size 4.0
2023-06-15 05:27:05,213 - 1 day, 3:09:33 - 122.3s - INFO - __main__ - epoch 2/20 done , tot steps 3208 , lr 5.6E-05 , loss 0.69 , qa loss 0.69 , lm loss 0.00 , avg batch size 4.0
2023-06-15 05:30:22,496 - 1 day, 3:12:50 - 197.3s - INFO - __main__ - progress 2.624 , lr 5.4E-05 , loss 0.553 , qa loss 0.553 , lm loss 0.000 , avg batch size 4.0
2023-06-15 05:32:24,600 - 1 day, 3:14:53 - 122.1s - INFO - __main__ - epoch 3/20 done , tot steps 4812 , lr 5.3E-05 , loss 0.55 , qa loss 0.55 , lm loss 0.00 , avg batch size 4.0
2023-06-15 05:35:44,383 - 1 day, 3:18:12 - 199.8s - INFO - __main__ - progress 3.624 , lr 5.1E-05 , loss 0.477 , qa loss 0.477 , lm loss 0.000 , avg batch size 4.0
2023-06-15 05:37:48,600 - 1 day, 3:20:17 - 124.2s - INFO - __main__ - epoch 4/20 done , tot steps 6416 , lr 5.0E-05 , loss 0.47 , qa loss 0.47 , lm loss 0.00 , avg batch size 4.0
2023-06-15 05:41:09,089 - 1 day, 3:23:37 - 200.5s - INFO - __main__ - progress 4.624 , lr 4.8E-05 , loss 0.431 , qa loss 0.431 , lm loss 0.000 , avg batch size 4.0
2023-06-15 05:43:10,999 - 1 day, 3:25:39 - 121.9s - INFO - __main__ - epoch 5/20 done , tot steps 8020 , lr 4.7E-05 , loss 0.43 , qa loss 0.43 , lm loss 0.00 , avg batch size 4.0
2023-06-15 05:46:29,897 - 1 day, 3:28:58 - 198.9s - INFO - __main__ - progress 5.624 , lr 4.5E-05 , loss 0.378 , qa loss 0.378 , lm loss 0.000 , avg batch size 4.0
2023-06-15 05:48:33,676 - 1 day, 3:31:02 - 123.8s - INFO - __main__ - epoch 6/20 done , tot steps 9624 , lr 4.4E-05 , loss 0.38 , qa loss 0.38 , lm loss 0.00 , avg batch size 4.0
2023-06-15 05:51:55,315 - 1 day, 3:34:23 - 201.6s - INFO - __main__ - progress 6.624 , lr 4.2E-05 , loss 0.351 , qa loss 0.351 , lm loss 0.000 , avg batch size 4.0
2023-06-15 05:53:57,841 - 1 day, 3:36:26 - 122.5s - INFO - __main__ - epoch 7/20 done , tot steps 11228 , lr 4.1E-05 , loss 0.35 , qa loss 0.35 , lm loss 0.00 , avg batch size 4.0
2023-06-15 05:57:22,155 - 1 day, 3:39:50 - 204.3s - INFO - __main__ - progress 7.624 , lr 3.9E-05 , loss 0.316 , qa loss 0.316 , lm loss 0.000 , avg batch size 4.0
2023-06-15 05:59:22,154 - 1 day, 3:41:50 - 120.0s - INFO - __main__ - epoch 8/20 done , tot steps 12832 , lr 3.8E-05 , loss 0.32 , qa loss 0.32 , lm loss 0.00 , avg batch size 4.0
2023-06-15 06:02:41,121 - 1 day, 3:45:09 - 199.0s - INFO - __main__ - progress 8.624 , lr 3.6E-05 , loss 0.285 , qa loss 0.285 , lm loss 0.000 , avg batch size 4.0
2023-06-15 06:04:46,007 - 1 day, 3:47:14 - 124.9s - INFO - __main__ - epoch 9/20 done , tot steps 14436 , lr 3.4E-05 , loss 0.29 , qa loss 0.29 , lm loss 0.00 , avg batch size 4.0
2023-06-15 06:08:08,697 - 1 day, 3:50:37 - 202.7s - INFO - __main__ - progress 9.624 , lr 3.2E-05 , loss 0.276 , qa loss 0.276 , lm loss 0.000 , avg batch size 4.0
2023-06-15 06:10:09,446 - 1 day, 3:52:37 - 120.7s - INFO - __main__ - epoch 10/20 done , tot steps 16040 , lr 3.1E-05 , loss 0.27 , qa loss 0.27 , lm loss 0.00 , avg batch size 4.0
2023-06-15 06:13:31,741 - 1 day, 3:56:00 - 202.3s - INFO - __main__ - progress 10.624 , lr 2.9E-05 , loss 0.260 , qa loss 0.260 , lm loss 0.000 , avg batch size 4.0
2023-06-15 06:15:33,085 - 1 day, 3:58:01 - 121.3s - INFO - __main__ - epoch 11/20 done , tot steps 17644 , lr 2.8E-05 , loss 0.25 , qa loss 0.25 , lm loss 0.00 , avg batch size 4.0
2023-06-15 06:18:57,182 - 1 day, 4:01:25 - 204.1s - INFO - __main__ - progress 11.624 , lr 2.6E-05 , loss 0.232 , qa loss 0.232 , lm loss 0.000 , avg batch size 4.0
2023-06-15 06:20:56,010 - 1 day, 4:03:24 - 118.8s - INFO - __main__ - epoch 12/20 done , tot steps 19248 , lr 2.5E-05 , loss 0.23 , qa loss 0.23 , lm loss 0.00 , avg batch size 4.0
2023-06-15 06:24:23,683 - 1 day, 4:06:52 - 207.7s - INFO - __main__ - progress 12.624 , lr 2.3E-05 , loss 0.214 , qa loss 0.214 , lm loss 0.000 , avg batch size 4.0
2023-06-15 06:26:21,173 - 1 day, 4:08:49 - 117.5s - INFO - __main__ - epoch 13/20 done , tot steps 20852 , lr 2.2E-05 , loss 0.21 , qa loss 0.21 , lm loss 0.00 , avg batch size 4.0
2023-06-15 06:29:41,935 - 1 day, 4:12:10 - 200.8s - INFO - __main__ - progress 13.624 , lr 2.0E-05 , loss 0.206 , qa loss 0.206 , lm loss 0.000 , avg batch size 4.0
2023-06-15 06:31:43,120 - 1 day, 4:14:11 - 121.2s - INFO - __main__ - epoch 14/20 done , tot steps 22456 , lr 1.9E-05 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-06-15 06:35:04,856 - 1 day, 4:17:33 - 201.7s - INFO - __main__ - progress 14.624 , lr 1.7E-05 , loss 0.186 , qa loss 0.186 , lm loss 0.000 , avg batch size 4.0
2023-06-15 06:37:06,603 - 1 day, 4:19:35 - 121.7s - INFO - __main__ - epoch 15/20 done , tot steps 24060 , lr 1.6E-05 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-06-15 06:40:29,103 - 1 day, 4:22:57 - 202.5s - INFO - __main__ - progress 15.624 , lr 1.4E-05 , loss 0.177 , qa loss 0.177 , lm loss 0.000 , avg batch size 4.0
2023-06-15 06:42:29,416 - 1 day, 4:24:57 - 120.3s - INFO - __main__ - epoch 16/20 done , tot steps 25664 , lr 1.3E-05 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-06-15 06:45:50,957 - 1 day, 4:28:19 - 201.5s - INFO - __main__ - progress 16.624 , lr 1.1E-05 , loss 0.170 , qa loss 0.170 , lm loss 0.000 , avg batch size 4.0
2023-06-15 06:47:50,773 - 1 day, 4:30:19 - 119.8s - INFO - __main__ - epoch 17/20 done , tot steps 27268 , lr 9.4E-06 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
2023-06-15 06:51:15,363 - 1 day, 4:33:43 - 204.6s - INFO - __main__ - progress 17.624 , lr 7.4E-06 , loss 0.157 , qa loss 0.157 , lm loss 0.000 , avg batch size 4.0
2023-06-15 06:53:14,809 - 1 day, 4:35:43 - 119.4s - INFO - __main__ - epoch 18/20 done , tot steps 28872 , lr 6.3E-06 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-06-15 06:56:34,404 - 1 day, 4:39:02 - 199.6s - INFO - __main__ - progress 18.624 , lr 4.3E-06 , loss 0.155 , qa loss 0.155 , lm loss 0.000 , avg batch size 4.0
2023-06-15 06:58:35,753 - 1 day, 4:41:04 - 121.3s - INFO - __main__ - epoch 19/20 done , tot steps 30476 , lr 3.1E-06 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-06-15 07:01:55,411 - 1 day, 4:44:23 - 199.7s - INFO - __main__ - progress 19.624 , lr 1.2E-06 , loss 0.143 , qa loss 0.143 , lm loss 0.000 , avg batch size 4.0
2023-06-15 07:03:58,542 - 1 day, 4:46:27 - 123.1s - INFO - __main__ - epoch 20/20 done , tot steps 32080 , lr 1.6E-08 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
2023-06-15 07:04:10,688 - 1 day, 4:46:39 - 12.1s - INFO - __main__ - start to train { task: ['woz.en'], seq train type: lll }
2023-06-15 07:04:15,999 - 1 day, 4:46:44 - 5.3s - INFO - utils - writing extra data in ../../model_cm2/gpt2/lll/wikisql_ag_amazon_sst_srl_woz.en_0.0/srl/lm.csv ...
2023-06-15 07:04:16,424 - 1 day, 4:46:44 - 0.4s - INFO - __main__ - extra training data size: 0
2023-06-15 07:04:18,526 - 1 day, 4:46:47 - 2.1s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[srl]
The task with which model is saved srl
2023-06-15 07:04:26,276 - 1 day, 4:46:54 - 7.8s - INFO - __main__ - len of train dataset: 2536 , max train batch size 4 , num of opt steps: 50720
2023-06-15 07:06:09,818 - 1 day, 4:48:38 - 103.5s - INFO - __main__ - epoch 1/20 done , tot steps 634 , lr 5.9E-05 , loss 3.25 , qa loss 3.25 , lm loss 0.00 , avg batch size 4.0
2023-06-15 07:07:45,707 - 1 day, 4:50:14 - 95.9s - INFO - __main__ - epoch 2/20 done , tot steps 1268 , lr 5.6E-05 , loss 0.53 , qa loss 0.53 , lm loss 0.00 , avg batch size 4.0
2023-06-15 07:09:20,768 - 1 day, 4:51:49 - 95.1s - INFO - __main__ - epoch 3/20 done , tot steps 1902 , lr 5.3E-05 , loss 0.35 , qa loss 0.35 , lm loss 0.00 , avg batch size 4.0
2023-06-15 07:10:56,479 - 1 day, 4:53:24 - 95.7s - INFO - __main__ - epoch 4/20 done , tot steps 2536 , lr 5.0E-05 , loss 0.28 , qa loss 0.28 , lm loss 0.00 , avg batch size 4.0
2023-06-15 07:12:32,280 - 1 day, 4:55:00 - 95.8s - INFO - __main__ - epoch 5/20 done , tot steps 3170 , lr 4.7E-05 , loss 0.23 , qa loss 0.23 , lm loss 0.00 , avg batch size 4.0
2023-06-15 07:14:09,504 - 1 day, 4:56:38 - 97.2s - INFO - __main__ - epoch 6/20 done , tot steps 3804 , lr 4.4E-05 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-06-15 07:15:44,492 - 1 day, 4:58:12 - 95.0s - INFO - __main__ - epoch 7/20 done , tot steps 4438 , lr 4.1E-05 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-06-15 07:17:19,621 - 1 day, 4:59:48 - 95.1s - INFO - __main__ - epoch 8/20 done , tot steps 5072 , lr 3.8E-05 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
2023-06-15 07:18:55,136 - 1 day, 5:01:23 - 95.5s - INFO - __main__ - epoch 9/20 done , tot steps 5706 , lr 3.4E-05 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-06-15 07:20:30,006 - 1 day, 5:02:58 - 94.9s - INFO - __main__ - epoch 10/20 done , tot steps 6340 , lr 3.1E-05 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
2023-06-15 07:22:05,546 - 1 day, 5:04:34 - 95.5s - INFO - __main__ - epoch 11/20 done , tot steps 6974 , lr 2.8E-05 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-06-15 07:23:41,598 - 1 day, 5:06:10 - 96.1s - INFO - __main__ - epoch 12/20 done , tot steps 7608 , lr 2.5E-05 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-06-15 07:25:16,750 - 1 day, 5:07:45 - 95.2s - INFO - __main__ - epoch 13/20 done , tot steps 8242 , lr 2.2E-05 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2023-06-15 07:26:53,548 - 1 day, 5:09:22 - 96.8s - INFO - __main__ - epoch 14/20 done , tot steps 8876 , lr 1.9E-05 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2023-06-15 07:28:29,562 - 1 day, 5:10:58 - 96.0s - INFO - __main__ - epoch 15/20 done , tot steps 9510 , lr 1.6E-05 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 4.0
2023-06-15 07:30:05,475 - 1 day, 5:12:33 - 95.9s - INFO - __main__ - epoch 16/20 done , tot steps 10144 , lr 1.3E-05 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-06-15 07:31:40,918 - 1 day, 5:14:09 - 95.4s - INFO - __main__ - epoch 17/20 done , tot steps 10778 , lr 9.4E-06 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 4.0
2023-06-15 07:33:16,868 - 1 day, 5:15:45 - 96.0s - INFO - __main__ - epoch 18/20 done , tot steps 11412 , lr 6.3E-06 , loss 0.09 , qa loss 0.09 , lm loss 0.00 , avg batch size 4.0
2023-06-15 07:34:51,694 - 1 day, 5:17:20 - 94.8s - INFO - __main__ - epoch 19/20 done , tot steps 12046 , lr 3.1E-06 , loss 0.09 , qa loss 0.09 , lm loss 0.00 , avg batch size 4.0
2023-06-15 07:36:26,303 - 1 day, 5:18:54 - 94.6s - INFO - __main__ - epoch 20/20 done , tot steps 12680 , lr 1.5E-08 , loss 0.09 , qa loss 0.09 , lm loss 0.00 , avg batch size 4.0
The current active adapter is Stack[woz_en]
The task with which model is saved woz_en
Wall Execution time: 05:08:02
CPU Execution time: 15:53:41
