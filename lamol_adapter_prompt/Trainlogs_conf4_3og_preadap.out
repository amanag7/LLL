Not all gpus support fp16 training! Will use fp32 instead.
2023-04-27 15:18:55,074 - 0:00:04 - 0.0s - INFO - __main__ - args = Namespace(REG_TYPE_KEYS=['mas', 'ewc'], adam_epsilon=0.0001, add_task_tokens=False, data_dir='../../data', debug=False, decay_style='linear', device_ids=[0, 1, 2, 3, 4, 5], dynamic_epochs=False, fp32=True, gen_lm_sample_percentage=0.0, learning_rate=6.25e-05, lm_lambda=0.0, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[32429.600000000002, 42158.48, 42158.48, 42158.48, 42158.48, 42158.48], min_batch_size=4, min_n_steps=1500, model_dir_root='../../model_conf4/gpt2/lll/sst_srl_woz.en_0.0', model_name='gpt2', n_gpus=6, n_train_epochs={'sst': 12, 'srl': 12, 'woz.en': 12}, n_warmup_ratio=0.005, n_workers=75, qp_margin=0.5, real_sample=False, reg_lambda=1.0, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[11350, 14755, 14755, 14755, 14755, 14755], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[11350, 14755, 14755, 14755, 14755, 14755], unbound=0, use_sep=False, weight_decay=0.01)
2023-04-27 15:18:55,074 - 0:00:04 - 0.0s - INFO - __main__ - start to train { task: ['sst'], seq train type: lll }
2023-04-27 15:18:55,075 - 0:00:04 - 0.0s - INFO - __main__ - extra training data size: 0
2023-04-27 15:18:57,334 - 0:00:06 - 2.3s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
2023-04-27 15:19:00,646 - 0:00:09 - 3.3s - INFO - __main__ - len of train dataset: 6920 , max train batch size 4 , num of opt steps: 83040
/home/student/2021/cs21mtech11006/anaconda3/envs/lamol/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/home/student/2021/cs21mtech11006/anaconda3/envs/lamol/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
2023-04-27 15:21:15,018 - 0:02:24 - 134.4s - INFO - __main__ - progress 0.578 , lr 6.0E-05 , loss 2.158 , qa loss 2.158 , lm loss 0.000 , avg batch size 4.0
2023-04-27 15:22:50,853 - 0:04:00 - 95.8s - INFO - __main__ - epoch 1/12 done , tot steps 1730 , lr 5.7E-05 , loss 1.37 , qa loss 1.37 , lm loss 0.00 , avg batch size 4.0
2023-04-27 15:24:58,251 - 0:06:07 - 127.4s - INFO - __main__ - progress 1.578 , lr 5.4E-05 , loss 0.285 , qa loss 0.285 , lm loss 0.000 , avg batch size 4.0
2023-04-27 15:26:29,741 - 0:07:38 - 91.5s - INFO - __main__ - epoch 2/12 done , tot steps 3460 , lr 5.2E-05 , loss 0.27 , qa loss 0.27 , lm loss 0.00 , avg batch size 4.0
2023-04-27 15:28:36,380 - 0:09:45 - 126.6s - INFO - __main__ - progress 2.578 , lr 4.9E-05 , loss 0.241 , qa loss 0.241 , lm loss 0.000 , avg batch size 4.0
2023-04-27 15:30:08,529 - 0:11:17 - 92.1s - INFO - __main__ - epoch 3/12 done , tot steps 5190 , lr 4.7E-05 , loss 0.24 , qa loss 0.24 , lm loss 0.00 , avg batch size 4.0
2023-04-27 15:32:16,176 - 0:13:25 - 127.6s - INFO - __main__ - progress 3.578 , lr 4.4E-05 , loss 0.228 , qa loss 0.228 , lm loss 0.000 , avg batch size 4.0
2023-04-27 15:33:47,075 - 0:14:56 - 90.9s - INFO - __main__ - epoch 4/12 done , tot steps 6920 , lr 4.2E-05 , loss 0.22 , qa loss 0.22 , lm loss 0.00 , avg batch size 4.0
2023-04-27 15:35:55,292 - 0:17:04 - 128.2s - INFO - __main__ - progress 4.578 , lr 3.9E-05 , loss 0.217 , qa loss 0.217 , lm loss 0.000 , avg batch size 4.0
2023-04-27 15:37:26,636 - 0:18:35 - 91.3s - INFO - __main__ - epoch 5/12 done , tot steps 8650 , lr 3.6E-05 , loss 0.21 , qa loss 0.21 , lm loss 0.00 , avg batch size 4.0
2023-04-27 15:39:34,128 - 0:20:43 - 127.5s - INFO - __main__ - progress 5.578 , lr 3.3E-05 , loss 0.195 , qa loss 0.195 , lm loss 0.000 , avg batch size 4.0
2023-04-27 15:41:05,032 - 0:22:14 - 90.9s - INFO - __main__ - epoch 6/12 done , tot steps 10380 , lr 3.1E-05 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-04-27 15:43:12,998 - 0:24:22 - 128.0s - INFO - __main__ - progress 6.578 , lr 2.8E-05 , loss 0.180 , qa loss 0.180 , lm loss 0.000 , avg batch size 4.0
2023-04-27 15:44:43,240 - 0:25:52 - 90.2s - INFO - __main__ - epoch 7/12 done , tot steps 12110 , lr 2.6E-05 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-04-27 15:46:50,559 - 0:27:59 - 127.3s - INFO - __main__ - progress 7.578 , lr 2.3E-05 , loss 0.175 , qa loss 0.175 , lm loss 0.000 , avg batch size 4.0
2023-04-27 15:48:21,821 - 0:29:31 - 91.3s - INFO - __main__ - epoch 8/12 done , tot steps 13840 , lr 2.1E-05 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-04-27 15:50:29,206 - 0:31:38 - 127.4s - INFO - __main__ - progress 8.578 , lr 1.8E-05 , loss 0.162 , qa loss 0.162 , lm loss 0.000 , avg batch size 4.0
2023-04-27 15:52:00,648 - 0:33:09 - 91.4s - INFO - __main__ - epoch 9/12 done , tot steps 15570 , lr 1.6E-05 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-04-27 15:54:08,317 - 0:35:17 - 127.7s - INFO - __main__ - progress 9.578 , lr 1.3E-05 , loss 0.146 , qa loss 0.146 , lm loss 0.000 , avg batch size 4.0
2023-04-27 15:55:39,176 - 0:36:48 - 90.9s - INFO - __main__ - epoch 10/12 done , tot steps 17300 , lr 1.0E-05 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-04-27 15:57:45,751 - 0:38:54 - 126.6s - INFO - __main__ - progress 10.578 , lr 7.4E-06 , loss 0.142 , qa loss 0.142 , lm loss 0.000 , avg batch size 4.0
2023-04-27 15:59:17,284 - 0:40:26 - 91.5s - INFO - __main__ - epoch 11/12 done , tot steps 19030 , lr 5.2E-06 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-04-27 16:01:24,808 - 0:42:34 - 127.5s - INFO - __main__ - progress 11.578 , lr 2.2E-06 , loss 0.145 , qa loss 0.145 , lm loss 0.000 , avg batch size 4.0
2023-04-27 16:02:56,129 - 0:44:05 - 91.3s - INFO - __main__ - epoch 12/12 done , tot steps 20760 , lr 2.6E-08 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
2023-04-27 16:03:03,258 - 0:44:12 - 7.1s - INFO - __main__ - start to train { task: ['srl'], seq train type: lll }
2023-04-27 16:03:05,537 - 0:44:14 - 2.3s - INFO - utils - writing extra data in ../../model_conf4/gpt2/lll/sst_srl_woz.en_0.0/sst/lm.csv ...
2023-04-27 16:03:05,538 - 0:44:14 - 0.0s - INFO - __main__ - extra training data size: 0
2023-04-27 16:03:05,659 - 0:44:14 - 0.1s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[sst]
The task with which model is saved sst
2023-04-27 16:03:10,570 - 0:44:19 - 4.9s - INFO - __main__ - len of train dataset: 6414 , max train batch size 4 , num of opt steps: 76968
2023-04-27 16:06:21,694 - 0:47:30 - 191.1s - INFO - __main__ - progress 0.624 , lr 5.9E-05 , loss 5.223 , qa loss 5.223 , lm loss 0.000 , avg batch size 4.0
2023-04-27 16:08:15,639 - 0:49:24 - 113.9s - INFO - __main__ - epoch 1/12 done , tot steps 1604 , lr 5.7E-05 , loss 3.94 , qa loss 3.94 , lm loss 0.00 , avg batch size 4.0
2023-04-27 16:11:21,771 - 0:52:30 - 186.1s - INFO - __main__ - progress 1.624 , lr 5.4E-05 , loss 1.228 , qa loss 1.228 , lm loss 0.000 , avg batch size 4.0
2023-04-27 16:13:15,164 - 0:54:24 - 113.4s - INFO - __main__ - epoch 2/12 done , tot steps 3208 , lr 5.2E-05 , loss 1.14 , qa loss 1.14 , lm loss 0.00 , avg batch size 4.0
2023-04-27 16:16:23,408 - 0:57:32 - 188.2s - INFO - __main__ - progress 2.624 , lr 4.9E-05 , loss 0.855 , qa loss 0.855 , lm loss 0.000 , avg batch size 4.0
2023-04-27 16:18:13,432 - 0:59:22 - 110.0s - INFO - __main__ - epoch 3/12 done , tot steps 4812 , lr 4.7E-05 , loss 0.83 , qa loss 0.83 , lm loss 0.00 , avg batch size 4.0
2023-04-27 16:21:23,016 - 1:02:32 - 189.6s - INFO - __main__ - progress 3.624 , lr 4.4E-05 , loss 0.685 , qa loss 0.685 , lm loss 0.000 , avg batch size 4.0
2023-04-27 16:23:13,971 - 1:04:23 - 111.0s - INFO - __main__ - epoch 4/12 done , tot steps 6416 , lr 4.2E-05 , loss 0.69 , qa loss 0.69 , lm loss 0.00 , avg batch size 4.0
2023-04-27 16:26:22,747 - 1:07:31 - 188.8s - INFO - __main__ - progress 4.624 , lr 3.8E-05 , loss 0.610 , qa loss 0.610 , lm loss 0.000 , avg batch size 4.0
2023-04-27 16:28:14,532 - 1:09:23 - 111.8s - INFO - __main__ - epoch 5/12 done , tot steps 8020 , lr 3.6E-05 , loss 0.61 , qa loss 0.61 , lm loss 0.00 , avg batch size 4.0
2023-04-27 16:31:25,220 - 1:12:34 - 190.7s - INFO - __main__ - progress 5.624 , lr 3.3E-05 , loss 0.560 , qa loss 0.560 , lm loss 0.000 , avg batch size 4.0
2023-04-27 16:33:14,303 - 1:14:23 - 109.1s - INFO - __main__ - epoch 6/12 done , tot steps 9624 , lr 3.1E-05 , loss 0.56 , qa loss 0.56 , lm loss 0.00 , avg batch size 4.0
2023-04-27 16:36:20,682 - 1:17:29 - 186.4s - INFO - __main__ - progress 6.624 , lr 2.8E-05 , loss 0.484 , qa loss 0.484 , lm loss 0.000 , avg batch size 4.0
2023-04-27 16:38:12,244 - 1:19:21 - 111.6s - INFO - __main__ - epoch 7/12 done , tot steps 11228 , lr 2.6E-05 , loss 0.49 , qa loss 0.49 , lm loss 0.00 , avg batch size 4.0
2023-04-27 16:41:14,794 - 1:22:24 - 182.6s - INFO - __main__ - progress 7.624 , lr 2.3E-05 , loss 0.452 , qa loss 0.452 , lm loss 0.000 , avg batch size 4.0
2023-04-27 16:43:06,555 - 1:24:15 - 111.8s - INFO - __main__ - epoch 8/12 done , tot steps 12832 , lr 2.1E-05 , loss 0.44 , qa loss 0.44 , lm loss 0.00 , avg batch size 4.0
2023-04-27 16:46:16,780 - 1:27:26 - 190.2s - INFO - __main__ - progress 8.624 , lr 1.8E-05 , loss 0.411 , qa loss 0.411 , lm loss 0.000 , avg batch size 4.0
2023-04-27 16:48:06,971 - 1:29:16 - 110.2s - INFO - __main__ - epoch 9/12 done , tot steps 14436 , lr 1.6E-05 , loss 0.41 , qa loss 0.41 , lm loss 0.00 , avg batch size 4.0
2023-04-27 16:51:15,074 - 1:32:24 - 188.1s - INFO - __main__ - progress 9.624 , lr 1.2E-05 , loss 0.379 , qa loss 0.379 , lm loss 0.000 , avg batch size 4.0
2023-04-27 16:53:06,433 - 1:34:15 - 111.4s - INFO - __main__ - epoch 10/12 done , tot steps 16040 , lr 1.0E-05 , loss 0.38 , qa loss 0.38 , lm loss 0.00 , avg batch size 4.0
2023-04-27 16:56:13,271 - 1:37:22 - 186.8s - INFO - __main__ - progress 10.624 , lr 7.2E-06 , loss 0.356 , qa loss 0.356 , lm loss 0.000 , avg batch size 4.0
2023-04-27 16:58:05,873 - 1:39:15 - 112.6s - INFO - __main__ - epoch 11/12 done , tot steps 17644 , lr 5.2E-06 , loss 0.35 , qa loss 0.35 , lm loss 0.00 , avg batch size 4.0
2023-04-27 17:01:15,567 - 1:42:24 - 189.7s - INFO - __main__ - progress 11.624 , lr 2.0E-06 , loss 0.339 , qa loss 0.339 , lm loss 0.000 , avg batch size 4.0
2023-04-27 17:03:08,136 - 1:44:17 - 112.6s - INFO - __main__ - epoch 12/12 done , tot steps 19248 , lr 2.6E-08 , loss 0.34 , qa loss 0.34 , lm loss 0.00 , avg batch size 4.0
2023-04-27 17:03:15,273 - 1:44:24 - 7.1s - INFO - __main__ - start to train { task: ['woz.en'], seq train type: lll }
2023-04-27 17:03:17,586 - 1:44:26 - 2.3s - INFO - utils - writing extra data in ../../model_conf4/gpt2/lll/sst_srl_woz.en_0.0/srl/lm.csv ...
2023-04-27 17:03:17,587 - 1:44:26 - 0.0s - INFO - __main__ - extra training data size: 0
2023-04-27 17:03:17,706 - 1:44:26 - 0.1s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[sst]
The task with which model is saved sst
2023-04-27 17:03:22,737 - 1:44:31 - 5.0s - INFO - __main__ - len of train dataset: 2536 , max train batch size 4 , num of opt steps: 30432
2023-04-27 17:04:52,271 - 1:46:01 - 89.5s - INFO - __main__ - epoch 1/12 done , tot steps 634 , lr 5.7E-05 , loss 4.04 , qa loss 4.04 , lm loss 0.00 , avg batch size 4.0
2023-04-27 17:06:15,571 - 1:47:24 - 83.3s - INFO - __main__ - epoch 2/12 done , tot steps 1268 , lr 5.2E-05 , loss 0.54 , qa loss 0.54 , lm loss 0.00 , avg batch size 4.0
2023-04-27 17:07:39,834 - 1:48:49 - 84.3s - INFO - __main__ - epoch 3/12 done , tot steps 1902 , lr 4.7E-05 , loss 0.36 , qa loss 0.36 , lm loss 0.00 , avg batch size 4.0
2023-04-27 17:09:03,577 - 1:50:12 - 83.7s - INFO - __main__ - epoch 4/12 done , tot steps 2536 , lr 4.2E-05 , loss 0.31 , qa loss 0.31 , lm loss 0.00 , avg batch size 4.0
2023-04-27 17:10:28,359 - 1:51:37 - 84.8s - INFO - __main__ - epoch 5/12 done , tot steps 3170 , lr 3.6E-05 , loss 0.25 , qa loss 0.25 , lm loss 0.00 , avg batch size 4.0
2023-04-27 17:11:52,240 - 1:53:01 - 83.9s - INFO - __main__ - epoch 6/12 done , tot steps 3804 , lr 3.1E-05 , loss 0.23 , qa loss 0.23 , lm loss 0.00 , avg batch size 4.0
2023-04-27 17:13:15,864 - 1:54:25 - 83.6s - INFO - __main__ - epoch 7/12 done , tot steps 4438 , lr 2.6E-05 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-04-27 17:14:39,555 - 1:55:48 - 83.7s - INFO - __main__ - epoch 8/12 done , tot steps 5072 , lr 2.1E-05 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-04-27 17:16:03,275 - 1:57:12 - 83.7s - INFO - __main__ - epoch 9/12 done , tot steps 5706 , lr 1.6E-05 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
2023-04-27 17:17:25,756 - 1:58:34 - 82.5s - INFO - __main__ - epoch 10/12 done , tot steps 6340 , lr 1.0E-05 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-04-27 17:18:49,937 - 1:59:59 - 84.2s - INFO - __main__ - epoch 11/12 done , tot steps 6974 , lr 5.2E-06 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-04-27 17:20:14,001 - 2:01:23 - 84.1s - INFO - __main__ - epoch 12/12 done , tot steps 7608 , lr 2.5E-08 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
The current active adapter is Stack[sst]
The task with which model is saved sst
Wall Execution time: 02:01:26
CPU Execution time: 02:01:15
