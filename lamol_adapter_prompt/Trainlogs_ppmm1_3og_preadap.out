Not all gpus support fp16 training! Will use fp32 instead.
2023-06-29 16:50:33,632 - 0:00:04 - 0.0s - INFO - __main__ - args = Namespace(REG_TYPE_KEYS=['mas', 'ewc'], adam_epsilon=0.0001, add_task_tokens=False, data_dir='../../data', debug=False, decay_style='linear', device_ids=[2], dynamic_epochs=False, fp32=True, gen_lm_sample_percentage=0.0, learning_rate=6.25e-05, lm_lambda=0.0, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[40537.0], min_batch_size=4, min_n_steps=1500, model_dir_root='../../model_ppmm1/gpt2/lll/sst_srl_woz.en_0.0', model_name='gpt2', n_gpus=1, n_train_epochs={'sst': 20, 'srl': 20, 'woz.en': 20}, n_warmup_ratio=0.005, n_workers=75, qp_margin=0.5, real_sample=False, reg_lambda=1.0, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[14187], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[14187], unbound=0, use_sep=False, weight_decay=0.01)
2023-06-29 16:50:33,632 - 0:00:04 - 0.0s - INFO - __main__ - start to train { task: ['sst'], seq train type: lll }
2023-06-29 16:50:33,641 - 0:00:04 - 0.0s - INFO - __main__ - extra training data size: 0
2023-06-29 16:50:36,128 - 0:00:07 - 2.5s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
2023-06-29 16:50:38,976 - 0:00:10 - 2.8s - INFO - __main__ - len of train dataset: 6920 , max train batch size 4 , num of opt steps: 138400
/home/student/2021/cs21mtech11006/anaconda3/envs/lamol/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
2023-06-29 16:52:38,421 - 0:02:09 - 119.4s - INFO - __main__ - progress 0.578 , lr 6.1E-05 , loss 2.133 , qa loss 2.133 , lm loss 0.000 , avg batch size 4.0
2023-06-29 16:54:04,979 - 0:03:36 - 86.6s - INFO - __main__ - epoch 1/20 done , tot steps 1730 , lr 5.9E-05 , loss 1.43 , qa loss 1.43 , lm loss 0.00 , avg batch size 4.0
2023-06-29 16:56:00,024 - 0:05:31 - 115.0s - INFO - __main__ - progress 1.578 , lr 5.8E-05 , loss 0.348 , qa loss 0.348 , lm loss 0.000 , avg batch size 4.0
2023-06-29 16:57:26,220 - 0:06:57 - 86.2s - INFO - __main__ - epoch 2/20 done , tot steps 3460 , lr 5.6E-05 , loss 0.31 , qa loss 0.31 , lm loss 0.00 , avg batch size 4.0
2023-06-29 16:59:22,819 - 0:08:54 - 116.6s - INFO - __main__ - progress 2.578 , lr 5.4E-05 , loss 0.269 , qa loss 0.269 , lm loss 0.000 , avg batch size 4.0
2023-06-29 17:00:48,793 - 0:10:19 - 86.0s - INFO - __main__ - epoch 3/20 done , tot steps 5190 , lr 5.3E-05 , loss 0.25 , qa loss 0.25 , lm loss 0.00 , avg batch size 4.0
2023-06-29 17:02:48,318 - 0:12:19 - 119.5s - INFO - __main__ - progress 3.578 , lr 5.1E-05 , loss 0.219 , qa loss 0.219 , lm loss 0.000 , avg batch size 4.0
2023-06-29 17:04:11,129 - 0:13:42 - 82.8s - INFO - __main__ - epoch 4/20 done , tot steps 6920 , lr 5.0E-05 , loss 0.22 , qa loss 0.22 , lm loss 0.00 , avg batch size 4.0
2023-06-29 17:06:10,504 - 0:15:41 - 119.4s - INFO - __main__ - progress 4.578 , lr 4.8E-05 , loss 0.202 , qa loss 0.202 , lm loss 0.000 , avg batch size 4.0
2023-06-29 17:07:32,962 - 0:17:04 - 82.5s - INFO - __main__ - epoch 5/20 done , tot steps 8650 , lr 4.7E-05 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-06-29 17:09:31,143 - 0:19:02 - 118.2s - INFO - __main__ - progress 5.578 , lr 4.5E-05 , loss 0.192 , qa loss 0.192 , lm loss 0.000 , avg batch size 4.0
2023-06-29 17:10:55,724 - 0:20:26 - 84.6s - INFO - __main__ - epoch 6/20 done , tot steps 10380 , lr 4.4E-05 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-06-29 17:12:55,264 - 0:22:26 - 119.5s - INFO - __main__ - progress 6.578 , lr 4.2E-05 , loss 0.176 , qa loss 0.176 , lm loss 0.000 , avg batch size 4.0
2023-06-29 17:14:18,907 - 0:23:50 - 83.6s - INFO - __main__ - epoch 7/20 done , tot steps 12110 , lr 4.1E-05 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-06-29 17:16:18,663 - 0:25:49 - 119.8s - INFO - __main__ - progress 7.578 , lr 3.9E-05 , loss 0.177 , qa loss 0.177 , lm loss 0.000 , avg batch size 4.0
2023-06-29 17:17:41,809 - 0:27:13 - 83.1s - INFO - __main__ - epoch 8/20 done , tot steps 13840 , lr 3.8E-05 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-06-29 17:19:39,766 - 0:29:10 - 118.0s - INFO - __main__ - progress 8.578 , lr 3.6E-05 , loss 0.164 , qa loss 0.164 , lm loss 0.000 , avg batch size 4.0
2023-06-29 17:21:03,019 - 0:30:34 - 83.3s - INFO - __main__ - epoch 9/20 done , tot steps 15570 , lr 3.4E-05 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
2023-06-29 17:23:03,201 - 0:32:34 - 120.2s - INFO - __main__ - progress 9.578 , lr 3.3E-05 , loss 0.168 , qa loss 0.168 , lm loss 0.000 , avg batch size 4.0
2023-06-29 17:24:24,770 - 0:33:55 - 81.6s - INFO - __main__ - epoch 10/20 done , tot steps 17300 , lr 3.1E-05 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
2023-06-29 17:26:22,938 - 0:35:54 - 118.2s - INFO - __main__ - progress 10.578 , lr 2.9E-05 , loss 0.170 , qa loss 0.170 , lm loss 0.000 , avg batch size 4.0
2023-06-29 17:27:46,150 - 0:37:17 - 83.2s - INFO - __main__ - epoch 11/20 done , tot steps 19030 , lr 2.8E-05 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-06-29 17:29:44,888 - 0:39:16 - 118.7s - INFO - __main__ - progress 11.578 , lr 2.6E-05 , loss 0.155 , qa loss 0.155 , lm loss 0.000 , avg batch size 4.0
2023-06-29 17:31:07,400 - 0:40:38 - 82.5s - INFO - __main__ - epoch 12/20 done , tot steps 20760 , lr 2.5E-05 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-06-29 17:33:06,418 - 0:42:37 - 119.0s - INFO - __main__ - progress 12.578 , lr 2.3E-05 , loss 0.159 , qa loss 0.159 , lm loss 0.000 , avg batch size 4.0
2023-06-29 17:34:34,920 - 0:44:06 - 88.5s - INFO - __main__ - epoch 13/20 done , tot steps 22490 , lr 2.2E-05 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-06-29 17:36:34,391 - 0:46:05 - 119.5s - INFO - __main__ - progress 13.578 , lr 2.0E-05 , loss 0.155 , qa loss 0.155 , lm loss 0.000 , avg batch size 4.0
2023-06-29 17:38:00,459 - 0:47:31 - 86.1s - INFO - __main__ - epoch 14/20 done , tot steps 24220 , lr 1.9E-05 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-06-29 17:39:55,705 - 0:49:26 - 115.2s - INFO - __main__ - progress 14.578 , lr 1.7E-05 , loss 0.148 , qa loss 0.148 , lm loss 0.000 , avg batch size 4.0
2023-06-29 17:41:23,823 - 0:50:55 - 88.1s - INFO - __main__ - epoch 15/20 done , tot steps 25950 , lr 1.6E-05 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-06-29 17:43:19,140 - 0:52:50 - 115.3s - INFO - __main__ - progress 15.578 , lr 1.4E-05 , loss 0.144 , qa loss 0.144 , lm loss 0.000 , avg batch size 4.0
2023-06-29 17:44:46,027 - 0:54:17 - 86.9s - INFO - __main__ - epoch 16/20 done , tot steps 27680 , lr 1.3E-05 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-06-29 17:46:41,425 - 0:56:12 - 115.4s - INFO - __main__ - progress 16.578 , lr 1.1E-05 , loss 0.133 , qa loss 0.133 , lm loss 0.000 , avg batch size 4.0
2023-06-29 17:48:08,063 - 0:57:39 - 86.6s - INFO - __main__ - epoch 17/20 done , tot steps 29410 , lr 9.4E-06 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
2023-06-29 17:50:03,213 - 0:59:34 - 115.2s - INFO - __main__ - progress 17.578 , lr 7.6E-06 , loss 0.131 , qa loss 0.131 , lm loss 0.000 , avg batch size 4.0
2023-06-29 17:51:29,949 - 1:01:01 - 86.7s - INFO - __main__ - epoch 18/20 done , tot steps 31140 , lr 6.3E-06 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-06-29 17:53:24,942 - 1:02:56 - 115.0s - INFO - __main__ - progress 18.578 , lr 4.5E-06 , loss 0.130 , qa loss 0.130 , lm loss 0.000 , avg batch size 4.0
2023-06-29 17:54:52,913 - 1:04:24 - 88.0s - INFO - __main__ - epoch 19/20 done , tot steps 32870 , lr 3.1E-06 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
2023-06-29 17:56:49,591 - 1:06:20 - 116.7s - INFO - __main__ - progress 19.578 , lr 1.3E-06 , loss 0.131 , qa loss 0.131 , lm loss 0.000 , avg batch size 4.0
2023-06-29 17:58:20,501 - 1:07:51 - 90.9s - INFO - __main__ - epoch 20/20 done , tot steps 34600 , lr 1.5E-08 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-06-29 17:58:29,405 - 1:08:00 - 8.9s - INFO - __main__ - start to train { task: ['srl'], seq train type: lll }
2023-06-29 17:58:31,804 - 1:08:03 - 2.4s - INFO - utils - writing extra data in ../../model_ppmm1/gpt2/lll/sst_srl_woz.en_0.0/sst/lm.csv ...
2023-06-29 17:58:31,854 - 1:08:03 - 0.0s - INFO - __main__ - extra training data size: 0
2023-06-29 17:58:32,252 - 1:08:03 - 0.4s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[sst]
The task with which model is saved sst
2023-06-29 17:58:35,919 - 1:08:07 - 3.7s - INFO - __main__ - len of train dataset: 6414 , max train batch size 4 , num of opt steps: 128280
2023-06-29 18:02:24,730 - 1:11:55 - 228.8s - INFO - __main__ - progress 0.624 , lr 6.1E-05 , loss 3.324 , qa loss 3.324 , lm loss 0.000 , avg batch size 4.0
2023-06-29 18:04:57,499 - 1:14:28 - 152.8s - INFO - __main__ - epoch 1/20 done , tot steps 1604 , lr 5.9E-05 , loss 2.53 , qa loss 2.53 , lm loss 0.00 , avg batch size 4.0
2023-06-29 18:09:12,559 - 1:18:43 - 255.1s - INFO - __main__ - progress 1.624 , lr 5.7E-05 , loss 1.032 , qa loss 1.032 , lm loss 0.000 , avg batch size 4.0
2023-06-29 18:11:51,767 - 1:21:22 - 159.2s - INFO - __main__ - epoch 2/20 done , tot steps 3208 , lr 5.6E-05 , loss 1.00 , qa loss 1.00 , lm loss 0.00 , avg batch size 4.0
2023-06-29 18:16:04,144 - 1:25:35 - 252.4s - INFO - __main__ - progress 2.624 , lr 5.4E-05 , loss 0.887 , qa loss 0.887 , lm loss 0.000 , avg batch size 4.0
2023-06-29 18:18:39,382 - 1:28:10 - 155.2s - INFO - __main__ - epoch 3/20 done , tot steps 4812 , lr 5.3E-05 , loss 0.88 , qa loss 0.88 , lm loss 0.00 , avg batch size 4.0
2023-06-29 18:22:49,990 - 1:32:21 - 250.6s - INFO - __main__ - progress 3.624 , lr 5.1E-05 , loss 0.783 , qa loss 0.783 , lm loss 0.000 , avg batch size 4.0
2023-06-29 18:25:31,086 - 1:35:02 - 161.1s - INFO - __main__ - epoch 4/20 done , tot steps 6416 , lr 5.0E-05 , loss 0.79 , qa loss 0.79 , lm loss 0.00 , avg batch size 4.0
2023-06-29 18:29:44,849 - 1:39:16 - 253.8s - INFO - __main__ - progress 4.624 , lr 4.8E-05 , loss 0.745 , qa loss 0.745 , lm loss 0.000 , avg batch size 4.0
2023-06-29 18:32:19,650 - 1:41:50 - 154.8s - INFO - __main__ - epoch 5/20 done , tot steps 8020 , lr 4.7E-05 , loss 0.75 , qa loss 0.75 , lm loss 0.00 , avg batch size 4.0
2023-06-29 18:36:31,830 - 1:46:03 - 252.2s - INFO - __main__ - progress 5.624 , lr 4.5E-05 , loss 0.705 , qa loss 0.705 , lm loss 0.000 , avg batch size 4.0
2023-06-29 18:39:09,815 - 1:48:41 - 158.0s - INFO - __main__ - epoch 6/20 done , tot steps 9624 , lr 4.4E-05 , loss 0.71 , qa loss 0.71 , lm loss 0.00 , avg batch size 4.0
2023-06-29 18:43:19,087 - 1:52:50 - 249.3s - INFO - __main__ - progress 6.624 , lr 4.2E-05 , loss 0.675 , qa loss 0.675 , lm loss 0.000 , avg batch size 4.0
2023-06-29 18:45:53,354 - 1:55:24 - 154.3s - INFO - __main__ - epoch 7/20 done , tot steps 11228 , lr 4.1E-05 , loss 0.68 , qa loss 0.68 , lm loss 0.00 , avg batch size 4.0
2023-06-29 18:49:57,119 - 1:59:28 - 243.8s - INFO - __main__ - progress 7.624 , lr 3.9E-05 , loss 0.652 , qa loss 0.652 , lm loss 0.000 , avg batch size 4.0
2023-06-29 18:52:42,083 - 2:02:13 - 165.0s - INFO - __main__ - epoch 8/20 done , tot steps 12832 , lr 3.8E-05 , loss 0.65 , qa loss 0.65 , lm loss 0.00 , avg batch size 4.0
2023-06-29 18:56:59,375 - 2:06:30 - 257.3s - INFO - __main__ - progress 8.624 , lr 3.6E-05 , loss 0.630 , qa loss 0.630 , lm loss 0.000 , avg batch size 4.0
2023-06-29 18:59:38,488 - 2:09:09 - 159.1s - INFO - __main__ - epoch 9/20 done , tot steps 14436 , lr 3.4E-05 , loss 0.62 , qa loss 0.62 , lm loss 0.00 , avg batch size 4.0
2023-06-29 19:03:53,440 - 2:13:24 - 255.0s - INFO - __main__ - progress 9.624 , lr 3.2E-05 , loss 0.617 , qa loss 0.617 , lm loss 0.000 , avg batch size 4.0
2023-06-29 19:06:28,206 - 2:15:59 - 154.8s - INFO - __main__ - epoch 10/20 done , tot steps 16040 , lr 3.1E-05 , loss 0.61 , qa loss 0.61 , lm loss 0.00 , avg batch size 4.0
2023-06-29 19:10:38,609 - 2:20:09 - 250.4s - INFO - __main__ - progress 10.624 , lr 2.9E-05 , loss 0.596 , qa loss 0.596 , lm loss 0.000 , avg batch size 4.0
2023-06-29 19:13:19,229 - 2:22:50 - 160.6s - INFO - __main__ - epoch 11/20 done , tot steps 17644 , lr 2.8E-05 , loss 0.59 , qa loss 0.59 , lm loss 0.00 , avg batch size 4.0
2023-06-29 19:17:32,966 - 2:27:04 - 253.7s - INFO - __main__ - progress 11.624 , lr 2.6E-05 , loss 0.564 , qa loss 0.564 , lm loss 0.000 , avg batch size 4.0
2023-06-29 19:20:09,002 - 2:29:40 - 156.0s - INFO - __main__ - epoch 12/20 done , tot steps 19248 , lr 2.5E-05 , loss 0.58 , qa loss 0.58 , lm loss 0.00 , avg batch size 4.0
2023-06-29 19:24:23,189 - 2:33:54 - 254.2s - INFO - __main__ - progress 12.624 , lr 2.3E-05 , loss 0.557 , qa loss 0.557 , lm loss 0.000 , avg batch size 4.0
2023-06-29 19:26:58,225 - 2:36:29 - 155.0s - INFO - __main__ - epoch 13/20 done , tot steps 20852 , lr 2.2E-05 , loss 0.56 , qa loss 0.56 , lm loss 0.00 , avg batch size 4.0
2023-06-29 19:31:07,932 - 2:40:39 - 249.7s - INFO - __main__ - progress 13.624 , lr 2.0E-05 , loss 0.549 , qa loss 0.549 , lm loss 0.000 , avg batch size 4.0
2023-06-29 19:33:45,343 - 2:43:16 - 157.4s - INFO - __main__ - epoch 14/20 done , tot steps 22456 , lr 1.9E-05 , loss 0.54 , qa loss 0.54 , lm loss 0.00 , avg batch size 4.0
2023-06-29 19:37:57,549 - 2:47:28 - 252.2s - INFO - __main__ - progress 14.624 , lr 1.7E-05 , loss 0.523 , qa loss 0.523 , lm loss 0.000 , avg batch size 4.0
2023-06-29 19:40:29,263 - 2:50:00 - 151.7s - INFO - __main__ - epoch 15/20 done , tot steps 24060 , lr 1.6E-05 , loss 0.53 , qa loss 0.53 , lm loss 0.00 , avg batch size 4.0
2023-06-29 19:44:36,020 - 2:54:07 - 246.8s - INFO - __main__ - progress 15.624 , lr 1.4E-05 , loss 0.515 , qa loss 0.515 , lm loss 0.000 , avg batch size 4.0
2023-06-29 19:47:20,103 - 2:56:51 - 164.1s - INFO - __main__ - epoch 16/20 done , tot steps 25664 , lr 1.3E-05 , loss 0.51 , qa loss 0.51 , lm loss 0.00 , avg batch size 4.0
2023-06-29 19:51:32,527 - 3:01:03 - 252.4s - INFO - __main__ - progress 16.624 , lr 1.1E-05 , loss 0.515 , qa loss 0.515 , lm loss 0.000 , avg batch size 4.0
2023-06-29 19:54:15,603 - 3:03:46 - 163.1s - INFO - __main__ - epoch 17/20 done , tot steps 27268 , lr 9.4E-06 , loss 0.51 , qa loss 0.51 , lm loss 0.00 , avg batch size 4.0
2023-06-29 19:58:27,399 - 3:07:58 - 251.8s - INFO - __main__ - progress 17.624 , lr 7.4E-06 , loss 0.498 , qa loss 0.498 , lm loss 0.000 , avg batch size 4.0
2023-06-29 20:01:05,389 - 3:10:36 - 158.0s - INFO - __main__ - epoch 18/20 done , tot steps 28872 , lr 6.3E-06 , loss 0.50 , qa loss 0.50 , lm loss 0.00 , avg batch size 4.0
2023-06-29 20:05:15,473 - 3:14:46 - 250.1s - INFO - __main__ - progress 18.624 , lr 4.3E-06 , loss 0.490 , qa loss 0.490 , lm loss 0.000 , avg batch size 4.0
2023-06-29 20:07:52,256 - 3:17:23 - 156.8s - INFO - __main__ - epoch 19/20 done , tot steps 30476 , lr 3.1E-06 , loss 0.49 , qa loss 0.49 , lm loss 0.00 , avg batch size 4.0
2023-06-29 20:12:06,780 - 3:21:37 - 254.5s - INFO - __main__ - progress 19.624 , lr 1.2E-06 , loss 0.485 , qa loss 0.485 , lm loss 0.000 , avg batch size 4.0
2023-06-29 20:14:49,703 - 3:24:20 - 162.9s - INFO - __main__ - epoch 20/20 done , tot steps 32080 , lr 1.6E-08 , loss 0.48 , qa loss 0.48 , lm loss 0.00 , avg batch size 4.0
2023-06-29 20:15:00,235 - 3:24:31 - 10.5s - INFO - __main__ - start to train { task: ['woz.en'], seq train type: lll }
2023-06-29 20:15:05,195 - 3:24:36 - 5.0s - INFO - utils - writing extra data in ../../model_ppmm1/gpt2/lll/sst_srl_woz.en_0.0/srl/lm.csv ...
2023-06-29 20:15:05,326 - 3:24:36 - 0.1s - INFO - __main__ - extra training data size: 0
2023-06-29 20:15:06,043 - 3:24:37 - 0.7s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[srl]
The task with which model is saved srl
2023-06-29 20:15:11,500 - 3:24:42 - 5.5s - INFO - __main__ - len of train dataset: 2536 , max train batch size 4 , num of opt steps: 50720
2023-06-29 20:16:34,946 - 3:26:06 - 83.4s - INFO - __main__ - epoch 1/20 done , tot steps 634 , lr 5.9E-05 , loss 3.22 , qa loss 3.22 , lm loss 0.00 , avg batch size 4.0
2023-06-29 20:17:28,546 - 3:26:59 - 53.6s - INFO - __main__ - epoch 2/20 done , tot steps 1268 , lr 5.6E-05 , loss 0.76 , qa loss 0.76 , lm loss 0.00 , avg batch size 4.0
2023-06-29 20:18:39,465 - 3:28:10 - 70.9s - INFO - __main__ - epoch 3/20 done , tot steps 1902 , lr 5.3E-05 , loss 0.57 , qa loss 0.57 , lm loss 0.00 , avg batch size 4.0
2023-06-29 20:19:46,253 - 3:29:17 - 66.8s - INFO - __main__ - epoch 4/20 done , tot steps 2536 , lr 5.0E-05 , loss 0.46 , qa loss 0.46 , lm loss 0.00 , avg batch size 4.0
2023-06-29 20:20:56,509 - 3:30:27 - 70.3s - INFO - __main__ - epoch 5/20 done , tot steps 3170 , lr 4.7E-05 , loss 0.37 , qa loss 0.37 , lm loss 0.00 , avg batch size 4.0
2023-06-29 20:22:04,890 - 3:31:36 - 68.4s - INFO - __main__ - epoch 6/20 done , tot steps 3804 , lr 4.4E-05 , loss 0.33 , qa loss 0.33 , lm loss 0.00 , avg batch size 4.0
2023-06-29 20:23:12,849 - 3:32:44 - 68.0s - INFO - __main__ - epoch 7/20 done , tot steps 4438 , lr 4.1E-05 , loss 0.30 , qa loss 0.30 , lm loss 0.00 , avg batch size 4.0
2023-06-29 20:24:19,775 - 3:33:50 - 66.9s - INFO - __main__ - epoch 8/20 done , tot steps 5072 , lr 3.8E-05 , loss 0.27 , qa loss 0.27 , lm loss 0.00 , avg batch size 4.0
2023-06-29 20:25:25,375 - 3:34:56 - 65.6s - INFO - __main__ - epoch 9/20 done , tot steps 5706 , lr 3.4E-05 , loss 0.25 , qa loss 0.25 , lm loss 0.00 , avg batch size 4.0
2023-06-29 20:26:31,807 - 3:36:03 - 66.4s - INFO - __main__ - epoch 10/20 done , tot steps 6340 , lr 3.1E-05 , loss 0.24 , qa loss 0.24 , lm loss 0.00 , avg batch size 4.0
2023-06-29 20:27:36,791 - 3:37:07 - 65.0s - INFO - __main__ - epoch 11/20 done , tot steps 6974 , lr 2.8E-05 , loss 0.22 , qa loss 0.22 , lm loss 0.00 , avg batch size 4.0
2023-06-29 20:28:42,124 - 3:38:13 - 65.3s - INFO - __main__ - epoch 12/20 done , tot steps 7608 , lr 2.5E-05 , loss 0.21 , qa loss 0.21 , lm loss 0.00 , avg batch size 4.0
2023-06-29 20:29:47,878 - 3:39:19 - 65.8s - INFO - __main__ - epoch 13/20 done , tot steps 8242 , lr 2.2E-05 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-06-29 20:30:52,324 - 3:40:23 - 64.4s - INFO - __main__ - epoch 14/20 done , tot steps 8876 , lr 1.9E-05 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-06-29 20:31:57,255 - 3:41:28 - 64.9s - INFO - __main__ - epoch 15/20 done , tot steps 9510 , lr 1.6E-05 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-06-29 20:33:02,234 - 3:42:33 - 65.0s - INFO - __main__ - epoch 16/20 done , tot steps 10144 , lr 1.3E-05 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-06-29 20:34:07,367 - 3:43:38 - 65.1s - INFO - __main__ - epoch 17/20 done , tot steps 10778 , lr 9.4E-06 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-06-29 20:35:12,023 - 3:44:43 - 64.7s - INFO - __main__ - epoch 18/20 done , tot steps 11412 , lr 6.3E-06 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
2023-06-29 20:36:16,658 - 3:45:47 - 64.6s - INFO - __main__ - epoch 19/20 done , tot steps 12046 , lr 3.1E-06 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-06-29 20:37:28,216 - 3:46:59 - 71.6s - INFO - __main__ - epoch 20/20 done , tot steps 12680 , lr 1.5E-08 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
The current active adapter is Stack[woz_en]
The task with which model is saved woz_en
Wall Execution time: 03:47:03
CPU Execution time: 03:46:42
