Not all gpus support fp16 training! Will use fp32 instead.
2023-07-02 12:35:36,685 - 0:00:04 - 0.0s - INFO - __main__ - args = Namespace(REG_TYPE_KEYS=['mas', 'ewc'], adam_epsilon=0.0001, add_task_tokens=False, data_dir='../../data', debug=False, decay_style='linear', device_ids=[0], dynamic_epochs=False, fp32=True, gen_lm_sample_percentage=0.0, learning_rate=6.25e-05, lm_lambda=0.0, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[40537.0], min_batch_size=4, min_n_steps=1500, model_dir_root='../../model_cm120/gpt2/lll/sst_srl_woz.en_0.0', model_name='gpt2', n_gpus=1, n_train_epochs={'sst': 20, 'srl': 20, 'woz.en': 20}, n_warmup_ratio=0.005, n_workers=75, qp_margin=0.5, real_sample=False, reg_lambda=1.0, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[14187], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[14187], unbound=0, use_sep=False, weight_decay=0.01)
2023-07-02 12:35:36,685 - 0:00:04 - 0.0s - INFO - __main__ - start to train { task: ['sst'], seq train type: lll }
2023-07-02 12:35:36,693 - 0:00:04 - 0.0s - INFO - __main__ - extra training data size: 0
2023-07-02 12:35:39,111 - 0:00:06 - 2.4s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
2023-07-02 12:35:40,978 - 0:00:08 - 1.9s - INFO - __main__ - len of train dataset: 6920 , max train batch size 4 , num of opt steps: 138400
/home/student/2021/cs21mtech11006/anaconda3/envs/lamol/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
2023-07-02 12:38:20,450 - 0:02:47 - 159.5s - INFO - __main__ - progress 0.578 , lr 6.1E-05 , loss 2.262 , qa loss 2.262 , lm loss 0.000 , avg batch size 4.0
2023-07-02 12:40:16,567 - 0:04:44 - 116.1s - INFO - __main__ - epoch 1/20 done , tot steps 1730 , lr 5.9E-05 , loss 1.44 , qa loss 1.44 , lm loss 0.00 , avg batch size 4.0
2023-07-02 12:42:54,318 - 0:07:21 - 157.8s - INFO - __main__ - progress 1.578 , lr 5.8E-05 , loss 0.280 , qa loss 0.280 , lm loss 0.000 , avg batch size 4.0
2023-07-02 12:44:36,593 - 0:09:04 - 102.3s - INFO - __main__ - epoch 2/20 done , tot steps 3460 , lr 5.6E-05 , loss 0.27 , qa loss 0.27 , lm loss 0.00 , avg batch size 4.0
2023-07-02 12:46:00,028 - 0:10:27 - 83.4s - INFO - __main__ - progress 2.578 , lr 5.4E-05 , loss 0.236 , qa loss 0.236 , lm loss 0.000 , avg batch size 4.0
2023-07-02 12:47:04,485 - 0:11:32 - 64.5s - INFO - __main__ - epoch 3/20 done , tot steps 5190 , lr 5.3E-05 , loss 0.23 , qa loss 0.23 , lm loss 0.00 , avg batch size 4.0
2023-07-02 12:48:30,225 - 0:12:57 - 85.7s - INFO - __main__ - progress 3.578 , lr 5.1E-05 , loss 0.208 , qa loss 0.208 , lm loss 0.000 , avg batch size 4.0
2023-07-02 12:49:33,246 - 0:14:00 - 63.0s - INFO - __main__ - epoch 4/20 done , tot steps 6920 , lr 5.0E-05 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-07-02 12:51:25,394 - 0:15:52 - 112.1s - INFO - __main__ - progress 4.578 , lr 4.8E-05 , loss 0.188 , qa loss 0.188 , lm loss 0.000 , avg batch size 4.0
2023-07-02 12:53:21,441 - 0:17:48 - 116.0s - INFO - __main__ - epoch 5/20 done , tot steps 8650 , lr 4.7E-05 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-07-02 12:55:58,461 - 0:20:26 - 157.0s - INFO - __main__ - progress 5.578 , lr 4.5E-05 , loss 0.169 , qa loss 0.169 , lm loss 0.000 , avg batch size 4.0
2023-07-02 12:57:58,347 - 0:22:25 - 119.9s - INFO - __main__ - epoch 6/20 done , tot steps 10380 , lr 4.4E-05 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
2023-07-02 13:00:34,587 - 0:25:02 - 156.2s - INFO - __main__ - progress 6.578 , lr 4.2E-05 , loss 0.182 , qa loss 0.182 , lm loss 0.000 , avg batch size 4.0
2023-07-02 13:02:30,711 - 0:26:58 - 116.1s - INFO - __main__ - epoch 7/20 done , tot steps 12110 , lr 4.1E-05 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
2023-07-02 13:05:06,616 - 0:29:34 - 155.9s - INFO - __main__ - progress 7.578 , lr 3.9E-05 , loss 0.146 , qa loss 0.146 , lm loss 0.000 , avg batch size 4.0
2023-07-02 13:07:04,522 - 0:31:32 - 117.9s - INFO - __main__ - epoch 8/20 done , tot steps 13840 , lr 3.8E-05 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-07-02 13:09:21,646 - 0:33:49 - 137.1s - INFO - __main__ - progress 8.578 , lr 3.6E-05 , loss 0.143 , qa loss 0.143 , lm loss 0.000 , avg batch size 4.0
2023-07-02 13:10:24,284 - 0:34:51 - 62.6s - INFO - __main__ - epoch 9/20 done , tot steps 15570 , lr 3.4E-05 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
2023-07-02 13:11:48,698 - 0:36:16 - 84.4s - INFO - __main__ - progress 9.578 , lr 3.3E-05 , loss 0.128 , qa loss 0.128 , lm loss 0.000 , avg batch size 4.0
2023-07-02 13:12:51,679 - 0:37:19 - 63.0s - INFO - __main__ - epoch 10/20 done , tot steps 17300 , lr 3.1E-05 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-07-02 13:14:17,892 - 0:38:45 - 86.2s - INFO - __main__ - progress 10.578 , lr 2.9E-05 , loss 0.114 , qa loss 0.114 , lm loss 0.000 , avg batch size 4.0
2023-07-02 13:15:31,507 - 0:39:59 - 73.6s - INFO - __main__ - epoch 11/20 done , tot steps 19030 , lr 2.8E-05 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2023-07-02 13:18:07,219 - 0:42:34 - 155.7s - INFO - __main__ - progress 11.578 , lr 2.6E-05 , loss 0.121 , qa loss 0.121 , lm loss 0.000 , avg batch size 4.0
2023-07-02 13:20:03,423 - 0:44:30 - 116.2s - INFO - __main__ - epoch 12/20 done , tot steps 20760 , lr 2.5E-05 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2023-07-02 13:22:40,615 - 0:47:08 - 157.2s - INFO - __main__ - progress 12.578 , lr 2.3E-05 , loss 0.120 , qa loss 0.120 , lm loss 0.000 , avg batch size 4.0
2023-07-02 13:24:36,708 - 0:49:04 - 116.1s - INFO - __main__ - epoch 13/20 done , tot steps 22490 , lr 2.2E-05 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-07-02 13:27:14,962 - 0:51:42 - 158.3s - INFO - __main__ - progress 13.578 , lr 2.0E-05 , loss 0.108 , qa loss 0.108 , lm loss 0.000 , avg batch size 4.0
2023-07-02 13:29:10,673 - 0:53:38 - 115.7s - INFO - __main__ - epoch 14/20 done , tot steps 24220 , lr 1.9E-05 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-07-02 13:31:47,756 - 0:56:15 - 157.1s - INFO - __main__ - progress 14.578 , lr 1.7E-05 , loss 0.101 , qa loss 0.101 , lm loss 0.000 , avg batch size 4.0
2023-07-02 13:33:43,412 - 0:58:10 - 115.7s - INFO - __main__ - epoch 15/20 done , tot steps 25950 , lr 1.6E-05 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 4.0
2023-07-02 13:35:06,671 - 0:59:34 - 83.3s - INFO - __main__ - progress 15.578 , lr 1.4E-05 , loss 0.101 , qa loss 0.101 , lm loss 0.000 , avg batch size 4.0
2023-07-02 13:36:09,549 - 1:00:37 - 62.9s - INFO - __main__ - epoch 16/20 done , tot steps 27680 , lr 1.3E-05 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 4.0
2023-07-02 13:37:35,220 - 1:02:02 - 85.7s - INFO - __main__ - progress 16.578 , lr 1.1E-05 , loss 0.092 , qa loss 0.092 , lm loss 0.000 , avg batch size 4.0
2023-07-02 13:38:39,595 - 1:03:07 - 64.4s - INFO - __main__ - epoch 17/20 done , tot steps 29410 , lr 9.4E-06 , loss 0.09 , qa loss 0.09 , lm loss 0.00 , avg batch size 4.0
2023-07-02 13:40:14,786 - 1:04:42 - 95.2s - INFO - __main__ - progress 17.578 , lr 7.6E-06 , loss 0.086 , qa loss 0.086 , lm loss 0.000 , avg batch size 4.0
2023-07-02 13:42:10,271 - 1:06:37 - 115.5s - INFO - __main__ - epoch 18/20 done , tot steps 31140 , lr 6.3E-06 , loss 0.09 , qa loss 0.09 , lm loss 0.00 , avg batch size 4.0
2023-07-02 13:44:49,593 - 1:09:17 - 159.3s - INFO - __main__ - progress 18.578 , lr 4.5E-06 , loss 0.096 , qa loss 0.096 , lm loss 0.000 , avg batch size 4.0
2023-07-02 13:46:43,626 - 1:11:11 - 114.0s - INFO - __main__ - epoch 19/20 done , tot steps 32870 , lr 3.1E-06 , loss 0.09 , qa loss 0.09 , lm loss 0.00 , avg batch size 4.0
2023-07-02 13:49:20,294 - 1:13:47 - 156.7s - INFO - __main__ - progress 19.578 , lr 1.3E-06 , loss 0.080 , qa loss 0.080 , lm loss 0.000 , avg batch size 4.0
2023-07-02 13:51:22,142 - 1:15:49 - 121.8s - INFO - __main__ - epoch 20/20 done , tot steps 34600 , lr 1.5E-08 , loss 0.08 , qa loss 0.08 , lm loss 0.00 , avg batch size 4.0
2023-07-02 13:51:31,244 - 1:15:58 - 9.1s - INFO - __main__ - start to train { task: ['srl'], seq train type: lll }
2023-07-02 13:51:33,634 - 1:16:01 - 2.4s - INFO - utils - writing extra data in ../../model_cm120/gpt2/lll/sst_srl_woz.en_0.0/sst/lm.csv ...
2023-07-02 13:51:33,662 - 1:16:01 - 0.0s - INFO - __main__ - extra training data size: 0
2023-07-02 13:51:33,991 - 1:16:01 - 0.3s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[sst]
The task with which model is saved sst
2023-07-02 13:51:36,185 - 1:16:03 - 2.2s - INFO - __main__ - len of train dataset: 6414 , max train batch size 4 , num of opt steps: 128280
2023-07-02 13:56:29,301 - 1:20:56 - 293.1s - INFO - __main__ - progress 0.624 , lr 6.1E-05 , loss 3.775 , qa loss 3.775 , lm loss 0.000 , avg batch size 4.0
2023-07-02 13:59:15,693 - 1:23:43 - 166.4s - INFO - __main__ - epoch 1/20 done , tot steps 1604 , lr 5.9E-05 , loss 2.79 , qa loss 2.79 , lm loss 0.00 , avg batch size 4.0
2023-07-02 14:01:54,319 - 1:26:21 - 158.6s - INFO - __main__ - progress 1.624 , lr 5.7E-05 , loss 1.014 , qa loss 1.014 , lm loss 0.000 , avg batch size 4.0
2023-07-02 14:03:34,833 - 1:28:02 - 100.5s - INFO - __main__ - epoch 2/20 done , tot steps 3208 , lr 5.6E-05 , loss 0.97 , qa loss 0.97 , lm loss 0.00 , avg batch size 4.0
2023-07-02 14:07:06,829 - 1:31:34 - 212.0s - INFO - __main__ - progress 2.624 , lr 5.4E-05 , loss 0.814 , qa loss 0.814 , lm loss 0.000 , avg batch size 4.0
2023-07-02 14:09:55,858 - 1:34:23 - 169.0s - INFO - __main__ - epoch 3/20 done , tot steps 4812 , lr 5.3E-05 , loss 0.81 , qa loss 0.81 , lm loss 0.00 , avg batch size 4.0
2023-07-02 14:14:42,975 - 1:39:10 - 287.1s - INFO - __main__ - progress 3.624 , lr 5.1E-05 , loss 0.721 , qa loss 0.721 , lm loss 0.000 , avg batch size 4.0
2023-07-02 14:17:33,646 - 1:42:01 - 170.7s - INFO - __main__ - epoch 4/20 done , tot steps 6416 , lr 5.0E-05 , loss 0.72 , qa loss 0.72 , lm loss 0.00 , avg batch size 4.0
2023-07-02 14:22:20,543 - 1:46:48 - 286.9s - INFO - __main__ - progress 4.624 , lr 4.8E-05 , loss 0.668 , qa loss 0.668 , lm loss 0.000 , avg batch size 4.0
2023-07-02 14:25:15,760 - 1:49:43 - 175.2s - INFO - __main__ - epoch 5/20 done , tot steps 8020 , lr 4.7E-05 , loss 0.66 , qa loss 0.66 , lm loss 0.00 , avg batch size 4.0
2023-07-02 14:28:15,292 - 1:52:42 - 179.5s - INFO - __main__ - progress 5.624 , lr 4.5E-05 , loss 0.631 , qa loss 0.631 , lm loss 0.000 , avg batch size 4.0
2023-07-02 14:29:55,692 - 1:54:23 - 100.4s - INFO - __main__ - epoch 6/20 done , tot steps 9624 , lr 4.4E-05 , loss 0.62 , qa loss 0.62 , lm loss 0.00 , avg batch size 4.0
2023-07-02 14:33:03,373 - 1:57:30 - 187.7s - INFO - __main__ - progress 6.624 , lr 4.2E-05 , loss 0.592 , qa loss 0.592 , lm loss 0.000 , avg batch size 4.0
2023-07-02 14:35:55,666 - 2:00:23 - 172.3s - INFO - __main__ - epoch 7/20 done , tot steps 11228 , lr 4.1E-05 , loss 0.59 , qa loss 0.59 , lm loss 0.00 , avg batch size 4.0
2023-07-02 14:40:47,951 - 2:05:15 - 292.3s - INFO - __main__ - progress 7.624 , lr 3.9E-05 , loss 0.535 , qa loss 0.535 , lm loss 0.000 , avg batch size 4.0
2023-07-02 14:43:40,677 - 2:08:08 - 172.7s - INFO - __main__ - epoch 8/20 done , tot steps 12832 , lr 3.8E-05 , loss 0.54 , qa loss 0.54 , lm loss 0.00 , avg batch size 4.0
2023-07-02 14:48:32,339 - 2:12:59 - 291.7s - INFO - __main__ - progress 8.624 , lr 3.6E-05 , loss 0.526 , qa loss 0.526 , lm loss 0.000 , avg batch size 4.0
2023-07-02 14:51:23,556 - 2:15:51 - 171.2s - INFO - __main__ - epoch 9/20 done , tot steps 14436 , lr 3.4E-05 , loss 0.52 , qa loss 0.52 , lm loss 0.00 , avg batch size 4.0
2023-07-02 14:54:44,444 - 2:19:11 - 200.9s - INFO - __main__ - progress 9.624 , lr 3.2E-05 , loss 0.488 , qa loss 0.488 , lm loss 0.000 , avg batch size 4.0
2023-07-02 14:56:19,531 - 2:20:47 - 95.1s - INFO - __main__ - epoch 10/20 done , tot steps 16040 , lr 3.1E-05 , loss 0.49 , qa loss 0.49 , lm loss 0.00 , avg batch size 4.0
2023-07-02 14:58:58,236 - 2:23:25 - 158.7s - INFO - __main__ - progress 10.624 , lr 2.9E-05 , loss 0.489 , qa loss 0.489 , lm loss 0.000 , avg batch size 4.0
2023-07-02 15:00:29,027 - 2:24:56 - 90.8s - INFO - __main__ - epoch 11/20 done , tot steps 17644 , lr 2.8E-05 , loss 0.48 , qa loss 0.48 , lm loss 0.00 , avg batch size 4.0
2023-07-02 15:02:45,756 - 2:27:13 - 136.7s - INFO - __main__ - progress 11.624 , lr 2.6E-05 , loss 0.453 , qa loss 0.453 , lm loss 0.000 , avg batch size 4.0
2023-07-02 15:05:13,055 - 2:29:40 - 147.3s - INFO - __main__ - epoch 12/20 done , tot steps 19248 , lr 2.5E-05 , loss 0.46 , qa loss 0.46 , lm loss 0.00 , avg batch size 4.0
2023-07-02 15:10:16,599 - 2:34:44 - 303.5s - INFO - __main__ - progress 12.624 , lr 2.3E-05 , loss 0.441 , qa loss 0.441 , lm loss 0.000 , avg batch size 4.0
2023-07-02 15:13:12,789 - 2:37:40 - 176.2s - INFO - __main__ - epoch 13/20 done , tot steps 20852 , lr 2.2E-05 , loss 0.44 , qa loss 0.44 , lm loss 0.00 , avg batch size 4.0
2023-07-02 15:18:16,879 - 2:42:44 - 304.1s - INFO - __main__ - progress 13.624 , lr 2.0E-05 , loss 0.433 , qa loss 0.433 , lm loss 0.000 , avg batch size 4.0
2023-07-02 15:21:13,743 - 2:45:41 - 176.9s - INFO - __main__ - epoch 14/20 done , tot steps 22456 , lr 1.9E-05 , loss 0.43 , qa loss 0.43 , lm loss 0.00 , avg batch size 4.0
2023-07-02 15:25:34,914 - 2:50:02 - 261.2s - INFO - __main__ - progress 14.624 , lr 1.7E-05 , loss 0.412 , qa loss 0.412 , lm loss 0.000 , avg batch size 4.0
2023-07-02 15:27:34,830 - 2:52:02 - 119.9s - INFO - __main__ - epoch 15/20 done , tot steps 24060 , lr 1.6E-05 , loss 0.41 , qa loss 0.41 , lm loss 0.00 , avg batch size 4.0
2023-07-02 15:30:24,439 - 2:54:51 - 169.6s - INFO - __main__ - progress 15.624 , lr 1.4E-05 , loss 0.398 , qa loss 0.398 , lm loss 0.000 , avg batch size 4.0
2023-07-02 15:32:01,822 - 2:56:29 - 97.4s - INFO - __main__ - epoch 16/20 done , tot steps 25664 , lr 1.3E-05 , loss 0.40 , qa loss 0.40 , lm loss 0.00 , avg batch size 4.0
2023-07-02 15:35:20,220 - 2:59:47 - 198.4s - INFO - __main__ - progress 16.624 , lr 1.1E-05 , loss 0.387 , qa loss 0.387 , lm loss 0.000 , avg batch size 4.0
2023-07-02 15:38:14,582 - 3:02:42 - 174.4s - INFO - __main__ - epoch 17/20 done , tot steps 27268 , lr 9.4E-06 , loss 0.39 , qa loss 0.39 , lm loss 0.00 , avg batch size 4.0
2023-07-02 15:42:55,514 - 3:07:23 - 280.9s - INFO - __main__ - progress 17.624 , lr 7.4E-06 , loss 0.394 , qa loss 0.394 , lm loss 0.000 , avg batch size 4.0
2023-07-02 15:45:53,805 - 3:10:21 - 178.3s - INFO - __main__ - epoch 18/20 done , tot steps 28872 , lr 6.3E-06 , loss 0.38 , qa loss 0.38 , lm loss 0.00 , avg batch size 4.0
2023-07-02 15:50:46,233 - 3:15:13 - 292.4s - INFO - __main__ - progress 18.624 , lr 4.3E-06 , loss 0.375 , qa loss 0.375 , lm loss 0.000 , avg batch size 4.0
2023-07-02 15:53:39,776 - 3:18:07 - 173.5s - INFO - __main__ - epoch 19/20 done , tot steps 30476 , lr 3.1E-06 , loss 0.37 , qa loss 0.37 , lm loss 0.00 , avg batch size 4.0
2023-07-02 15:57:49,000 - 3:22:16 - 249.2s - INFO - __main__ - progress 19.624 , lr 1.2E-06 , loss 0.376 , qa loss 0.376 , lm loss 0.000 , avg batch size 4.0
2023-07-02 15:59:48,615 - 3:24:16 - 119.6s - INFO - __main__ - epoch 20/20 done , tot steps 32080 , lr 1.6E-08 , loss 0.37 , qa loss 0.37 , lm loss 0.00 , avg batch size 4.0
2023-07-02 15:59:57,848 - 3:24:25 - 9.2s - INFO - __main__ - start to train { task: ['woz.en'], seq train type: lll }
2023-07-02 16:00:02,527 - 3:24:30 - 4.7s - INFO - utils - writing extra data in ../../model_cm120/gpt2/lll/sst_srl_woz.en_0.0/srl/lm.csv ...
2023-07-02 16:00:02,590 - 3:24:30 - 0.1s - INFO - __main__ - extra training data size: 0
2023-07-02 16:00:02,968 - 3:24:30 - 0.4s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[srl]
The task with which model is saved srl
2023-07-02 16:00:06,161 - 3:24:33 - 3.2s - INFO - __main__ - len of train dataset: 2536 , max train batch size 4 , num of opt steps: 50720
2023-07-02 16:01:01,245 - 3:25:28 - 55.1s - INFO - __main__ - epoch 1/20 done , tot steps 634 , lr 5.9E-05 , loss 3.56 , qa loss 3.56 , lm loss 0.00 , avg batch size 4.0
2023-07-02 16:01:55,519 - 3:26:23 - 54.3s - INFO - __main__ - epoch 2/20 done , tot steps 1268 , lr 5.6E-05 , loss 0.66 , qa loss 0.66 , lm loss 0.00 , avg batch size 4.0
2023-07-02 16:02:48,980 - 3:27:16 - 53.5s - INFO - __main__ - epoch 3/20 done , tot steps 1902 , lr 5.3E-05 , loss 0.46 , qa loss 0.46 , lm loss 0.00 , avg batch size 4.0
2023-07-02 16:03:41,311 - 3:28:08 - 52.3s - INFO - __main__ - epoch 4/20 done , tot steps 2536 , lr 5.0E-05 , loss 0.37 , qa loss 0.37 , lm loss 0.00 , avg batch size 4.0
2023-07-02 16:05:00,649 - 3:29:28 - 79.3s - INFO - __main__ - epoch 5/20 done , tot steps 3170 , lr 4.7E-05 , loss 0.33 , qa loss 0.33 , lm loss 0.00 , avg batch size 4.0
2023-07-02 16:06:34,188 - 3:31:01 - 93.5s - INFO - __main__ - epoch 6/20 done , tot steps 3804 , lr 4.4E-05 , loss 0.28 , qa loss 0.28 , lm loss 0.00 , avg batch size 4.0
2023-07-02 16:08:08,365 - 3:32:35 - 94.2s - INFO - __main__ - epoch 7/20 done , tot steps 4438 , lr 4.1E-05 , loss 0.25 , qa loss 0.25 , lm loss 0.00 , avg batch size 4.0
2023-07-02 16:09:41,800 - 3:34:09 - 93.4s - INFO - __main__ - epoch 8/20 done , tot steps 5072 , lr 3.8E-05 , loss 0.23 , qa loss 0.23 , lm loss 0.00 , avg batch size 4.0
2023-07-02 16:11:16,073 - 3:35:43 - 94.3s - INFO - __main__ - epoch 9/20 done , tot steps 5706 , lr 3.4E-05 , loss 0.21 , qa loss 0.21 , lm loss 0.00 , avg batch size 4.0
2023-07-02 16:12:50,054 - 3:37:17 - 94.0s - INFO - __main__ - epoch 10/20 done , tot steps 6340 , lr 3.1E-05 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-07-02 16:14:24,572 - 3:38:52 - 94.5s - INFO - __main__ - epoch 11/20 done , tot steps 6974 , lr 2.8E-05 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-07-02 16:15:59,765 - 3:40:27 - 95.2s - INFO - __main__ - epoch 12/20 done , tot steps 7608 , lr 2.5E-05 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-07-02 16:17:33,787 - 3:42:01 - 94.0s - INFO - __main__ - epoch 13/20 done , tot steps 8242 , lr 2.2E-05 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
2023-07-02 16:19:09,044 - 3:43:36 - 95.3s - INFO - __main__ - epoch 14/20 done , tot steps 8876 , lr 1.9E-05 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-07-02 16:20:43,087 - 3:45:10 - 94.0s - INFO - __main__ - epoch 15/20 done , tot steps 9510 , lr 1.6E-05 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-07-02 16:22:05,530 - 3:46:33 - 82.4s - INFO - __main__ - epoch 16/20 done , tot steps 10144 , lr 1.3E-05 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-07-02 16:23:16,575 - 3:47:44 - 71.0s - INFO - __main__ - epoch 17/20 done , tot steps 10778 , lr 9.4E-06 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-07-02 16:24:27,682 - 3:48:55 - 71.1s - INFO - __main__ - epoch 18/20 done , tot steps 11412 , lr 6.3E-06 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
2023-07-02 16:25:30,947 - 3:49:58 - 63.3s - INFO - __main__ - epoch 19/20 done , tot steps 12046 , lr 3.1E-06 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
2023-07-02 16:26:36,228 - 3:51:03 - 65.3s - INFO - __main__ - epoch 20/20 done , tot steps 12680 , lr 1.5E-08 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
The current active adapter is Stack[woz_en]
The task with which model is saved woz_en
Wall Execution time: 03:51:08
CPU Execution time: 03:51:54
