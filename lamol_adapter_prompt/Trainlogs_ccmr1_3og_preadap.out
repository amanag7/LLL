Not all gpus support fp16 training! Will use fp32 instead.
2023-06-29 23:13:14,274 - 0:00:06 - 0.0s - INFO - __main__ - args = Namespace(REG_TYPE_KEYS=['mas', 'ewc'], adam_epsilon=0.0001, add_task_tokens=False, data_dir='../../data', debug=False, decay_style='linear', device_ids=[0, 1], dynamic_epochs=False, fp32=True, gen_lm_sample_percentage=0.0, learning_rate=6.25e-05, lm_lambda=0.0, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[38915.52, 42158.48], min_batch_size=4, min_n_steps=1500, model_dir_root='../../model_ccmr1/gpt2/lll/sst_srl_woz.en_0.0', model_name='gpt2', n_gpus=2, n_train_epochs={'sst': 20, 'srl': 20, 'woz.en': 20}, n_warmup_ratio=0.005, n_workers=75, qp_margin=0.5, real_sample=False, reg_lambda=1.0, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[13620, 14755], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[13620, 14755], unbound=0, use_sep=False, weight_decay=0.01)
2023-06-29 23:13:14,274 - 0:00:06 - 0.0s - INFO - __main__ - start to train { task: ['sst'], seq train type: lll }
2023-06-29 23:13:14,289 - 0:00:06 - 0.0s - INFO - __main__ - extra training data size: 0
2023-06-29 23:13:17,700 - 0:00:09 - 3.4s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
2023-06-29 23:13:20,599 - 0:00:12 - 2.9s - INFO - __main__ - len of train dataset: 6920 , max train batch size 4 , num of opt steps: 138400
/home/student/2021/cs21mtech11006/anaconda3/envs/lamol/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/home/student/2021/cs21mtech11006/anaconda3/envs/lamol/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
2023-06-29 23:15:11,605 - 0:02:03 - 111.0s - INFO - __main__ - progress 0.578 , lr 6.1E-05 , loss 1.610 , qa loss 1.610 , lm loss 0.000 , avg batch size 4.0
2023-06-29 23:16:55,424 - 0:03:47 - 103.8s - INFO - __main__ - epoch 1/20 done , tot steps 1730 , lr 5.9E-05 , loss 1.07 , qa loss 1.07 , lm loss 0.00 , avg batch size 4.0
2023-06-29 23:19:16,432 - 0:06:08 - 141.0s - INFO - __main__ - progress 1.578 , lr 5.8E-05 , loss 0.259 , qa loss 0.259 , lm loss 0.000 , avg batch size 4.0
2023-06-29 23:21:00,467 - 0:07:52 - 104.0s - INFO - __main__ - epoch 2/20 done , tot steps 3460 , lr 5.6E-05 , loss 0.25 , qa loss 0.25 , lm loss 0.00 , avg batch size 4.0
2023-06-29 23:23:21,367 - 0:10:13 - 140.9s - INFO - __main__ - progress 2.578 , lr 5.4E-05 , loss 0.214 , qa loss 0.214 , lm loss 0.000 , avg batch size 4.0
2023-06-29 23:25:04,774 - 0:11:56 - 103.4s - INFO - __main__ - epoch 3/20 done , tot steps 5190 , lr 5.3E-05 , loss 0.22 , qa loss 0.22 , lm loss 0.00 , avg batch size 4.0
2023-06-29 23:27:26,700 - 0:14:18 - 141.9s - INFO - __main__ - progress 3.578 , lr 5.1E-05 , loss 0.195 , qa loss 0.195 , lm loss 0.000 , avg batch size 4.0
2023-06-29 23:29:10,928 - 0:16:02 - 104.2s - INFO - __main__ - epoch 4/20 done , tot steps 6920 , lr 5.0E-05 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-06-29 23:31:31,226 - 0:18:23 - 140.3s - INFO - __main__ - progress 4.578 , lr 4.8E-05 , loss 0.162 , qa loss 0.162 , lm loss 0.000 , avg batch size 4.0
2023-06-29 23:33:15,610 - 0:20:07 - 104.4s - INFO - __main__ - epoch 5/20 done , tot steps 8650 , lr 4.7E-05 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-06-29 23:35:38,737 - 0:22:30 - 143.1s - INFO - __main__ - progress 5.578 , lr 4.5E-05 , loss 0.144 , qa loss 0.144 , lm loss 0.000 , avg batch size 4.0
2023-06-29 23:37:23,234 - 0:24:15 - 104.5s - INFO - __main__ - epoch 6/20 done , tot steps 10380 , lr 4.4E-05 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-06-29 23:39:44,030 - 0:26:35 - 140.8s - INFO - __main__ - progress 6.578 , lr 4.2E-05 , loss 0.135 , qa loss 0.135 , lm loss 0.000 , avg batch size 4.0
2023-06-29 23:41:28,184 - 0:28:20 - 104.2s - INFO - __main__ - epoch 7/20 done , tot steps 12110 , lr 4.1E-05 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
2023-06-29 23:43:49,011 - 0:30:40 - 140.8s - INFO - __main__ - progress 7.578 , lr 3.9E-05 , loss 0.121 , qa loss 0.121 , lm loss 0.000 , avg batch size 4.0
2023-06-29 23:45:32,379 - 0:32:24 - 103.4s - INFO - __main__ - epoch 8/20 done , tot steps 13840 , lr 3.8E-05 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2023-06-29 23:47:52,611 - 0:34:44 - 140.2s - INFO - __main__ - progress 8.578 , lr 3.6E-05 , loss 0.128 , qa loss 0.128 , lm loss 0.000 , avg batch size 4.0
2023-06-29 23:49:37,364 - 0:36:29 - 104.8s - INFO - __main__ - epoch 9/20 done , tot steps 15570 , lr 3.4E-05 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2023-06-29 23:51:58,752 - 0:38:50 - 141.4s - INFO - __main__ - progress 9.578 , lr 3.3E-05 , loss 0.098 , qa loss 0.098 , lm loss 0.000 , avg batch size 4.0
2023-06-29 23:53:42,896 - 0:40:34 - 104.1s - INFO - __main__ - epoch 10/20 done , tot steps 17300 , lr 3.1E-05 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 4.0
2023-06-29 23:56:03,725 - 0:42:55 - 140.8s - INFO - __main__ - progress 10.578 , lr 2.9E-05 , loss 0.097 , qa loss 0.097 , lm loss 0.000 , avg batch size 4.0
2023-06-29 23:57:48,410 - 0:44:40 - 104.7s - INFO - __main__ - epoch 11/20 done , tot steps 19030 , lr 2.8E-05 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 4.0
2023-06-30 00:00:09,698 - 0:47:01 - 141.3s - INFO - __main__ - progress 11.578 , lr 2.6E-05 , loss 0.087 , qa loss 0.087 , lm loss 0.000 , avg batch size 4.0
2023-06-30 00:01:53,463 - 0:48:45 - 103.8s - INFO - __main__ - epoch 12/20 done , tot steps 20760 , lr 2.5E-05 , loss 0.09 , qa loss 0.09 , lm loss 0.00 , avg batch size 4.0
2023-06-30 00:04:13,701 - 0:51:05 - 140.2s - INFO - __main__ - progress 12.578 , lr 2.3E-05 , loss 0.081 , qa loss 0.081 , lm loss 0.000 , avg batch size 4.0
2023-06-30 00:05:57,646 - 0:52:49 - 103.9s - INFO - __main__ - epoch 13/20 done , tot steps 22490 , lr 2.2E-05 , loss 0.08 , qa loss 0.08 , lm loss 0.00 , avg batch size 4.0
2023-06-30 00:08:17,557 - 0:55:09 - 139.9s - INFO - __main__ - progress 13.578 , lr 2.0E-05 , loss 0.067 , qa loss 0.067 , lm loss 0.000 , avg batch size 4.0
2023-06-30 00:10:02,489 - 0:56:54 - 104.9s - INFO - __main__ - epoch 14/20 done , tot steps 24220 , lr 1.9E-05 , loss 0.08 , qa loss 0.08 , lm loss 0.00 , avg batch size 4.0
2023-06-30 00:12:22,350 - 0:59:14 - 139.9s - INFO - __main__ - progress 14.578 , lr 1.7E-05 , loss 0.067 , qa loss 0.067 , lm loss 0.000 , avg batch size 4.0
2023-06-30 00:14:07,681 - 1:00:59 - 105.3s - INFO - __main__ - epoch 15/20 done , tot steps 25950 , lr 1.6E-05 , loss 0.07 , qa loss 0.07 , lm loss 0.00 , avg batch size 4.0
2023-06-30 00:16:27,581 - 1:03:19 - 139.9s - INFO - __main__ - progress 15.578 , lr 1.4E-05 , loss 0.063 , qa loss 0.063 , lm loss 0.000 , avg batch size 4.0
2023-06-30 00:18:12,435 - 1:05:04 - 104.9s - INFO - __main__ - epoch 16/20 done , tot steps 27680 , lr 1.3E-05 , loss 0.06 , qa loss 0.06 , lm loss 0.00 , avg batch size 4.0
2023-06-30 00:20:32,478 - 1:07:24 - 140.0s - INFO - __main__ - progress 16.578 , lr 1.1E-05 , loss 0.068 , qa loss 0.068 , lm loss 0.000 , avg batch size 4.0
2023-06-30 00:22:17,609 - 1:09:09 - 105.1s - INFO - __main__ - epoch 17/20 done , tot steps 29410 , lr 9.4E-06 , loss 0.07 , qa loss 0.07 , lm loss 0.00 , avg batch size 4.0
2023-06-30 00:24:38,591 - 1:11:30 - 141.0s - INFO - __main__ - progress 17.578 , lr 7.6E-06 , loss 0.064 , qa loss 0.064 , lm loss 0.000 , avg batch size 4.0
2023-06-30 00:26:23,548 - 1:13:15 - 105.0s - INFO - __main__ - epoch 18/20 done , tot steps 31140 , lr 6.3E-06 , loss 0.06 , qa loss 0.06 , lm loss 0.00 , avg batch size 4.0
2023-06-30 00:28:42,510 - 1:15:34 - 139.0s - INFO - __main__ - progress 18.578 , lr 4.5E-06 , loss 0.061 , qa loss 0.061 , lm loss 0.000 , avg batch size 4.0
2023-06-30 00:30:27,392 - 1:17:19 - 104.9s - INFO - __main__ - epoch 19/20 done , tot steps 32870 , lr 3.1E-06 , loss 0.06 , qa loss 0.06 , lm loss 0.00 , avg batch size 4.0
2023-06-30 00:32:46,788 - 1:19:38 - 139.4s - INFO - __main__ - progress 19.578 , lr 1.3E-06 , loss 0.056 , qa loss 0.056 , lm loss 0.000 , avg batch size 4.0
2023-06-30 00:34:40,061 - 1:21:31 - 113.3s - INFO - __main__ - epoch 20/20 done , tot steps 34600 , lr 1.5E-08 , loss 0.05 , qa loss 0.05 , lm loss 0.00 , avg batch size 4.0
2023-06-30 00:34:49,958 - 1:21:41 - 9.9s - INFO - __main__ - start to train { task: ['srl'], seq train type: lll }
2023-06-30 00:34:54,418 - 1:21:46 - 4.5s - INFO - utils - writing extra data in ../../model_ccmr1/gpt2/lll/sst_srl_woz.en_0.0/sst/lm.csv ...
2023-06-30 00:34:54,453 - 1:21:46 - 0.0s - INFO - __main__ - extra training data size: 0
2023-06-30 00:34:54,737 - 1:21:46 - 0.3s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[sst]
The task with which model is saved sst
2023-06-30 00:34:58,844 - 1:21:50 - 4.1s - INFO - __main__ - len of train dataset: 6414 , max train batch size 4 , num of opt steps: 128280
2023-06-30 00:39:12,982 - 1:26:04 - 254.1s - INFO - __main__ - progress 0.624 , lr 6.1E-05 , loss 2.993 , qa loss 2.993 , lm loss 0.000 , avg batch size 4.0
2023-06-30 00:42:06,124 - 1:28:57 - 173.1s - INFO - __main__ - epoch 1/20 done , tot steps 1604 , lr 5.9E-05 , loss 2.27 , qa loss 2.27 , lm loss 0.00 , avg batch size 4.0
2023-06-30 00:46:49,013 - 1:33:40 - 282.9s - INFO - __main__ - progress 1.624 , lr 5.7E-05 , loss 0.879 , qa loss 0.879 , lm loss 0.000 , avg batch size 4.0
2023-06-30 00:49:35,225 - 1:36:27 - 166.2s - INFO - __main__ - epoch 2/20 done , tot steps 3208 , lr 5.6E-05 , loss 0.84 , qa loss 0.84 , lm loss 0.00 , avg batch size 4.0
2023-06-30 00:54:14,050 - 1:41:05 - 278.8s - INFO - __main__ - progress 2.624 , lr 5.4E-05 , loss 0.717 , qa loss 0.717 , lm loss 0.000 , avg batch size 4.0
2023-06-30 00:57:00,853 - 1:43:52 - 166.8s - INFO - __main__ - epoch 3/20 done , tot steps 4812 , lr 5.3E-05 , loss 0.69 , qa loss 0.69 , lm loss 0.00 , avg batch size 4.0
2023-06-30 01:01:32,597 - 1:48:24 - 271.7s - INFO - __main__ - progress 3.624 , lr 5.1E-05 , loss 0.634 , qa loss 0.634 , lm loss 0.000 , avg batch size 4.0
2023-06-30 01:04:22,708 - 1:51:14 - 170.1s - INFO - __main__ - epoch 4/20 done , tot steps 6416 , lr 5.0E-05 , loss 0.63 , qa loss 0.63 , lm loss 0.00 , avg batch size 4.0
2023-06-30 01:09:02,524 - 1:55:54 - 279.8s - INFO - __main__ - progress 4.624 , lr 4.8E-05 , loss 0.571 , qa loss 0.571 , lm loss 0.000 , avg batch size 4.0
2023-06-30 01:11:48,971 - 1:58:40 - 166.4s - INFO - __main__ - epoch 5/20 done , tot steps 8020 , lr 4.7E-05 , loss 0.57 , qa loss 0.57 , lm loss 0.00 , avg batch size 4.0
2023-06-30 01:16:24,749 - 2:03:16 - 275.8s - INFO - __main__ - progress 5.624 , lr 4.5E-05 , loss 0.519 , qa loss 0.519 , lm loss 0.000 , avg batch size 4.0
2023-06-30 01:19:17,342 - 2:06:09 - 172.6s - INFO - __main__ - epoch 6/20 done , tot steps 9624 , lr 4.4E-05 , loss 0.51 , qa loss 0.51 , lm loss 0.00 , avg batch size 4.0
2023-06-30 01:23:53,821 - 2:10:45 - 276.5s - INFO - __main__ - progress 6.624 , lr 4.2E-05 , loss 0.470 , qa loss 0.470 , lm loss 0.000 , avg batch size 4.0
2023-06-30 01:26:45,349 - 2:13:37 - 171.5s - INFO - __main__ - epoch 7/20 done , tot steps 11228 , lr 4.1E-05 , loss 0.48 , qa loss 0.48 , lm loss 0.00 , avg batch size 4.0
2023-06-30 01:31:24,934 - 2:18:16 - 279.6s - INFO - __main__ - progress 7.624 , lr 3.9E-05 , loss 0.446 , qa loss 0.446 , lm loss 0.000 , avg batch size 4.0
2023-06-30 01:34:09,252 - 2:21:01 - 164.3s - INFO - __main__ - epoch 8/20 done , tot steps 12832 , lr 3.8E-05 , loss 0.45 , qa loss 0.45 , lm loss 0.00 , avg batch size 4.0
2023-06-30 01:38:39,221 - 2:25:31 - 270.0s - INFO - __main__ - progress 8.624 , lr 3.6E-05 , loss 0.429 , qa loss 0.429 , lm loss 0.000 , avg batch size 4.0
2023-06-30 01:41:34,327 - 2:28:26 - 175.1s - INFO - __main__ - epoch 9/20 done , tot steps 14436 , lr 3.4E-05 , loss 0.42 , qa loss 0.42 , lm loss 0.00 , avg batch size 4.0
2023-06-30 01:46:08,449 - 2:33:00 - 274.1s - INFO - __main__ - progress 9.624 , lr 3.2E-05 , loss 0.390 , qa loss 0.390 , lm loss 0.000 , avg batch size 4.0
2023-06-30 01:48:56,108 - 2:35:47 - 167.7s - INFO - __main__ - epoch 10/20 done , tot steps 16040 , lr 3.1E-05 , loss 0.39 , qa loss 0.39 , lm loss 0.00 , avg batch size 4.0
2023-06-30 01:53:31,007 - 2:40:22 - 274.9s - INFO - __main__ - progress 10.624 , lr 2.9E-05 , loss 0.388 , qa loss 0.388 , lm loss 0.000 , avg batch size 4.0
2023-06-30 01:56:20,318 - 2:43:12 - 169.3s - INFO - __main__ - epoch 11/20 done , tot steps 17644 , lr 2.8E-05 , loss 0.38 , qa loss 0.38 , lm loss 0.00 , avg batch size 4.0
2023-06-30 02:00:56,132 - 2:47:48 - 275.8s - INFO - __main__ - progress 11.624 , lr 2.6E-05 , loss 0.353 , qa loss 0.353 , lm loss 0.000 , avg batch size 4.0
2023-06-30 02:03:44,069 - 2:50:35 - 167.9s - INFO - __main__ - epoch 12/20 done , tot steps 19248 , lr 2.5E-05 , loss 0.35 , qa loss 0.35 , lm loss 0.00 , avg batch size 4.0
2023-06-30 02:08:17,988 - 2:55:09 - 273.9s - INFO - __main__ - progress 12.624 , lr 2.3E-05 , loss 0.344 , qa loss 0.344 , lm loss 0.000 , avg batch size 4.0
2023-06-30 02:11:10,795 - 2:58:02 - 172.8s - INFO - __main__ - epoch 13/20 done , tot steps 20852 , lr 2.2E-05 , loss 0.34 , qa loss 0.34 , lm loss 0.00 , avg batch size 4.0
2023-06-30 02:15:50,188 - 3:02:42 - 279.4s - INFO - __main__ - progress 13.624 , lr 2.0E-05 , loss 0.327 , qa loss 0.327 , lm loss 0.000 , avg batch size 4.0
2023-06-30 02:18:41,096 - 3:05:32 - 170.9s - INFO - __main__ - epoch 14/20 done , tot steps 22456 , lr 1.9E-05 , loss 0.33 , qa loss 0.33 , lm loss 0.00 , avg batch size 4.0
2023-06-30 02:23:21,348 - 3:10:13 - 280.3s - INFO - __main__ - progress 14.624 , lr 1.7E-05 , loss 0.307 , qa loss 0.307 , lm loss 0.000 , avg batch size 4.0
2023-06-30 02:26:06,488 - 3:12:58 - 165.1s - INFO - __main__ - epoch 15/20 done , tot steps 24060 , lr 1.6E-05 , loss 0.31 , qa loss 0.31 , lm loss 0.00 , avg batch size 4.0
2023-06-30 02:30:39,343 - 3:17:31 - 272.9s - INFO - __main__ - progress 15.624 , lr 1.4E-05 , loss 0.306 , qa loss 0.306 , lm loss 0.000 , avg batch size 4.0
2023-06-30 02:33:29,419 - 3:20:21 - 170.1s - INFO - __main__ - epoch 16/20 done , tot steps 25664 , lr 1.3E-05 , loss 0.31 , qa loss 0.31 , lm loss 0.00 , avg batch size 4.0
2023-06-30 02:38:08,568 - 3:25:00 - 279.1s - INFO - __main__ - progress 16.624 , lr 1.1E-05 , loss 0.281 , qa loss 0.281 , lm loss 0.000 , avg batch size 4.0
2023-06-30 02:40:56,673 - 3:27:48 - 168.1s - INFO - __main__ - epoch 17/20 done , tot steps 27268 , lr 9.4E-06 , loss 0.29 , qa loss 0.29 , lm loss 0.00 , avg batch size 4.0
2023-06-30 02:45:30,226 - 3:32:22 - 273.6s - INFO - __main__ - progress 17.624 , lr 7.4E-06 , loss 0.292 , qa loss 0.292 , lm loss 0.000 , avg batch size 4.0
2023-06-30 02:48:23,575 - 3:35:15 - 173.3s - INFO - __main__ - epoch 18/20 done , tot steps 28872 , lr 6.3E-06 , loss 0.29 , qa loss 0.29 , lm loss 0.00 , avg batch size 4.0
2023-06-30 02:52:57,360 - 3:39:49 - 273.8s - INFO - __main__ - progress 18.624 , lr 4.3E-06 , loss 0.285 , qa loss 0.285 , lm loss 0.000 , avg batch size 4.0
2023-06-30 02:55:51,708 - 3:42:43 - 174.3s - INFO - __main__ - epoch 19/20 done , tot steps 30476 , lr 3.1E-06 , loss 0.28 , qa loss 0.28 , lm loss 0.00 , avg batch size 4.0
2023-06-30 03:00:32,167 - 3:47:24 - 280.5s - INFO - __main__ - progress 19.624 , lr 1.2E-06 , loss 0.278 , qa loss 0.278 , lm loss 0.000 , avg batch size 4.0
2023-06-30 03:03:25,521 - 3:50:17 - 173.4s - INFO - __main__ - epoch 20/20 done , tot steps 32080 , lr 1.6E-08 , loss 0.28 , qa loss 0.28 , lm loss 0.00 , avg batch size 4.0
2023-06-30 03:03:34,899 - 3:50:26 - 9.4s - INFO - __main__ - start to train { task: ['woz.en'], seq train type: lll }
2023-06-30 03:03:40,019 - 3:50:31 - 5.1s - INFO - utils - writing extra data in ../../model_ccmr1/gpt2/lll/sst_srl_woz.en_0.0/srl/lm.csv ...
2023-06-30 03:03:40,045 - 3:50:31 - 0.0s - INFO - __main__ - extra training data size: 0
2023-06-30 03:03:40,437 - 3:50:32 - 0.4s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[srl]
The task with which model is saved srl
2023-06-30 03:03:44,659 - 3:50:36 - 4.2s - INFO - __main__ - len of train dataset: 2536 , max train batch size 4 , num of opt steps: 50720
2023-06-30 03:05:01,167 - 3:51:53 - 76.5s - INFO - __main__ - epoch 1/20 done , tot steps 634 , lr 5.9E-05 , loss 3.04 , qa loss 3.04 , lm loss 0.00 , avg batch size 4.0
2023-06-30 03:06:24,792 - 3:53:16 - 83.6s - INFO - __main__ - epoch 2/20 done , tot steps 1268 , lr 5.6E-05 , loss 0.52 , qa loss 0.52 , lm loss 0.00 , avg batch size 4.0
2023-06-30 03:07:47,847 - 3:54:39 - 83.1s - INFO - __main__ - epoch 3/20 done , tot steps 1902 , lr 5.3E-05 , loss 0.37 , qa loss 0.37 , lm loss 0.00 , avg batch size 4.0
2023-06-30 03:09:11,225 - 3:56:03 - 83.4s - INFO - __main__ - epoch 4/20 done , tot steps 2536 , lr 5.0E-05 , loss 0.29 , qa loss 0.29 , lm loss 0.00 , avg batch size 4.0
2023-06-30 03:10:34,325 - 3:57:26 - 83.1s - INFO - __main__ - epoch 5/20 done , tot steps 3170 , lr 4.7E-05 , loss 0.25 , qa loss 0.25 , lm loss 0.00 , avg batch size 4.0
2023-06-30 03:11:56,750 - 3:58:48 - 82.4s - INFO - __main__ - epoch 6/20 done , tot steps 3804 , lr 4.4E-05 , loss 0.23 , qa loss 0.23 , lm loss 0.00 , avg batch size 4.0
2023-06-30 03:13:20,619 - 4:00:12 - 83.9s - INFO - __main__ - epoch 7/20 done , tot steps 4438 , lr 4.1E-05 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-06-30 03:14:43,256 - 4:01:35 - 82.6s - INFO - __main__ - epoch 8/20 done , tot steps 5072 , lr 3.8E-05 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-06-30 03:16:05,866 - 4:02:57 - 82.6s - INFO - __main__ - epoch 9/20 done , tot steps 5706 , lr 3.4E-05 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-06-30 03:17:28,100 - 4:04:19 - 82.2s - INFO - __main__ - epoch 10/20 done , tot steps 6340 , lr 3.1E-05 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
2023-06-30 03:18:50,474 - 4:05:42 - 82.4s - INFO - __main__ - epoch 11/20 done , tot steps 6974 , lr 2.8E-05 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-06-30 03:20:14,488 - 4:07:06 - 84.0s - INFO - __main__ - epoch 12/20 done , tot steps 7608 , lr 2.5E-05 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-06-30 03:21:37,660 - 4:08:29 - 83.2s - INFO - __main__ - epoch 13/20 done , tot steps 8242 , lr 2.2E-05 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-06-30 03:23:01,318 - 4:09:53 - 83.7s - INFO - __main__ - epoch 14/20 done , tot steps 8876 , lr 1.9E-05 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
2023-06-30 03:24:24,881 - 4:11:16 - 83.6s - INFO - __main__ - epoch 15/20 done , tot steps 9510 , lr 1.6E-05 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-06-30 03:25:47,387 - 4:12:39 - 82.5s - INFO - __main__ - epoch 16/20 done , tot steps 10144 , lr 1.3E-05 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-06-30 03:27:10,918 - 4:14:02 - 83.5s - INFO - __main__ - epoch 17/20 done , tot steps 10778 , lr 9.4E-06 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2023-06-30 03:28:33,731 - 4:15:25 - 82.8s - INFO - __main__ - epoch 18/20 done , tot steps 11412 , lr 6.3E-06 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2023-06-30 03:29:57,759 - 4:16:49 - 84.0s - INFO - __main__ - epoch 19/20 done , tot steps 12046 , lr 3.1E-06 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2023-06-30 03:31:30,456 - 4:18:22 - 92.7s - INFO - __main__ - epoch 20/20 done , tot steps 12680 , lr 1.5E-08 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
The current active adapter is Stack[woz_en]
The task with which model is saved woz_en
Wall Execution time: 04:18:30
CPU Execution time: 04:20:50
