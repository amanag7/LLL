Not all gpus support fp16 training! Will use fp32 instead.
2023-06-29 23:16:01,994 - 0:00:05 - 0.0s - INFO - __main__ - args = Namespace(REG_TYPE_KEYS=['mas', 'ewc'], adam_epsilon=0.0001, add_task_tokens=False, data_dir='../../data', debug=False, decay_style='linear', device_ids=[1, 2], dynamic_epochs=False, fp32=True, gen_lm_sample_percentage=0.0, learning_rate=6.25e-05, lm_lambda=0.0, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[38915.52, 42158.48], min_batch_size=4, min_n_steps=1500, model_dir_root='../../model_ccmr3/gpt2/lll/sst_srl_woz.en_0.0', model_name='gpt2', n_gpus=2, n_train_epochs={'sst': 20, 'srl': 20, 'woz.en': 20}, n_warmup_ratio=0.005, n_workers=75, qp_margin=0.5, real_sample=False, reg_lambda=1.0, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[13620, 14755], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[13620, 14755], unbound=0, use_sep=False, weight_decay=0.01)
2023-06-29 23:16:01,994 - 0:00:05 - 0.0s - INFO - __main__ - start to train { task: ['sst'], seq train type: lll }
2023-06-29 23:16:02,011 - 0:00:05 - 0.0s - INFO - __main__ - extra training data size: 0
2023-06-29 23:16:05,624 - 0:00:08 - 3.6s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
2023-06-29 23:16:07,698 - 0:00:10 - 2.1s - INFO - __main__ - len of train dataset: 6920 , max train batch size 4 , num of opt steps: 138400
/home/student/2021/cs21mtech11006/anaconda3/envs/lamol/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/home/student/2021/cs21mtech11006/anaconda3/envs/lamol/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
2023-06-29 23:17:45,665 - 0:01:48 - 98.0s - INFO - __main__ - progress 0.578 , lr 6.1E-05 , loss 2.271 , qa loss 2.271 , lm loss 0.000 , avg batch size 4.0
2023-06-29 23:18:56,472 - 0:02:59 - 70.8s - INFO - __main__ - epoch 1/20 done , tot steps 1730 , lr 5.9E-05 , loss 1.44 , qa loss 1.44 , lm loss 0.00 , avg batch size 4.0
2023-06-29 23:20:33,365 - 0:04:36 - 96.9s - INFO - __main__ - progress 1.578 , lr 5.8E-05 , loss 0.294 , qa loss 0.294 , lm loss 0.000 , avg batch size 4.0
2023-06-29 23:21:44,326 - 0:05:47 - 71.0s - INFO - __main__ - epoch 2/20 done , tot steps 3460 , lr 5.6E-05 , loss 0.29 , qa loss 0.29 , lm loss 0.00 , avg batch size 4.0
2023-06-29 23:23:20,609 - 0:07:23 - 96.3s - INFO - __main__ - progress 2.578 , lr 5.4E-05 , loss 0.241 , qa loss 0.241 , lm loss 0.000 , avg batch size 4.0
2023-06-29 23:24:32,911 - 0:08:36 - 72.3s - INFO - __main__ - epoch 3/20 done , tot steps 5190 , lr 5.3E-05 , loss 0.24 , qa loss 0.24 , lm loss 0.00 , avg batch size 4.0
2023-06-29 23:26:10,356 - 0:10:13 - 97.4s - INFO - __main__ - progress 3.578 , lr 5.1E-05 , loss 0.205 , qa loss 0.205 , lm loss 0.000 , avg batch size 4.0
2023-06-29 23:27:21,955 - 0:11:25 - 71.6s - INFO - __main__ - epoch 4/20 done , tot steps 6920 , lr 5.0E-05 , loss 0.21 , qa loss 0.21 , lm loss 0.00 , avg batch size 4.0
2023-06-29 23:28:59,125 - 0:13:02 - 97.2s - INFO - __main__ - progress 4.578 , lr 4.8E-05 , loss 0.185 , qa loss 0.185 , lm loss 0.000 , avg batch size 4.0
2023-06-29 23:30:08,928 - 0:14:12 - 69.8s - INFO - __main__ - epoch 5/20 done , tot steps 8650 , lr 4.7E-05 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-06-29 23:31:45,354 - 0:15:48 - 96.4s - INFO - __main__ - progress 5.578 , lr 4.5E-05 , loss 0.184 , qa loss 0.184 , lm loss 0.000 , avg batch size 4.0
2023-06-29 23:32:57,450 - 0:17:00 - 72.1s - INFO - __main__ - epoch 6/20 done , tot steps 10380 , lr 4.4E-05 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-06-29 23:34:34,710 - 0:18:37 - 97.3s - INFO - __main__ - progress 6.578 , lr 4.2E-05 , loss 0.173 , qa loss 0.173 , lm loss 0.000 , avg batch size 4.0
2023-06-29 23:35:45,569 - 0:19:48 - 70.9s - INFO - __main__ - epoch 7/20 done , tot steps 12110 , lr 4.1E-05 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
2023-06-29 23:38:28,480 - 0:22:31 - 162.9s - INFO - __main__ - progress 7.578 , lr 3.9E-05 , loss 0.155 , qa loss 0.155 , lm loss 0.000 , avg batch size 4.0
2023-06-29 23:40:46,376 - 0:24:49 - 137.9s - INFO - __main__ - epoch 8/20 done , tot steps 13840 , lr 3.8E-05 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-06-29 23:43:55,142 - 0:27:58 - 188.8s - INFO - __main__ - progress 8.578 , lr 3.6E-05 , loss 0.153 , qa loss 0.153 , lm loss 0.000 , avg batch size 4.0
2023-06-29 23:46:13,815 - 0:30:16 - 138.7s - INFO - __main__ - epoch 9/20 done , tot steps 15570 , lr 3.4E-05 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-06-29 23:49:22,135 - 0:33:25 - 188.3s - INFO - __main__ - progress 9.578 , lr 3.3E-05 , loss 0.146 , qa loss 0.146 , lm loss 0.000 , avg batch size 4.0
2023-06-29 23:51:40,761 - 0:35:43 - 138.6s - INFO - __main__ - epoch 10/20 done , tot steps 17300 , lr 3.1E-05 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-06-29 23:54:49,471 - 0:38:52 - 188.7s - INFO - __main__ - progress 10.578 , lr 2.9E-05 , loss 0.141 , qa loss 0.141 , lm loss 0.000 , avg batch size 4.0
2023-06-29 23:57:08,410 - 0:41:11 - 138.9s - INFO - __main__ - epoch 11/20 done , tot steps 19030 , lr 2.8E-05 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-06-30 00:00:15,990 - 0:44:19 - 187.6s - INFO - __main__ - progress 11.578 , lr 2.6E-05 , loss 0.126 , qa loss 0.126 , lm loss 0.000 , avg batch size 4.0
2023-06-30 00:02:35,167 - 0:46:38 - 139.2s - INFO - __main__ - epoch 12/20 done , tot steps 20760 , lr 2.5E-05 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-06-30 00:05:42,974 - 0:49:46 - 187.8s - INFO - __main__ - progress 12.578 , lr 2.3E-05 , loss 0.129 , qa loss 0.129 , lm loss 0.000 , avg batch size 4.0
2023-06-30 00:07:53,682 - 0:51:56 - 130.7s - INFO - __main__ - epoch 13/20 done , tot steps 22490 , lr 2.2E-05 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-06-30 00:11:03,089 - 0:55:06 - 189.4s - INFO - __main__ - progress 13.578 , lr 2.0E-05 , loss 0.127 , qa loss 0.127 , lm loss 0.000 , avg batch size 4.0
2023-06-30 00:13:21,430 - 0:57:24 - 138.3s - INFO - __main__ - epoch 14/20 done , tot steps 24220 , lr 1.9E-05 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-06-30 00:16:30,370 - 1:00:33 - 188.9s - INFO - __main__ - progress 14.578 , lr 1.7E-05 , loss 0.125 , qa loss 0.125 , lm loss 0.000 , avg batch size 4.0
2023-06-30 00:18:48,802 - 1:02:51 - 138.4s - INFO - __main__ - epoch 15/20 done , tot steps 25950 , lr 1.6E-05 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2023-06-30 00:21:56,374 - 1:05:59 - 187.6s - INFO - __main__ - progress 15.578 , lr 1.4E-05 , loss 0.122 , qa loss 0.122 , lm loss 0.000 , avg batch size 4.0
2023-06-30 00:24:16,725 - 1:08:19 - 140.4s - INFO - __main__ - epoch 16/20 done , tot steps 27680 , lr 1.3E-05 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2023-06-30 00:27:25,238 - 1:11:28 - 188.5s - INFO - __main__ - progress 16.578 , lr 1.1E-05 , loss 0.114 , qa loss 0.114 , lm loss 0.000 , avg batch size 4.0
2023-06-30 00:29:44,491 - 1:13:47 - 139.3s - INFO - __main__ - epoch 17/20 done , tot steps 29410 , lr 9.4E-06 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2023-06-30 00:32:51,927 - 1:16:55 - 187.4s - INFO - __main__ - progress 17.578 , lr 7.6E-06 , loss 0.113 , qa loss 0.113 , lm loss 0.000 , avg batch size 4.0
2023-06-30 00:35:11,606 - 1:19:14 - 139.7s - INFO - __main__ - epoch 18/20 done , tot steps 31140 , lr 6.3E-06 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-06-30 00:38:13,195 - 1:22:16 - 181.6s - INFO - __main__ - progress 18.578 , lr 4.5E-06 , loss 0.103 , qa loss 0.103 , lm loss 0.000 , avg batch size 4.0
2023-06-30 00:40:30,485 - 1:24:33 - 137.3s - INFO - __main__ - epoch 19/20 done , tot steps 32870 , lr 3.1E-06 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-06-30 00:43:39,571 - 1:27:42 - 189.1s - INFO - __main__ - progress 19.578 , lr 1.3E-06 , loss 0.105 , qa loss 0.105 , lm loss 0.000 , avg batch size 4.0
2023-06-30 00:46:05,381 - 1:30:08 - 145.8s - INFO - __main__ - epoch 20/20 done , tot steps 34600 , lr 1.5E-08 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2023-06-30 00:46:15,375 - 1:30:18 - 10.0s - INFO - __main__ - start to train { task: ['srl'], seq train type: lll }
2023-06-30 00:46:18,040 - 1:30:21 - 2.7s - INFO - utils - writing extra data in ../../model_ccmr3/gpt2/lll/sst_srl_woz.en_0.0/sst/lm.csv ...
2023-06-30 00:46:18,074 - 1:30:21 - 0.0s - INFO - __main__ - extra training data size: 0
2023-06-30 00:46:18,387 - 1:30:21 - 0.3s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[sst]
The task with which model is saved sst
2023-06-30 00:46:21,456 - 1:30:24 - 3.1s - INFO - __main__ - len of train dataset: 6414 , max train batch size 4 , num of opt steps: 128280
2023-06-30 00:51:41,453 - 1:35:44 - 320.0s - INFO - __main__ - progress 0.624 , lr 6.1E-05 , loss 3.255 , qa loss 3.255 , lm loss 0.000 , avg batch size 4.0
2023-06-30 00:55:03,315 - 1:39:06 - 201.9s - INFO - __main__ - epoch 1/20 done , tot steps 1604 , lr 5.9E-05 , loss 2.45 , qa loss 2.45 , lm loss 0.00 , avg batch size 4.0
2023-06-30 01:00:25,268 - 1:44:28 - 322.0s - INFO - __main__ - progress 1.624 , lr 5.7E-05 , loss 0.953 , qa loss 0.953 , lm loss 0.000 , avg batch size 4.0
2023-06-30 01:03:41,974 - 1:47:45 - 196.7s - INFO - __main__ - epoch 2/20 done , tot steps 3208 , lr 5.6E-05 , loss 0.91 , qa loss 0.91 , lm loss 0.00 , avg batch size 4.0
2023-06-30 01:09:07,761 - 1:53:10 - 325.8s - INFO - __main__ - progress 2.624 , lr 5.4E-05 , loss 0.784 , qa loss 0.784 , lm loss 0.000 , avg batch size 4.0
2023-06-30 01:12:14,013 - 1:56:17 - 186.3s - INFO - __main__ - epoch 3/20 done , tot steps 4812 , lr 5.3E-05 , loss 0.77 , qa loss 0.77 , lm loss 0.00 , avg batch size 4.0
2023-06-30 01:17:30,801 - 2:01:33 - 316.8s - INFO - __main__ - progress 3.624 , lr 5.1E-05 , loss 0.691 , qa loss 0.691 , lm loss 0.000 , avg batch size 4.0
2023-06-30 01:20:47,090 - 2:04:50 - 196.3s - INFO - __main__ - epoch 4/20 done , tot steps 6416 , lr 5.0E-05 , loss 0.68 , qa loss 0.68 , lm loss 0.00 , avg batch size 4.0
2023-06-30 01:26:04,743 - 2:10:07 - 317.7s - INFO - __main__ - progress 4.624 , lr 4.8E-05 , loss 0.648 , qa loss 0.648 , lm loss 0.000 , avg batch size 4.0
2023-06-30 01:29:27,512 - 2:13:30 - 202.8s - INFO - __main__ - epoch 5/20 done , tot steps 8020 , lr 4.7E-05 , loss 0.64 , qa loss 0.64 , lm loss 0.00 , avg batch size 4.0
2023-06-30 01:34:51,949 - 2:18:55 - 324.4s - INFO - __main__ - progress 5.624 , lr 4.5E-05 , loss 0.606 , qa loss 0.606 , lm loss 0.000 , avg batch size 4.0
2023-06-30 01:38:06,712 - 2:22:09 - 194.8s - INFO - __main__ - epoch 6/20 done , tot steps 9624 , lr 4.4E-05 , loss 0.61 , qa loss 0.61 , lm loss 0.00 , avg batch size 4.0
2023-06-30 01:43:28,565 - 2:27:31 - 321.9s - INFO - __main__ - progress 6.624 , lr 4.2E-05 , loss 0.580 , qa loss 0.580 , lm loss 0.000 , avg batch size 4.0
2023-06-30 01:46:37,827 - 2:30:41 - 189.3s - INFO - __main__ - epoch 7/20 done , tot steps 11228 , lr 4.1E-05 , loss 0.58 , qa loss 0.58 , lm loss 0.00 , avg batch size 4.0
2023-06-30 01:52:08,540 - 2:36:11 - 330.7s - INFO - __main__ - progress 7.624 , lr 3.9E-05 , loss 0.542 , qa loss 0.542 , lm loss 0.000 , avg batch size 4.0
2023-06-30 01:55:26,817 - 2:39:29 - 198.3s - INFO - __main__ - epoch 8/20 done , tot steps 12832 , lr 3.8E-05 , loss 0.54 , qa loss 0.54 , lm loss 0.00 , avg batch size 4.0
2023-06-30 02:00:40,965 - 2:44:44 - 314.1s - INFO - __main__ - progress 8.624 , lr 3.6E-05 , loss 0.526 , qa loss 0.526 , lm loss 0.000 , avg batch size 4.0
2023-06-30 02:04:01,581 - 2:48:04 - 200.6s - INFO - __main__ - epoch 9/20 done , tot steps 14436 , lr 3.4E-05 , loss 0.53 , qa loss 0.53 , lm loss 0.00 , avg batch size 4.0
2023-06-30 02:09:22,375 - 2:53:25 - 320.8s - INFO - __main__ - progress 9.624 , lr 3.2E-05 , loss 0.511 , qa loss 0.511 , lm loss 0.000 , avg batch size 4.0
2023-06-30 02:12:37,192 - 2:56:40 - 194.8s - INFO - __main__ - epoch 10/20 done , tot steps 16040 , lr 3.1E-05 , loss 0.51 , qa loss 0.51 , lm loss 0.00 , avg batch size 4.0
2023-06-30 02:18:06,000 - 3:02:09 - 328.8s - INFO - __main__ - progress 10.624 , lr 2.9E-05 , loss 0.495 , qa loss 0.495 , lm loss 0.000 , avg batch size 4.0
2023-06-30 02:21:08,902 - 3:05:12 - 182.9s - INFO - __main__ - epoch 11/20 done , tot steps 17644 , lr 2.8E-05 , loss 0.49 , qa loss 0.49 , lm loss 0.00 , avg batch size 4.0
2023-06-30 02:26:34,863 - 3:10:38 - 326.0s - INFO - __main__ - progress 11.624 , lr 2.6E-05 , loss 0.468 , qa loss 0.468 , lm loss 0.000 , avg batch size 4.0
2023-06-30 02:29:51,499 - 3:13:54 - 196.6s - INFO - __main__ - epoch 12/20 done , tot steps 19248 , lr 2.5E-05 , loss 0.47 , qa loss 0.47 , lm loss 0.00 , avg batch size 4.0
2023-06-30 02:35:18,708 - 3:19:21 - 327.2s - INFO - __main__ - progress 12.624 , lr 2.3E-05 , loss 0.472 , qa loss 0.472 , lm loss 0.000 , avg batch size 4.0
2023-06-30 02:38:36,044 - 3:22:39 - 197.3s - INFO - __main__ - epoch 13/20 done , tot steps 20852 , lr 2.2E-05 , loss 0.47 , qa loss 0.47 , lm loss 0.00 , avg batch size 4.0
2023-06-30 02:43:58,252 - 3:28:01 - 322.2s - INFO - __main__ - progress 13.624 , lr 2.0E-05 , loss 0.450 , qa loss 0.450 , lm loss 0.000 , avg batch size 4.0
2023-06-30 02:47:20,339 - 3:31:23 - 202.1s - INFO - __main__ - epoch 14/20 done , tot steps 22456 , lr 1.9E-05 , loss 0.44 , qa loss 0.44 , lm loss 0.00 , avg batch size 4.0
2023-06-30 02:52:49,235 - 3:36:52 - 328.9s - INFO - __main__ - progress 14.624 , lr 1.7E-05 , loss 0.426 , qa loss 0.426 , lm loss 0.000 , avg batch size 4.0
2023-06-30 02:55:49,779 - 3:39:52 - 180.5s - INFO - __main__ - epoch 15/20 done , tot steps 24060 , lr 1.6E-05 , loss 0.43 , qa loss 0.43 , lm loss 0.00 , avg batch size 4.0
2023-06-30 03:01:10,584 - 3:45:13 - 320.8s - INFO - __main__ - progress 15.624 , lr 1.4E-05 , loss 0.411 , qa loss 0.411 , lm loss 0.000 , avg batch size 4.0
2023-06-30 03:04:29,230 - 3:48:32 - 198.6s - INFO - __main__ - epoch 16/20 done , tot steps 25664 , lr 1.3E-05 , loss 0.42 , qa loss 0.42 , lm loss 0.00 , avg batch size 4.0
2023-06-30 03:09:50,899 - 3:53:54 - 321.7s - INFO - __main__ - progress 16.624 , lr 1.1E-05 , loss 0.419 , qa loss 0.419 , lm loss 0.000 , avg batch size 4.0
2023-06-30 03:13:13,088 - 3:57:16 - 202.2s - INFO - __main__ - epoch 17/20 done , tot steps 27268 , lr 9.4E-06 , loss 0.42 , qa loss 0.42 , lm loss 0.00 , avg batch size 4.0
2023-06-30 03:18:40,623 - 4:02:43 - 327.5s - INFO - __main__ - progress 17.624 , lr 7.4E-06 , loss 0.412 , qa loss 0.412 , lm loss 0.000 , avg batch size 4.0
2023-06-30 03:21:55,200 - 4:05:58 - 194.6s - INFO - __main__ - epoch 18/20 done , tot steps 28872 , lr 6.3E-06 , loss 0.41 , qa loss 0.41 , lm loss 0.00 , avg batch size 4.0
2023-06-30 03:27:15,587 - 4:11:18 - 320.4s - INFO - __main__ - progress 18.624 , lr 4.3E-06 , loss 0.399 , qa loss 0.399 , lm loss 0.000 , avg batch size 4.0
2023-06-30 03:30:23,454 - 4:14:26 - 187.9s - INFO - __main__ - epoch 19/20 done , tot steps 30476 , lr 3.1E-06 , loss 0.40 , qa loss 0.40 , lm loss 0.00 , avg batch size 4.0
2023-06-30 03:35:45,231 - 4:19:48 - 321.8s - INFO - __main__ - progress 19.624 , lr 1.2E-06 , loss 0.388 , qa loss 0.388 , lm loss 0.000 , avg batch size 4.0
2023-06-30 03:39:12,026 - 4:23:15 - 206.8s - INFO - __main__ - epoch 20/20 done , tot steps 32080 , lr 1.6E-08 , loss 0.40 , qa loss 0.40 , lm loss 0.00 , avg batch size 4.0
2023-06-30 03:39:20,950 - 4:23:24 - 8.9s - INFO - __main__ - start to train { task: ['woz.en'], seq train type: lll }
2023-06-30 03:39:24,283 - 4:23:27 - 3.3s - INFO - utils - writing extra data in ../../model_ccmr3/gpt2/lll/sst_srl_woz.en_0.0/srl/lm.csv ...
2023-06-30 03:39:24,342 - 4:23:27 - 0.1s - INFO - __main__ - extra training data size: 0
2023-06-30 03:39:24,824 - 4:23:28 - 0.5s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[srl]
The task with which model is saved srl
2023-06-30 03:39:28,384 - 4:23:31 - 3.6s - INFO - __main__ - len of train dataset: 2536 , max train batch size 4 , num of opt steps: 50720
2023-06-30 03:41:22,415 - 4:25:25 - 114.0s - INFO - __main__ - epoch 1/20 done , tot steps 634 , lr 5.9E-05 , loss 5.63 , qa loss 5.63 , lm loss 0.00 , avg batch size 4.0
2023-06-30 03:43:15,455 - 4:27:18 - 113.0s - INFO - __main__ - epoch 2/20 done , tot steps 1268 , lr 5.6E-05 , loss 0.89 , qa loss 0.89 , lm loss 0.00 , avg batch size 4.0
2023-06-30 03:45:08,605 - 4:29:11 - 113.2s - INFO - __main__ - epoch 3/20 done , tot steps 1902 , lr 5.3E-05 , loss 0.55 , qa loss 0.55 , lm loss 0.00 , avg batch size 4.0
2023-06-30 03:47:02,132 - 4:31:05 - 113.5s - INFO - __main__ - epoch 4/20 done , tot steps 2536 , lr 5.0E-05 , loss 0.42 , qa loss 0.42 , lm loss 0.00 , avg batch size 4.0
2023-06-30 03:48:55,526 - 4:32:58 - 113.4s - INFO - __main__ - epoch 5/20 done , tot steps 3170 , lr 4.7E-05 , loss 0.36 , qa loss 0.36 , lm loss 0.00 , avg batch size 4.0
2023-06-30 03:50:48,813 - 4:34:51 - 113.3s - INFO - __main__ - epoch 6/20 done , tot steps 3804 , lr 4.4E-05 , loss 0.30 , qa loss 0.30 , lm loss 0.00 , avg batch size 4.0
2023-06-30 03:52:42,653 - 4:36:45 - 113.8s - INFO - __main__ - epoch 7/20 done , tot steps 4438 , lr 4.1E-05 , loss 0.28 , qa loss 0.28 , lm loss 0.00 , avg batch size 4.0
2023-06-30 03:54:36,314 - 4:38:39 - 113.7s - INFO - __main__ - epoch 8/20 done , tot steps 5072 , lr 3.8E-05 , loss 0.26 , qa loss 0.26 , lm loss 0.00 , avg batch size 4.0
2023-06-30 03:56:29,879 - 4:40:33 - 113.6s - INFO - __main__ - epoch 9/20 done , tot steps 5706 , lr 3.4E-05 , loss 0.23 , qa loss 0.23 , lm loss 0.00 , avg batch size 4.0
2023-06-30 03:58:23,167 - 4:42:26 - 113.3s - INFO - __main__ - epoch 10/20 done , tot steps 6340 , lr 3.1E-05 , loss 0.21 , qa loss 0.21 , lm loss 0.00 , avg batch size 4.0
2023-06-30 04:00:16,156 - 4:44:19 - 113.0s - INFO - __main__ - epoch 11/20 done , tot steps 6974 , lr 2.8E-05 , loss 0.21 , qa loss 0.21 , lm loss 0.00 , avg batch size 4.0
2023-06-30 04:02:01,218 - 4:46:04 - 105.1s - INFO - __main__ - epoch 12/20 done , tot steps 7608 , lr 2.5E-05 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-06-30 04:03:53,893 - 4:47:57 - 112.7s - INFO - __main__ - epoch 13/20 done , tot steps 8242 , lr 2.2E-05 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2023-06-30 04:05:46,860 - 4:49:50 - 113.0s - INFO - __main__ - epoch 14/20 done , tot steps 8876 , lr 1.9E-05 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-06-30 04:07:40,157 - 4:51:43 - 113.3s - INFO - __main__ - epoch 15/20 done , tot steps 9510 , lr 1.6E-05 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
2023-06-30 04:09:33,504 - 4:53:36 - 113.3s - INFO - __main__ - epoch 16/20 done , tot steps 10144 , lr 1.3E-05 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-06-30 04:11:26,560 - 4:55:29 - 113.1s - INFO - __main__ - epoch 17/20 done , tot steps 10778 , lr 9.4E-06 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-06-30 04:13:20,244 - 4:57:23 - 113.7s - INFO - __main__ - epoch 18/20 done , tot steps 11412 , lr 6.3E-06 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-06-30 04:15:14,047 - 4:59:17 - 113.8s - INFO - __main__ - epoch 19/20 done , tot steps 12046 , lr 3.1E-06 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-06-30 04:17:13,389 - 5:01:16 - 119.3s - INFO - __main__ - epoch 20/20 done , tot steps 12680 , lr 1.5E-08 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
The current active adapter is Stack[woz_en]
The task with which model is saved woz_en
Wall Execution time: 05:01:19
CPU Execution time: 05:00:27
