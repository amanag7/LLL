nohup: ignoring input
Available number of GPU = 2 < n_gpus = 8
Continue training with 2 GPUs
2022-07-03 20:50:37,120 - 0:00:09 - 0.0s - INFO - __main__ - args = Namespace(REG_TYPE_KEYS=['mas', 'ewc'], adam_epsilon=0.0001, add_task_tokens=False, data_dir='data', debug=False, decay_style='linear', device_ids=[1, 3], dynamic_epochs=False, fp32=True, gen_lm_sample_percentage=0.0, learning_rate=6.25e-05, lm_lambda=0.0, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[31209.6, 33810.4], min_batch_size=4, min_n_steps=1500, model_dir_root='models/gpt2/lll/amazon_sst_srl_woz.en_0.0', model_name='gpt2', n_gpus=2, n_train_epochs={'amazon': 12, 'sst': 12, 'srl': 12, 'woz.en': 12}, n_warmup_ratio=0.005, n_workers=75, qp_margin=0.5, real_sample=False, reg_lambda=1.0, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['amazon', 'sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[10923, 11833], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[10923, 11833], unbound=0, use_sep=False, weight_decay=0.01)
2022-07-03 20:50:37,121 - 0:00:09 - 0.0s - INFO - __main__ - start to train { task: ['amazon'], seq train type: lll }
2022-07-03 20:50:37,121 - 0:00:09 - 0.0s - INFO - __main__ - extra training data size: 0
2022-07-03 20:50:42,122 - 0:00:14 - 5.0s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
2022-07-03 20:51:00,276 - 0:00:32 - 18.2s - INFO - __main__ - len of train dataset: 115000 , max train batch size 76 , num of opt steps: 1380000
/home/ai20mtech14009/.local/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  FutureWarning,
/home/ai20mtech14009/anaconda3/envs/LAMOL/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
2022-07-03 21:07:37,021 - 0:17:09 - 996.7s - INFO - __main__ - progress 0.348 , lr 6.1E-05 , loss 2.088 , qa loss 2.088 , lm loss 0.000 , avg batch size 40.0
2022-07-03 21:23:54,969 - 0:33:27 - 977.9s - INFO - __main__ - progress 0.696 , lr 5.9E-05 , loss 1.270 , qa loss 1.270 , lm loss 0.000 , avg batch size 40.0
2022-07-03 21:38:07,659 - 0:47:40 - 852.7s - INFO - __main__ - epoch 1/12 done , tot steps 2868 , lr 5.7E-05 , loss 1.02 , qa loss 1.02 , lm loss 0.00 , avg batch size 40.1
2022-07-03 21:54:37,382 - 1:04:10 - 989.7s - INFO - __main__ - progress 1.350 , lr 5.5E-05 , loss 0.421 , qa loss 0.421 , lm loss 0.000 , avg batch size 40.3
2022-07-03 22:10:55,682 - 1:20:28 - 978.3s - INFO - __main__ - progress 1.700 , lr 5.4E-05 , loss 0.415 , qa loss 0.415 , lm loss 0.000 , avg batch size 40.3
2022-07-03 22:25:03,500 - 1:34:36 - 847.8s - INFO - __main__ - epoch 2/12 done , tot steps 5731 , lr 5.2E-05 , loss 0.41 , qa loss 0.41 , lm loss 0.00 , avg batch size 40.2
2022-07-03 22:41:33,506 - 1:51:06 - 990.0s - INFO - __main__ - progress 2.349 , lr 5.0E-05 , loss 0.395 , qa loss 0.395 , lm loss 0.000 , avg batch size 40.1
2022-07-03 22:57:48,612 - 2:07:21 - 975.1s - INFO - __main__ - progress 2.698 , lr 4.8E-05 , loss 0.395 , qa loss 0.395 , lm loss 0.000 , avg batch size 40.1
2022-07-03 23:11:59,340 - 2:21:32 - 850.7s - INFO - __main__ - epoch 3/12 done , tot steps 8603 , lr 4.7E-05 , loss 0.39 , qa loss 0.39 , lm loss 0.00 , avg batch size 40.0
2022-07-03 23:28:25,273 - 2:37:57 - 985.9s - INFO - __main__ - progress 3.348 , lr 4.5E-05 , loss 0.384 , qa loss 0.384 , lm loss 0.000 , avg batch size 40.0
2022-07-03 23:44:42,035 - 2:54:14 - 976.8s - INFO - __main__ - progress 3.697 , lr 4.3E-05 , loss 0.383 , qa loss 0.383 , lm loss 0.000 , avg batch size 40.1
2022-07-03 23:58:52,307 - 3:08:24 - 850.3s - INFO - __main__ - epoch 4/12 done , tot steps 11472 , lr 4.2E-05 , loss 0.38 , qa loss 0.38 , lm loss 0.00 , avg batch size 40.1
2022-07-04 00:15:16,998 - 3:24:49 - 984.7s - INFO - __main__ - progress 4.349 , lr 4.0E-05 , loss 0.373 , qa loss 0.373 , lm loss 0.000 , avg batch size 40.1
2022-07-04 00:31:32,953 - 3:41:05 - 976.0s - INFO - __main__ - progress 4.698 , lr 3.8E-05 , loss 0.373 , qa loss 0.373 , lm loss 0.000 , avg batch size 40.1
2022-07-04 00:45:41,300 - 3:55:13 - 848.3s - INFO - __main__ - epoch 5/12 done , tot steps 14339 , lr 3.6E-05 , loss 0.37 , qa loss 0.37 , lm loss 0.00 , avg batch size 40.1
2022-07-04 01:02:05,735 - 4:11:38 - 984.4s - INFO - __main__ - progress 5.347 , lr 3.5E-05 , loss 0.365 , qa loss 0.365 , lm loss 0.000 , avg batch size 39.9
2022-07-04 01:18:17,785 - 4:27:50 - 972.1s - INFO - __main__ - progress 5.697 , lr 3.3E-05 , loss 0.364 , qa loss 0.364 , lm loss 0.000 , avg batch size 40.1
2022-07-04 01:32:23,604 - 4:41:56 - 845.8s - INFO - __main__ - epoch 6/12 done , tot steps 17209 , lr 3.1E-05 , loss 0.36 , qa loss 0.36 , lm loss 0.00 , avg batch size 40.1
2022-07-04 01:48:47,463 - 4:58:20 - 983.9s - INFO - __main__ - progress 6.349 , lr 2.9E-05 , loss 0.357 , qa loss 0.357 , lm loss 0.000 , avg batch size 40.1
2022-07-04 02:04:59,267 - 5:14:31 - 971.8s - INFO - __main__ - progress 6.696 , lr 2.8E-05 , loss 0.358 , qa loss 0.358 , lm loss 0.000 , avg batch size 40.0
2022-07-04 02:19:06,520 - 5:28:39 - 847.3s - INFO - __main__ - epoch 7/12 done , tot steps 20078 , lr 2.6E-05 , loss 0.36 , qa loss 0.36 , lm loss 0.00 , avg batch size 40.1
2022-07-04 02:35:28,081 - 5:45:00 - 981.6s - INFO - __main__ - progress 7.350 , lr 2.4E-05 , loss 0.352 , qa loss 0.352 , lm loss 0.000 , avg batch size 40.3
2022-07-04 02:51:40,937 - 6:01:13 - 972.9s - INFO - __main__ - progress 7.698 , lr 2.2E-05 , loss 0.354 , qa loss 0.354 , lm loss 0.000 , avg batch size 40.1
2022-07-04 03:05:47,150 - 6:15:19 - 846.2s - INFO - __main__ - epoch 8/12 done , tot steps 22946 , lr 2.1E-05 , loss 0.35 , qa loss 0.35 , lm loss 0.00 , avg batch size 40.1
2022-07-04 03:22:09,289 - 6:31:41 - 982.1s - INFO - __main__ - progress 8.348 , lr 1.9E-05 , loss 0.346 , qa loss 0.346 , lm loss 0.000 , avg batch size 40.0
2022-07-04 03:38:20,454 - 6:47:53 - 971.2s - INFO - __main__ - progress 8.696 , lr 1.7E-05 , loss 0.347 , qa loss 0.347 , lm loss 0.000 , avg batch size 40.0
2022-07-04 03:52:28,056 - 7:02:00 - 847.6s - INFO - __main__ - epoch 9/12 done , tot steps 25816 , lr 1.6E-05 , loss 0.35 , qa loss 0.35 , lm loss 0.00 , avg batch size 40.1
2022-07-04 04:08:49,522 - 7:18:22 - 981.5s - INFO - __main__ - progress 9.349 , lr 1.4E-05 , loss 0.343 , qa loss 0.343 , lm loss 0.000 , avg batch size 40.1
2022-07-04 04:24:59,416 - 7:34:32 - 969.9s - INFO - __main__ - progress 9.698 , lr 1.2E-05 , loss 0.343 , qa loss 0.343 , lm loss 0.000 , avg batch size 40.1
2022-07-04 04:39:02,692 - 7:48:35 - 843.3s - INFO - __main__ - epoch 10/12 done , tot steps 28681 , lr 1.0E-05 , loss 0.34 , qa loss 0.34 , lm loss 0.00 , avg batch size 40.1
2022-07-04 04:55:25,942 - 8:04:58 - 983.3s - INFO - __main__ - progress 10.350 , lr 8.6E-06 , loss 0.339 , qa loss 0.339 , lm loss 0.000 , avg batch size 40.3
2022-07-04 05:11:38,922 - 8:21:11 - 973.0s - INFO - __main__ - progress 10.698 , lr 6.8E-06 , loss 0.339 , qa loss 0.339 , lm loss 0.000 , avg batch size 40.2
2022-07-04 05:25:42,600 - 8:35:15 - 843.7s - INFO - __main__ - epoch 11/12 done , tot steps 31545 , lr 5.2E-06 , loss 0.34 , qa loss 0.34 , lm loss 0.00 , avg batch size 40.2
2022-07-04 05:42:03,851 - 8:51:36 - 981.3s - INFO - __main__ - progress 11.349 , lr 3.4E-06 , loss 0.335 , qa loss 0.335 , lm loss 0.000 , avg batch size 40.1
2022-07-04 05:58:16,675 - 9:07:49 - 972.8s - INFO - __main__ - progress 11.697 , lr 1.6E-06 , loss 0.335 , qa loss 0.335 , lm loss 0.000 , avg batch size 40.1
2022-07-04 06:12:21,569 - 9:21:54 - 844.9s - INFO - __main__ - epoch 12/12 done , tot steps 34411 , lr 2.6E-08 , loss 0.34 , qa loss 0.34 , lm loss 0.00 , avg batch size 40.1
2022-07-04 06:12:29,463 - 9:22:02 - 7.9s - INFO - __main__ - start to train { task: ['sst'], seq train type: lll }
2022-07-04 06:12:34,315 - 9:22:06 - 4.9s - INFO - utils - writing extra data in models/gpt2/lll/amazon_sst_srl_woz.en_0.0/amazon/lm.csv ...
2022-07-04 06:12:34,315 - 9:22:06 - 0.0s - INFO - __main__ - extra training data size: 0
2022-07-04 06:12:34,437 - 9:22:07 - 0.1s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[amazon]
The task with which model is saved amazon
2022-07-04 06:12:42,366 - 9:22:15 - 7.9s - INFO - __main__ - len of train dataset: 6920 , max train batch size 4 , num of opt steps: 83040
2022-07-04 06:15:53,362 - 9:25:26 - 191.0s - INFO - __main__ - progress 0.578 , lr 6.0E-05 , loss 1.986 , qa loss 1.986 , lm loss 0.000 , avg batch size 4.0
2022-07-04 06:18:10,836 - 9:27:43 - 137.5s - INFO - __main__ - epoch 1/12 done , tot steps 1730 , lr 5.7E-05 , loss 1.26 , qa loss 1.26 , lm loss 0.00 , avg batch size 4.0
2022-07-04 06:21:22,522 - 9:30:55 - 191.7s - INFO - __main__ - progress 1.578 , lr 5.4E-05 , loss 0.218 , qa loss 0.218 , lm loss 0.000 , avg batch size 4.0
2022-07-04 06:23:37,671 - 9:33:10 - 135.1s - INFO - __main__ - epoch 2/12 done , tot steps 3460 , lr 5.2E-05 , loss 0.22 , qa loss 0.22 , lm loss 0.00 , avg batch size 4.0
2022-07-04 06:26:50,212 - 9:36:22 - 192.5s - INFO - __main__ - progress 2.578 , lr 4.9E-05 , loss 0.191 , qa loss 0.191 , lm loss 0.000 , avg batch size 4.0
2022-07-04 06:29:06,240 - 9:38:38 - 136.0s - INFO - __main__ - epoch 3/12 done , tot steps 5190 , lr 4.7E-05 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2022-07-04 06:32:17,847 - 9:41:50 - 191.6s - INFO - __main__ - progress 3.578 , lr 4.4E-05 , loss 0.164 , qa loss 0.164 , lm loss 0.000 , avg batch size 4.0
2022-07-04 06:34:33,933 - 9:44:06 - 136.1s - INFO - __main__ - epoch 4/12 done , tot steps 6920 , lr 4.2E-05 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2022-07-04 06:37:46,883 - 9:47:19 - 193.0s - INFO - __main__ - progress 4.578 , lr 3.9E-05 , loss 0.135 , qa loss 0.135 , lm loss 0.000 , avg batch size 4.0
2022-07-04 06:40:02,377 - 9:49:35 - 135.5s - INFO - __main__ - epoch 5/12 done , tot steps 8650 , lr 3.6E-05 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
2022-07-04 06:43:13,331 - 9:52:46 - 191.0s - INFO - __main__ - progress 5.578 , lr 3.3E-05 , loss 0.131 , qa loss 0.131 , lm loss 0.000 , avg batch size 4.0
2022-07-04 06:45:30,317 - 9:55:02 - 137.0s - INFO - __main__ - epoch 6/12 done , tot steps 10380 , lr 3.1E-05 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2022-07-04 06:48:38,568 - 9:58:11 - 188.3s - INFO - __main__ - progress 6.578 , lr 2.8E-05 , loss 0.115 , qa loss 0.115 , lm loss 0.000 , avg batch size 4.0
2022-07-04 06:50:53,932 - 10:00:26 - 135.4s - INFO - __main__ - epoch 7/12 done , tot steps 12110 , lr 2.6E-05 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2022-07-04 06:54:03,069 - 10:03:35 - 189.1s - INFO - __main__ - progress 7.578 , lr 2.3E-05 , loss 0.099 , qa loss 0.099 , lm loss 0.000 , avg batch size 4.0
2022-07-04 06:56:21,071 - 10:05:53 - 138.0s - INFO - __main__ - epoch 8/12 done , tot steps 13840 , lr 2.1E-05 , loss 0.10 , qa loss 0.10 , lm loss 0.00 , avg batch size 4.0
2022-07-04 06:59:31,345 - 10:09:04 - 190.3s - INFO - __main__ - progress 8.578 , lr 1.8E-05 , loss 0.091 , qa loss 0.091 , lm loss 0.000 , avg batch size 4.0
2022-07-04 07:01:48,817 - 10:11:21 - 137.5s - INFO - __main__ - epoch 9/12 done , tot steps 15570 , lr 1.6E-05 , loss 0.09 , qa loss 0.09 , lm loss 0.00 , avg batch size 4.0
2022-07-04 07:04:59,152 - 10:14:31 - 190.3s - INFO - __main__ - progress 9.578 , lr 1.3E-05 , loss 0.072 , qa loss 0.072 , lm loss 0.000 , avg batch size 4.0
2022-07-04 07:07:14,654 - 10:16:47 - 135.5s - INFO - __main__ - epoch 10/12 done , tot steps 17300 , lr 1.0E-05 , loss 0.08 , qa loss 0.08 , lm loss 0.00 , avg batch size 4.0
2022-07-04 07:10:25,146 - 10:19:57 - 190.5s - INFO - __main__ - progress 10.578 , lr 7.4E-06 , loss 0.065 , qa loss 0.065 , lm loss 0.000 , avg batch size 4.0
2022-07-04 07:12:42,071 - 10:22:14 - 136.9s - INFO - __main__ - epoch 11/12 done , tot steps 19030 , lr 5.2E-06 , loss 0.07 , qa loss 0.07 , lm loss 0.00 , avg batch size 4.0
2022-07-04 07:15:54,344 - 10:25:27 - 192.3s - INFO - __main__ - progress 11.578 , lr 2.2E-06 , loss 0.057 , qa loss 0.057 , lm loss 0.000 , avg batch size 4.0
2022-07-04 07:18:10,039 - 10:27:42 - 135.7s - INFO - __main__ - epoch 12/12 done , tot steps 20760 , lr 2.6E-08 , loss 0.06 , qa loss 0.06 , lm loss 0.00 , avg batch size 4.0
2022-07-04 07:18:16,416 - 10:27:49 - 6.4s - INFO - __main__ - start to train { task: ['srl'], seq train type: lll }
2022-07-04 07:18:21,371 - 10:27:54 - 5.0s - INFO - utils - writing extra data in models/gpt2/lll/amazon_sst_srl_woz.en_0.0/sst/lm.csv ...
2022-07-04 07:18:21,371 - 10:27:54 - 0.0s - INFO - __main__ - extra training data size: 0
2022-07-04 07:18:21,520 - 10:27:54 - 0.1s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[amazon]
The task with which model is saved amazon
2022-07-04 07:18:29,413 - 10:28:02 - 7.9s - INFO - __main__ - len of train dataset: 6414 , max train batch size 4 , num of opt steps: 76968
2022-07-04 07:22:24,097 - 10:31:56 - 234.7s - INFO - __main__ - progress 0.624 , lr 5.9E-05 , loss 2.836 , qa loss 2.836 , lm loss 0.000 , avg batch size 4.0
2022-07-04 07:24:43,999 - 10:34:16 - 139.9s - INFO - __main__ - epoch 1/12 done , tot steps 1604 , lr 5.7E-05 , loss 2.10 , qa loss 2.10 , lm loss 0.00 , avg batch size 4.0
2022-07-04 07:28:38,890 - 10:38:11 - 234.9s - INFO - __main__ - progress 1.624 , lr 5.4E-05 , loss 0.727 , qa loss 0.727 , lm loss 0.000 , avg batch size 4.0
2022-07-04 07:30:58,509 - 10:40:31 - 139.6s - INFO - __main__ - epoch 2/12 done , tot steps 3208 , lr 5.2E-05 , loss 0.70 , qa loss 0.70 , lm loss 0.00 , avg batch size 4.0
2022-07-04 07:34:54,750 - 10:44:27 - 236.2s - INFO - __main__ - progress 2.624 , lr 4.9E-05 , loss 0.594 , qa loss 0.594 , lm loss 0.000 , avg batch size 4.0
2022-07-04 07:37:12,021 - 10:46:44 - 137.3s - INFO - __main__ - epoch 3/12 done , tot steps 4812 , lr 4.7E-05 , loss 0.59 , qa loss 0.59 , lm loss 0.00 , avg batch size 4.0
2022-07-04 07:41:07,309 - 10:50:39 - 235.3s - INFO - __main__ - progress 3.624 , lr 4.4E-05 , loss 0.514 , qa loss 0.514 , lm loss 0.000 , avg batch size 4.0
2022-07-04 07:43:25,693 - 10:52:58 - 138.4s - INFO - __main__ - epoch 4/12 done , tot steps 6416 , lr 4.2E-05 , loss 0.51 , qa loss 0.51 , lm loss 0.00 , avg batch size 4.0
2022-07-04 07:47:21,451 - 10:56:54 - 235.8s - INFO - __main__ - progress 4.624 , lr 3.8E-05 , loss 0.448 , qa loss 0.448 , lm loss 0.000 , avg batch size 4.0
2022-07-04 07:49:42,553 - 10:59:15 - 141.1s - INFO - __main__ - epoch 5/12 done , tot steps 8020 , lr 3.6E-05 , loss 0.44 , qa loss 0.44 , lm loss 0.00 , avg batch size 4.0
2022-07-04 07:53:36,100 - 11:03:08 - 233.5s - INFO - __main__ - progress 5.624 , lr 3.3E-05 , loss 0.403 , qa loss 0.403 , lm loss 0.000 , avg batch size 4.0
2022-07-04 07:55:54,671 - 11:05:27 - 138.6s - INFO - __main__ - epoch 6/12 done , tot steps 9624 , lr 3.1E-05 , loss 0.40 , qa loss 0.40 , lm loss 0.00 , avg batch size 4.0
2022-07-04 07:59:45,732 - 11:09:18 - 231.1s - INFO - __main__ - progress 6.624 , lr 2.8E-05 , loss 0.351 , qa loss 0.351 , lm loss 0.000 , avg batch size 4.0
2022-07-04 08:02:06,190 - 11:11:38 - 140.5s - INFO - __main__ - epoch 7/12 done , tot steps 11228 , lr 2.6E-05 , loss 0.36 , qa loss 0.36 , lm loss 0.00 , avg batch size 4.0
2022-07-04 08:05:59,001 - 11:15:31 - 232.8s - INFO - __main__ - progress 7.624 , lr 2.3E-05 , loss 0.319 , qa loss 0.319 , lm loss 0.000 , avg batch size 4.0
2022-07-04 08:08:18,106 - 11:17:50 - 139.1s - INFO - __main__ - epoch 8/12 done , tot steps 12832 , lr 2.1E-05 , loss 0.33 , qa loss 0.33 , lm loss 0.00 , avg batch size 4.0
2022-07-04 08:12:09,607 - 11:21:42 - 231.5s - INFO - __main__ - progress 8.624 , lr 1.8E-05 , loss 0.302 , qa loss 0.302 , lm loss 0.000 , avg batch size 4.0
2022-07-04 08:14:28,548 - 11:24:01 - 138.9s - INFO - __main__ - epoch 9/12 done , tot steps 14436 , lr 1.6E-05 , loss 0.30 , qa loss 0.30 , lm loss 0.00 , avg batch size 4.0
2022-07-04 08:18:21,593 - 11:27:54 - 233.0s - INFO - __main__ - progress 9.624 , lr 1.2E-05 , loss 0.271 , qa loss 0.271 , lm loss 0.000 , avg batch size 4.0
2022-07-04 08:20:38,761 - 11:30:11 - 137.2s - INFO - __main__ - epoch 10/12 done , tot steps 16040 , lr 1.0E-05 , loss 0.28 , qa loss 0.28 , lm loss 0.00 , avg batch size 4.0
2022-07-04 08:24:30,694 - 11:34:03 - 231.9s - INFO - __main__ - progress 10.624 , lr 7.2E-06 , loss 0.261 , qa loss 0.261 , lm loss 0.000 , avg batch size 4.0
2022-07-04 08:26:49,187 - 11:36:21 - 138.5s - INFO - __main__ - epoch 11/12 done , tot steps 17644 , lr 5.2E-06 , loss 0.26 , qa loss 0.26 , lm loss 0.00 , avg batch size 4.0
2022-07-04 08:30:43,279 - 11:40:15 - 234.1s - INFO - __main__ - progress 11.624 , lr 2.0E-06 , loss 0.259 , qa loss 0.259 , lm loss 0.000 , avg batch size 4.0
2022-07-04 08:33:01,381 - 11:42:34 - 138.1s - INFO - __main__ - epoch 12/12 done , tot steps 19248 , lr 2.6E-08 , loss 0.26 , qa loss 0.26 , lm loss 0.00 , avg batch size 4.0
2022-07-04 08:33:06,940 - 11:42:39 - 5.6s - INFO - __main__ - start to train { task: ['woz.en'], seq train type: lll }
2022-07-04 08:33:11,665 - 11:42:44 - 4.7s - INFO - utils - writing extra data in models/gpt2/lll/amazon_sst_srl_woz.en_0.0/srl/lm.csv ...
2022-07-04 08:33:11,666 - 11:42:44 - 0.0s - INFO - __main__ - extra training data size: 0
2022-07-04 08:33:11,800 - 11:42:44 - 0.1s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[amazon]
The task with which model is saved amazon
2022-07-04 08:33:19,580 - 11:42:52 - 7.8s - INFO - __main__ - len of train dataset: 2536 , max train batch size 4 , num of opt steps: 30432
2022-07-04 08:35:25,401 - 11:44:58 - 125.8s - INFO - __main__ - epoch 1/12 done , tot steps 634 , lr 5.7E-05 , loss 3.28 , qa loss 3.28 , lm loss 0.00 , avg batch size 4.0
2022-07-04 08:37:28,850 - 11:47:01 - 123.4s - INFO - __main__ - epoch 2/12 done , tot steps 1268 , lr 5.2E-05 , loss 0.38 , qa loss 0.38 , lm loss 0.00 , avg batch size 4.0
2022-07-04 08:39:32,866 - 11:49:05 - 124.0s - INFO - __main__ - epoch 3/12 done , tot steps 1902 , lr 4.7E-05 , loss 0.28 , qa loss 0.28 , lm loss 0.00 , avg batch size 4.0
2022-07-04 08:41:36,588 - 11:51:09 - 123.7s - INFO - __main__ - epoch 4/12 done , tot steps 2536 , lr 4.2E-05 , loss 0.23 , qa loss 0.23 , lm loss 0.00 , avg batch size 4.0
2022-07-04 08:43:40,884 - 11:53:13 - 124.3s - INFO - __main__ - epoch 5/12 done , tot steps 3170 , lr 3.6E-05 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
2022-07-04 08:45:44,970 - 11:55:17 - 124.1s - INFO - __main__ - epoch 6/12 done , tot steps 3804 , lr 3.1E-05 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
2022-07-04 08:47:50,148 - 11:57:22 - 125.2s - INFO - __main__ - epoch 7/12 done , tot steps 4438 , lr 2.6E-05 , loss 0.17 , qa loss 0.17 , lm loss 0.00 , avg batch size 4.0
2022-07-04 08:49:54,708 - 11:59:27 - 124.6s - INFO - __main__ - epoch 8/12 done , tot steps 5072 , lr 2.1E-05 , loss 0.14 , qa loss 0.14 , lm loss 0.00 , avg batch size 4.0
2022-07-04 08:51:58,793 - 12:01:31 - 124.1s - INFO - __main__ - epoch 9/12 done , tot steps 5706 , lr 1.6E-05 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2022-07-04 08:54:03,614 - 12:03:36 - 124.8s - INFO - __main__ - epoch 10/12 done , tot steps 6340 , lr 1.0E-05 , loss 0.12 , qa loss 0.12 , lm loss 0.00 , avg batch size 4.0
2022-07-04 08:56:06,868 - 12:05:39 - 123.3s - INFO - __main__ - epoch 11/12 done , tot steps 6974 , lr 5.2E-06 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
2022-07-04 08:58:11,040 - 12:07:43 - 124.2s - INFO - __main__ - epoch 12/12 done , tot steps 7608 , lr 2.5E-08 , loss 0.11 , qa loss 0.11 , lm loss 0.00 , avg batch size 4.0
The current active adapter is Stack[amazon]
The task with which model is saved amazon
Wall Execution time: 12:07:40
CPU Execution time: 17:15:01
