Not all gpus support fp16 training! Will use fp32 instead.
2023-07-03 23:45:42,629 - 0:00:04 - 0.0s - INFO - __main__ - args = Namespace(adam_epsilon=0.0001, add_task_tokens=False, data_dir='../../data', debug=False, decay_style='linear', device_ids=[0], dynamic_epochs=False, fp32=False, gen_lm_sample_percentage=0.0, learning_rate=6.25e-05, lm_lambda=0.0, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[40537.0], min_batch_size=4, min_n_steps=1500, model_dir_root='../../model_ao6/gpt2/lll/woz.en_srl_sst_0.0', model_name='gpt2', n_gpus=1, n_train_epochs={'woz.en': 8, 'srl': 8, 'sst': 8}, n_warmup_ratio=0.005, n_workers=25, qp_margin=0.5, real_sample=False, reg_lambda=1.0, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['woz.en', 'srl', 'sst'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[14187], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[14187], unbound=0, use_sep=False, weight_decay=0.01)
2023-07-03 23:45:55,654 - 0:00:17 - 13.0s - INFO - __main__ - task: woz.en, epoch: 8
2023-07-03 23:45:55,654 - 0:00:17 - 0.0s - INFO - __main__ - start to test { task: woz.en (load) woz.en (eval), seq train type: lll }
2023-07-03 23:45:56,524 - 0:00:18 - 0.9s - INFO - __main__ - len of test dataset: 1646
