Not all gpus support fp16 training! Will use fp32 instead.
2023-07-03 18:20:06,642 - 0:00:06 - 0.0s - INFO - __main__ - args = Namespace(REG_TYPE_KEYS=['mas', 'ewc'], adam_epsilon=0.0001, add_task_tokens=False, data_dir='../../data', debug=False, decay_style='linear', device_ids=[1], dynamic_epochs=False, fp32=True, gen_lm_sample_percentage=0.0, learning_rate=6.25e-05, lm_lambda=0.0, logging_steps=1000, lr_schedule='warmup_linear', max_grad_norm=1, max_len=1024, max_n_epochs=9, memory_sizes=[40537.0], min_batch_size=4, min_n_steps=1500, model_dir_root='../../model_ao1/gpt2/lll/sst_srl_woz.en_0.0', model_name='gpt2', n_gpus=1, n_train_epochs={'sst': 8, 'srl': 8, 'woz.en': 8}, n_warmup_ratio=0.005, n_workers=75, qp_margin=0.5, real_sample=False, reg_lambda=1.0, seed=42, seq_train_type='lll', skip_tasks=None, tasks=['sst', 'srl', 'woz.en'], temperature_lm=1.0, temperature_qa=1.0, test_batch_size=[14187], tokens_weight=5, top_k_lm=20, top_k_qa=20, top_p_lm=0.0, top_p_qa=0.0, train_batch_size=[14187], unbound=0, use_sep=False, weight_decay=0.01)
2023-07-03 18:20:06,642 - 0:00:06 - 0.0s - INFO - __main__ - start to train { task: ['sst'], seq train type: lll }
2023-07-03 18:20:06,650 - 0:00:06 - 0.0s - INFO - __main__ - extra training data size: 0
2023-07-03 18:20:09,576 - 0:00:09 - 2.9s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
2023-07-03 18:20:12,388 - 0:00:12 - 2.8s - INFO - __main__ - len of train dataset: 6920 , max train batch size 4 , num of opt steps: 55360
/home/student/2021/cs21mtech11006/anaconda3/envs/lamol/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
2023-07-03 18:21:35,414 - 0:01:35 - 83.0s - INFO - __main__ - progress 0.578 , lr 5.8E-05 , loss 3.501 , qa loss 3.501 , lm loss 0.000 , avg batch size 4.0
2023-07-03 18:22:50,523 - 0:02:50 - 75.1s - INFO - __main__ - epoch 1/8 done , tot steps 1730 , lr 5.5E-05 , loss 2.14 , qa loss 2.14 , lm loss 0.00 , avg batch size 4.0
2023-07-03 18:25:15,654 - 0:05:15 - 145.1s - INFO - __main__ - progress 1.578 , lr 5.0E-05 , loss 0.216 , qa loss 0.216 , lm loss 0.000 , avg batch size 4.0
2023-07-03 18:27:03,501 - 0:07:03 - 107.8s - INFO - __main__ - epoch 2/8 done , tot steps 3460 , lr 4.7E-05 , loss 0.21 , qa loss 0.21 , lm loss 0.00 , avg batch size 4.0
2023-07-03 18:29:30,114 - 0:09:29 - 146.6s - INFO - __main__ - progress 2.578 , lr 4.2E-05 , loss 0.184 , qa loss 0.184 , lm loss 0.000 , avg batch size 4.0
2023-07-03 18:31:17,344 - 0:11:17 - 107.2s - INFO - __main__ - epoch 3/8 done , tot steps 5190 , lr 3.9E-05 , loss 0.18 , qa loss 0.18 , lm loss 0.00 , avg batch size 4.0
2023-07-03 18:33:43,577 - 0:13:43 - 146.2s - INFO - __main__ - progress 3.578 , lr 3.5E-05 , loss 0.161 , qa loss 0.161 , lm loss 0.000 , avg batch size 4.0
2023-07-03 18:35:24,541 - 0:15:24 - 101.0s - INFO - __main__ - epoch 4/8 done , tot steps 6920 , lr 3.1E-05 , loss 0.16 , qa loss 0.16 , lm loss 0.00 , avg batch size 4.0
2023-07-03 18:37:47,723 - 0:17:47 - 143.2s - INFO - __main__ - progress 4.578 , lr 2.7E-05 , loss 0.152 , qa loss 0.152 , lm loss 0.000 , avg batch size 4.0
2023-07-03 18:39:35,538 - 0:19:35 - 107.8s - INFO - __main__ - epoch 5/8 done , tot steps 8650 , lr 2.3E-05 , loss 0.15 , qa loss 0.15 , lm loss 0.00 , avg batch size 4.0
2023-07-03 18:41:56,906 - 0:21:56 - 141.4s - INFO - __main__ - progress 5.578 , lr 1.9E-05 , loss 0.142 , qa loss 0.142 , lm loss 0.000 , avg batch size 4.0
2023-07-03 18:43:42,498 - 0:23:42 - 105.6s - INFO - __main__ - epoch 6/8 done , tot steps 10380 , lr 1.6E-05 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-07-03 18:46:07,685 - 0:26:07 - 145.2s - INFO - __main__ - progress 6.578 , lr 1.1E-05 , loss 0.124 , qa loss 0.124 , lm loss 0.000 , avg batch size 4.0
2023-07-03 18:47:47,972 - 0:27:47 - 100.3s - INFO - __main__ - epoch 7/8 done , tot steps 12110 , lr 7.9E-06 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-07-03 18:50:11,460 - 0:30:11 - 143.5s - INFO - __main__ - progress 7.578 , lr 3.3E-06 , loss 0.143 , qa loss 0.143 , lm loss 0.000 , avg batch size 4.0
2023-07-03 18:52:04,413 - 0:32:04 - 113.0s - INFO - __main__ - epoch 8/8 done , tot steps 13840 , lr 3.8E-08 , loss 0.13 , qa loss 0.13 , lm loss 0.00 , avg batch size 4.0
2023-07-03 18:52:13,702 - 0:32:13 - 9.3s - INFO - __main__ - start to train { task: ['srl'], seq train type: lll }
2023-07-03 18:52:18,392 - 0:32:18 - 4.7s - INFO - utils - writing extra data in ../../model_ao1/gpt2/lll/sst_srl_woz.en_0.0/sst/lm.csv ...
2023-07-03 18:52:18,448 - 0:32:18 - 0.1s - INFO - __main__ - extra training data size: 0
2023-07-03 18:52:18,723 - 0:32:18 - 0.3s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[sst]
The task with which model is saved sst
2023-07-03 18:52:21,458 - 0:32:21 - 2.7s - INFO - __main__ - len of train dataset: 6414 , max train batch size 4 , num of opt steps: 51312
2023-07-03 18:56:41,242 - 0:36:41 - 259.8s - INFO - __main__ - progress 0.624 , lr 5.8E-05 , loss 3.878 , qa loss 3.878 , lm loss 0.000 , avg batch size 4.0
2023-07-03 18:59:22,468 - 0:39:22 - 161.2s - INFO - __main__ - epoch 1/8 done , tot steps 1604 , lr 5.5E-05 , loss 2.74 , qa loss 2.74 , lm loss 0.00 , avg batch size 4.0
2023-07-03 19:03:44,710 - 0:43:44 - 262.2s - INFO - __main__ - progress 1.624 , lr 5.0E-05 , loss 0.785 , qa loss 0.785 , lm loss 0.000 , avg batch size 4.0
2023-07-03 19:06:28,931 - 0:46:28 - 164.2s - INFO - __main__ - epoch 2/8 done , tot steps 3208 , lr 4.7E-05 , loss 0.76 , qa loss 0.76 , lm loss 0.00 , avg batch size 4.0
2023-07-03 19:10:50,819 - 0:50:50 - 261.9s - INFO - __main__ - progress 2.624 , lr 4.2E-05 , loss 0.629 , qa loss 0.629 , lm loss 0.000 , avg batch size 4.0
2023-07-03 19:13:31,673 - 0:53:31 - 160.9s - INFO - __main__ - epoch 3/8 done , tot steps 4812 , lr 3.9E-05 , loss 0.63 , qa loss 0.63 , lm loss 0.00 , avg batch size 4.0
2023-07-03 19:16:54,314 - 0:56:54 - 202.6s - INFO - __main__ - progress 3.624 , lr 3.4E-05 , loss 0.593 , qa loss 0.593 , lm loss 0.000 , avg batch size 4.0
2023-07-03 19:19:08,875 - 0:59:08 - 134.6s - INFO - __main__ - epoch 4/8 done , tot steps 6416 , lr 3.1E-05 , loss 0.58 , qa loss 0.58 , lm loss 0.00 , avg batch size 4.0
2023-07-03 19:22:44,223 - 1:02:44 - 215.3s - INFO - __main__ - progress 4.624 , lr 2.6E-05 , loss 0.528 , qa loss 0.528 , lm loss 0.000 , avg batch size 4.0
2023-07-03 19:24:56,580 - 1:04:56 - 132.4s - INFO - __main__ - epoch 5/8 done , tot steps 8020 , lr 2.3E-05 , loss 0.53 , qa loss 0.53 , lm loss 0.00 , avg batch size 4.0
2023-07-03 19:28:32,007 - 1:08:31 - 215.4s - INFO - __main__ - progress 5.624 , lr 1.9E-05 , loss 0.499 , qa loss 0.499 , lm loss 0.000 , avg batch size 4.0
2023-07-03 19:30:46,945 - 1:10:46 - 134.9s - INFO - __main__ - epoch 6/8 done , tot steps 9624 , lr 1.6E-05 , loss 0.50 , qa loss 0.50 , lm loss 0.00 , avg batch size 4.0
2023-07-03 19:34:36,044 - 1:14:35 - 229.1s - INFO - __main__ - progress 6.624 , lr 1.1E-05 , loss 0.480 , qa loss 0.480 , lm loss 0.000 , avg batch size 4.0
2023-07-03 19:36:59,945 - 1:16:59 - 143.9s - INFO - __main__ - epoch 7/8 done , tot steps 11228 , lr 7.9E-06 , loss 0.47 , qa loss 0.47 , lm loss 0.00 , avg batch size 4.0
2023-07-03 19:40:52,270 - 1:20:52 - 232.3s - INFO - __main__ - progress 7.624 , lr 3.0E-06 , loss 0.450 , qa loss 0.450 , lm loss 0.000 , avg batch size 4.0
2023-07-03 19:43:18,303 - 1:23:18 - 146.0s - INFO - __main__ - epoch 8/8 done , tot steps 12832 , lr 3.9E-08 , loss 0.45 , qa loss 0.45 , lm loss 0.00 , avg batch size 4.0
2023-07-03 19:43:26,823 - 1:23:26 - 8.5s - INFO - __main__ - start to train { task: ['woz.en'], seq train type: lll }
2023-07-03 19:43:30,924 - 1:23:30 - 4.1s - INFO - utils - writing extra data in ../../model_ao1/gpt2/lll/sst_srl_woz.en_0.0/srl/lm.csv ...
2023-07-03 19:43:31,008 - 1:23:30 - 0.1s - INFO - __main__ - extra training data size: 0
2023-07-03 19:43:31,265 - 1:23:31 - 0.3s - INFO - __main__ - gen token = __gen__ , gen token id = 50260
The current active adapter is Stack[srl]
The task with which model is saved srl
2023-07-03 19:43:34,885 - 1:23:34 - 3.6s - INFO - __main__ - len of train dataset: 2536 , max train batch size 4 , num of opt steps: 20288
2023-07-03 19:44:54,760 - 1:24:54 - 79.9s - INFO - __main__ - epoch 1/8 done , tot steps 634 , lr 5.5E-05 , loss 4.21 , qa loss 4.21 , lm loss 0.00 , avg batch size 4.0
2023-07-03 19:46:12,923 - 1:26:12 - 78.2s - INFO - __main__ - epoch 2/8 done , tot steps 1268 , lr 4.7E-05 , loss 0.49 , qa loss 0.49 , lm loss 0.00 , avg batch size 4.0
2023-07-03 19:47:35,496 - 1:27:35 - 82.6s - INFO - __main__ - epoch 3/8 done , tot steps 1902 , lr 3.9E-05 , loss 0.34 , qa loss 0.34 , lm loss 0.00 , avg batch size 4.0
2023-07-03 19:48:56,103 - 1:28:55 - 80.6s - INFO - __main__ - epoch 4/8 done , tot steps 2536 , lr 3.1E-05 , loss 0.27 , qa loss 0.27 , lm loss 0.00 , avg batch size 4.0
2023-07-03 19:50:13,449 - 1:30:13 - 77.3s - INFO - __main__ - epoch 5/8 done , tot steps 3170 , lr 2.3E-05 , loss 0.24 , qa loss 0.24 , lm loss 0.00 , avg batch size 4.0
2023-07-03 19:51:31,642 - 1:31:31 - 78.2s - INFO - __main__ - epoch 6/8 done , tot steps 3804 , lr 1.6E-05 , loss 0.23 , qa loss 0.23 , lm loss 0.00 , avg batch size 4.0
2023-07-03 19:52:47,364 - 1:32:47 - 75.7s - INFO - __main__ - epoch 7/8 done , tot steps 4438 , lr 7.8E-06 , loss 0.20 , qa loss 0.20 , lm loss 0.00 , avg batch size 4.0
2023-07-03 19:54:12,911 - 1:34:12 - 85.5s - INFO - __main__ - epoch 8/8 done , tot steps 5072 , lr 3.7E-08 , loss 0.19 , qa loss 0.19 , lm loss 0.00 , avg batch size 4.0
The current active adapter is Stack[woz_en]
The task with which model is saved woz_en
Wall Execution time: 01:34:15
CPU Execution time: 01:37:20
